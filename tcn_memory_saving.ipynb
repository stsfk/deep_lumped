{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7c2bd-33c6-4a80-81ed-e08829fbfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: half percision training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a11959c-c2bb-4731-8d89-0cc6123d28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENT = 1758\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "TRAIN_YEAR = 19\n",
    "\n",
    "PATIENCE = 10\n",
    "\n",
    "dtypes = defaultdict(lambda: float)\n",
    "dtypes[\"catchment_id\"] = str\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a865b605-c842-48d1-bce6-1d7c8c39884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/Caravan/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "    ):\n",
    "        data_raw = pd.read_csv(fpath, dtype=dtypes)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = data_raw.loc[:, \"P\":\"PET\"]\n",
    "\n",
    "        x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        x = x.view(-1, record_length, n_feature)\n",
    "        self.x = x # edit here\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = data_raw[\"Q\"]\n",
    "\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        y = y.view(-1, record_length)\n",
    "        self.y = y # edit here\n",
    "        \n",
    "        self.record_length = self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # SEQ_LENGTH, BASE_LENGTH, and DEVICE is from global\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - SEQ_LENGTH + 1,\n",
    "            size=(N_CATCHMENT,)\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of SEQ_LENGTH, adding 0 to SEQ_LENGTH to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, SEQ_LENGTH) + torch.arange(SEQ_LENGTH)\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, FORCING_DIM)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = self.x.gather(dim=1, index=index_x), self.y.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return x_batch.to(DEVICE), y_batch[:, BASE_LENGTH:].to(DEVICE) # edit here\n",
    "    \n",
    "    def get_val_batch(self): # edit here\n",
    "        n_years = math.ceil((self.record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "        \n",
    "        out_x = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH, FORCING_DIM])*torch.nan\n",
    "        out_y = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH])*torch.nan\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH*i\n",
    "                \n",
    "            if i==n_years-1:\n",
    "                end_record_ind = self.record_length\n",
    "                \n",
    "                out_x[i,:,0:(end_record_ind-start_record_ind),:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,0:(end_record_ind-start_record_ind)] = self.y[:,start_record_ind:end_record_ind]\n",
    "                \n",
    "            else:\n",
    "                end_record_ind = start_record_ind + SEQ_LENGTH\n",
    "\n",
    "                out_x[i,:,:,:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,:] = self.y[:,start_record_ind:end_record_ind]\n",
    "        \n",
    "        return out_x, out_y[:,:,BASE_LENGTH:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7666e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = Forcing_Data(\"data/data_train_w_missing.csv\", record_length=7304)\n",
    "dval = Forcing_Data(\"data/data_val_w_missing.csv\",  record_length=4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfe6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Reference: https://stackoverflow.com/a/73704579/3361298\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d55f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        feature_dim,\n",
    "        num_channels,\n",
    "        kernel_size,\n",
    "        p=0.2,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_inputs = latent_dim + feature_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p = p\n",
    "\n",
    "        self.tcn = tcn.TemporalConvNet(\n",
    "            num_inputs=self.num_inputs,\n",
    "            num_channels=self.num_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            dropout=self.p,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): shape = [batch_size, input channels, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape = [batch_size, output channels, seq_len]\n",
    "        \"\"\"\n",
    "        return self.tcn(inputs)\n",
    "\n",
    "    def decode(self, code, x, base_length=BASE_LENGTH):\n",
    "        \"\"\"Predict hydrographs under forcing x for catchments charaterized by `code`\n",
    "\n",
    "        Args:\n",
    "            code (tensor): shape = [batch_size, latent dim]\n",
    "            x (tensor): shape = [batch_size, seq_len, latent dim]\n",
    "            base_length (int): predictions for the first 'base_length' time step is ignored. Defaults to BASE_LENGTH.\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape = [base_size, seq_len - base_length]\n",
    "        \"\"\"\n",
    "        \n",
    "        code = code.expand(x.shape[1], -1, -1).transpose(0, 1) # new shape [batch_size, seq_len, latent dim\n",
    "        \n",
    "        x = torch.cat((code, x), 2) # concatenate code and x in dim 2\n",
    "        \n",
    "        x = x.transpose(\n",
    "            1, 2\n",
    "        ).contiguous()  # new shape [batch_size, input channel, seq_len]\n",
    "        \n",
    "        out = (\n",
    "            self.tcn(x).transpose(1, 2)[:, base_length:, :].squeeze()\n",
    "        )  # new shape [batch_size, Target length]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1415dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_with_nans(input, target):\n",
    "    # Adapted from https://stackoverflow.com/a/59851632/3361298\n",
    "\n",
    "    # Missing data are nans\n",
    "    mask = torch.isnan(target)\n",
    "\n",
    "    out = (input[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4b266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(\n",
    "    embedding, decoder, dataset, val_metric=mse_loss_with_nans, return_summary=True, val_steps = 1000\n",
    "):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = dataset.get_val_batch()\n",
    "    \n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size = y.shape)\n",
    "\n",
    "    # iterate over years\n",
    "    for i in range(x.shape[0]):\n",
    "        # iterate over catchments\n",
    "        for j in range(math.ceil(N_CATCHMENT / val_steps)):\n",
    "            start_catchment_ind = j * val_steps\n",
    "            end_catchment_ind = min((j + 1) * val_steps, N_CATCHMENT)\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                with torch.no_grad():\n",
    "                    code = embedding(torch.arange(start=start_catchment_ind, end = end_catchment_ind).to(DEVICE))\n",
    "                    x_sub = x[i,start_catchment_ind:end_catchment_ind,:,:].to(DEVICE)\n",
    "                    preds[i,start_catchment_ind:end_catchment_ind,:] = decoder.decode(code, x_sub).cpu()\n",
    "                \n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9755c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "\n",
    "    # FORCING_DIM is from global\n",
    "\n",
    "    # latent dim\n",
    "    latent_dim_power = trial.suggest_int(\"latent_dim_power\", 1, 2)\n",
    "    latent_dim = 2**latent_dim_power\n",
    "\n",
    "    # kernel_size\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 6)\n",
    "\n",
    "    # num_channels\n",
    "    # ref: https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "    hidden_channel_dim = trial.suggest_int(\"hidden_channel_dim\", 1, 256)\n",
    "    base = 2  # dilation factor\n",
    "    n_levels = math.log(\n",
    "        (BASE_LENGTH - 1) * (base - 1) / (kernel_size - 1) / 2 + 1\n",
    "    ) / math.log(2)\n",
    "    n_levels = math.ceil(n_levels)\n",
    "\n",
    "    num_channels = []\n",
    "    for i in range(n_levels - 1):\n",
    "        num_channels.append(hidden_channel_dim)\n",
    "\n",
    "    num_channels.append(1)  # output dim = 1\n",
    "    \n",
    "    # p\n",
    "    drop_out_flag = trial.suggest_categorical(\"drop_out_flag\", [True, False])\n",
    "\n",
    "    if drop_out_flag:\n",
    "        p = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    else:\n",
    "        p = 0\n",
    "\n",
    "    # define model\n",
    "    decoder = Decoder(\n",
    "        latent_dim=latent_dim,\n",
    "        feature_dim=FORCING_DIM,\n",
    "        num_channels=num_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        p=p,\n",
    "    )\n",
    "\n",
    "    embedding = nn.Embedding(N_CATCHMENT, latent_dim)\n",
    "\n",
    "    return embedding, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a68aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    # prepare early stopper\n",
    "    early_stopper = EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "    \n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 7)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "        \n",
    "        for year in range(TRAIN_YEAR):\n",
    "\n",
    "            x_batch, y_batch = dtrain.get_random_batch()\n",
    "            catchment_index = torch.randperm(N_CATCHMENT).to(DEVICE)  # add randomness\n",
    "\n",
    "            # interate over catchments\n",
    "            for i in range(int(N_CATCHMENT / batch_size)):\n",
    "\n",
    "                # prepare data\n",
    "                ind_s = i * batch_size\n",
    "                ind_e = (i + 1) * batch_size\n",
    "\n",
    "                selected_catchments = catchment_index[ind_s:ind_e]\n",
    "\n",
    "                x_sub, y_sub = x_batch[ind_s:ind_e, :, :], y_batch[ind_s:ind_e, :]\n",
    "\n",
    "                # prepare training, put the models into training mode\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                    code = embedding(selected_catchments)\n",
    "                    out = decoder.decode(code, x_sub)\n",
    "\n",
    "                    # backprop\n",
    "                    loss = mse_loss_with_nans(out, y_sub)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "        # validate model after each epochs\n",
    "        decoder.eval()\n",
    "        embedding.eval()\n",
    "        \n",
    "        val_loss = val_model(embedding, decoder, dval).detach().cpu().numpy()\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Early stop using early_stopper, break for loop\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return early_stopper.min_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444ea97a-c3a7-4731-94ea-3e23ea9cc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-02 00:55:56,639]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 01:10:12,455]\u001b[0m Trial 0 finished with value: 4.3892598152160645 and parameters: {'latent_dim_power': 2, 'kernel_size': 2, 'hidden_channel_dim': 141, 'drop_out_flag': False, 'lr_embedding': 0.00033592468941881043, 'lr_decoder': 0.0021668469403275125, 'batch_size_power': 7}. Best is trial 0 with value: 4.3892598152160645.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 01:24:48,629]\u001b[0m Trial 1 finished with value: 4.455273628234863 and parameters: {'latent_dim_power': 1, 'kernel_size': 5, 'hidden_channel_dim': 53, 'drop_out_flag': False, 'lr_embedding': 0.0002943142643158566, 'lr_decoder': 0.0029264551946458477, 'batch_size_power': 4}. Best is trial 0 with value: 4.3892598152160645.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 02:05:09,548]\u001b[0m Trial 2 finished with value: 4.072592258453369 and parameters: {'latent_dim_power': 2, 'kernel_size': 4, 'hidden_channel_dim': 210, 'drop_out_flag': True, 'dropout_rate': 0.14991176856751953, 'lr_embedding': 7.207439419142573e-05, 'lr_decoder': 5.578592300150734e-05, 'batch_size_power': 7}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 02:29:50,103]\u001b[0m Trial 3 finished with value: 4.091434955596924 and parameters: {'latent_dim_power': 2, 'kernel_size': 3, 'hidden_channel_dim': 94, 'drop_out_flag': True, 'dropout_rate': 0.34929317975027474, 'lr_embedding': 0.0034321458527170326, 'lr_decoder': 0.00020283205911243227, 'batch_size_power': 7}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 07:05:29,363]\u001b[0m Trial 4 finished with value: 14.162660598754883 and parameters: {'latent_dim_power': 2, 'kernel_size': 3, 'hidden_channel_dim': 154, 'drop_out_flag': True, 'dropout_rate': 0.18884988138329184, 'lr_embedding': 9.233152389631115e-05, 'lr_decoder': 0.006687764353397119, 'batch_size_power': 6}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 07:23:48,138]\u001b[0m Trial 5 finished with value: 4.580957412719727 and parameters: {'latent_dim_power': 2, 'kernel_size': 4, 'hidden_channel_dim': 102, 'drop_out_flag': True, 'dropout_rate': 0.47501772197299863, 'lr_embedding': 0.0005730323030326358, 'lr_decoder': 0.00346807620185236, 'batch_size_power': 5}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 07:42:31,817]\u001b[0m Trial 6 finished with value: 4.312103271484375 and parameters: {'latent_dim_power': 1, 'kernel_size': 4, 'hidden_channel_dim': 101, 'drop_out_flag': False, 'lr_embedding': 0.004842763119616899, 'lr_decoder': 8.536577301514512e-05, 'batch_size_power': 4}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 07:52:36,457]\u001b[0m Trial 7 finished with value: 4.800870895385742 and parameters: {'latent_dim_power': 1, 'kernel_size': 3, 'hidden_channel_dim': 152, 'drop_out_flag': False, 'lr_embedding': 0.000273953107470869, 'lr_decoder': 0.005533319077691802, 'batch_size_power': 6}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 08:14:43,985]\u001b[0m Trial 8 finished with value: 4.177506446838379 and parameters: {'latent_dim_power': 2, 'kernel_size': 6, 'hidden_channel_dim': 43, 'drop_out_flag': False, 'lr_embedding': 0.000252140535936634, 'lr_decoder': 5.1695427754587324e-05, 'batch_size_power': 6}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 08:28:34,114]\u001b[0m Trial 9 finished with value: 5.753183364868164 and parameters: {'latent_dim_power': 2, 'kernel_size': 3, 'hidden_channel_dim': 3, 'drop_out_flag': True, 'dropout_rate': 0.2035118325504367, 'lr_embedding': 0.001043314014263012, 'lr_decoder': 0.005824796645522797, 'batch_size_power': 6}. Best is trial 2 with value: 4.072592258453369.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner()\n",
    ")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590e626-3ba7-43f8-9679-4af442102c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"base_TCN_study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b090000aab17214e4dd60cddf5bd8d4d225439b12a5c4dc7bae7f143c631854"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
