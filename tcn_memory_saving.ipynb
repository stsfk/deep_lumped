{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7c2bd-33c6-4a80-81ed-e08829fbfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: half percision training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a11959c-c2bb-4731-8d89-0cc6123d28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENT = 1758\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "TRAIN_YEAR = 19\n",
    "\n",
    "PATIENCE = 10\n",
    "\n",
    "dtypes = defaultdict(lambda: float)\n",
    "dtypes[\"catchment_id\"] = str\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a865b605-c842-48d1-bce6-1d7c8c39884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/Caravan/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "    ):\n",
    "        data_raw = pd.read_csv(fpath, dtype=dtypes)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = data_raw.loc[:, \"P\":\"PET\"]\n",
    "\n",
    "        x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        x = x.view(-1, record_length, n_feature)\n",
    "        self.x = x # edit here\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = data_raw[\"Q\"]\n",
    "\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        y = y.view(-1, record_length)\n",
    "        self.y = y # edit here\n",
    "        \n",
    "        self.record_length = self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # SEQ_LENGTH, BASE_LENGTH, and DEVICE is from global\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - SEQ_LENGTH + 1,\n",
    "            size=(N_CATCHMENT,)\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of SEQ_LENGTH, adding 0 to SEQ_LENGTH to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, SEQ_LENGTH) + torch.arange(SEQ_LENGTH)\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, FORCING_DIM)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = self.x.gather(dim=1, index=index_x), self.y.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return x_batch.to(DEVICE), y_batch[:, BASE_LENGTH:].to(DEVICE) # edit here\n",
    "    \n",
    "    def get_val_batch(self): # edit here\n",
    "        n_years = math.ceil((self.record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "        \n",
    "        out_x = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH, FORCING_DIM])*torch.nan\n",
    "        out_y = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH])*torch.nan\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH*i\n",
    "                \n",
    "            if i==n_years-1:\n",
    "                end_record_ind = self.record_length\n",
    "                \n",
    "                out_x[i,:,0:(end_record_ind-start_record_ind),:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,0:(end_record_ind-start_record_ind)] = self.y[:,start_record_ind:end_record_ind]\n",
    "                \n",
    "            else:\n",
    "                end_record_ind = start_record_ind + SEQ_LENGTH\n",
    "\n",
    "                out_x[i,:,:,:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,:] = self.y[:,start_record_ind:end_record_ind]\n",
    "        \n",
    "        return out_x, out_y[:,:,BASE_LENGTH:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7666e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = Forcing_Data(\"data/data_train_w_missing.csv\", record_length=7304)\n",
    "dval = Forcing_Data(\"data/data_val_w_missing.csv\",  record_length=4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfe6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Reference: https://stackoverflow.com/a/73704579/3361298\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d55f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        feature_dim,\n",
    "        num_channels,\n",
    "        kernel_size,\n",
    "        p=0.2,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_inputs = latent_dim + feature_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p = p\n",
    "\n",
    "        self.tcn = tcn.TemporalConvNet(\n",
    "            num_inputs=self.num_inputs,\n",
    "            num_channels=self.num_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            dropout=self.p,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): shape = [batch_size, input channels, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape = [batch_size, output channels, seq_len]\n",
    "        \"\"\"\n",
    "        return self.tcn(inputs)\n",
    "\n",
    "    def decode(self, code, x, base_length=BASE_LENGTH):\n",
    "        \"\"\"Predict hydrographs under forcing x for catchments charaterized by `code`\n",
    "\n",
    "        Args:\n",
    "            code (tensor): shape = [batch_size, latent dim]\n",
    "            x (tensor): shape = [batch_size, seq_len, latent dim]\n",
    "            base_length (int): predictions for the first 'base_length' time step is ignored. Defaults to BASE_LENGTH.\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape = [base_size, seq_len - base_length]\n",
    "        \"\"\"\n",
    "        \n",
    "        code = code.expand(x.shape[1], -1, -1).transpose(0, 1) # new shape [batch_size, seq_len, latent dim\n",
    "        \n",
    "        x = torch.cat((code, x), 2) # concatenate code and x in dim 2\n",
    "        \n",
    "        x = x.transpose(\n",
    "            1, 2\n",
    "        ).contiguous()  # new shape [batch_size, input channel, seq_len]\n",
    "        \n",
    "        out = (\n",
    "            self.tcn(x).transpose(1, 2)[:, base_length:, :].squeeze()\n",
    "        )  # new shape [batch_size, Target length]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1415dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_with_nans(input, target):\n",
    "    # Adapted from https://stackoverflow.com/a/59851632/3361298\n",
    "\n",
    "    # Missing data are nans\n",
    "    mask = torch.isnan(target)\n",
    "\n",
    "    out = (input[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4b266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(\n",
    "    embedding, decoder, dataset, val_metric=mse_loss_with_nans, return_summary=True, val_steps = 1000\n",
    "):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = dataset.get_val_batch()\n",
    "    \n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size = y.shape)\n",
    "\n",
    "    # iterate over years\n",
    "    for i in range(x.shape[0]):\n",
    "        # iterate over catchments\n",
    "        for j in range(math.ceil(N_CATCHMENT / val_steps)):\n",
    "            start_catchment_ind = j * val_steps\n",
    "            end_catchment_ind = min((j + 1) * val_steps, N_CATCHMENT)\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                with torch.no_grad():\n",
    "                    code = embedding(torch.arange(start=start_catchment_ind, end = end_catchment_ind).to(DEVICE))\n",
    "                    x_sub = x[i,start_catchment_ind:end_catchment_ind,:,:].to(DEVICE)\n",
    "                    preds[i,start_catchment_ind:end_catchment_ind,:] = decoder.decode(code, x_sub).cpu()\n",
    "                \n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9755c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "\n",
    "    # FORCING_DIM is from global\n",
    "\n",
    "    # latent dim\n",
    "    latent_dim_power = trial.suggest_int(\"latent_dim_power\", 1, 2)\n",
    "    latent_dim = 2**latent_dim_power\n",
    "\n",
    "    # kernel_size\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 6)\n",
    "\n",
    "    # num_channels\n",
    "    # ref: https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "    hidden_channel_dim = trial.suggest_int(\"hidden_channel_dim\", 1, 256)\n",
    "    base = 2  # dilation factor\n",
    "    n_levels = math.log(\n",
    "        (BASE_LENGTH - 1) * (base - 1) / (kernel_size - 1) / 2 + 1\n",
    "    ) / math.log(2)\n",
    "    n_levels = math.ceil(n_levels)\n",
    "\n",
    "    num_channels = []\n",
    "    for i in range(n_levels - 1):\n",
    "        num_channels.append(hidden_channel_dim)\n",
    "\n",
    "    num_channels.append(1)  # output dim = 1\n",
    "    \n",
    "    # p\n",
    "    drop_out_flag = trial.suggest_categorical(\"drop_out_flag\", [True, False])\n",
    "\n",
    "    if drop_out_flag:\n",
    "        p = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    else:\n",
    "        p = 0\n",
    "\n",
    "    # define model\n",
    "    decoder = Decoder(\n",
    "        latent_dim=latent_dim,\n",
    "        feature_dim=FORCING_DIM,\n",
    "        num_channels=num_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        p=p,\n",
    "    )\n",
    "\n",
    "    embedding = nn.Embedding(N_CATCHMENT, latent_dim)\n",
    "\n",
    "    return embedding, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a68aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    # prepare early stopper\n",
    "    early_stopper = EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "    \n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 7)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "        \n",
    "        for year in range(TRAIN_YEAR):\n",
    "\n",
    "            x_batch, y_batch = dtrain.get_random_batch()\n",
    "            catchment_index = torch.randperm(N_CATCHMENT).to(DEVICE)  # add randomness\n",
    "\n",
    "            # interate over catchments\n",
    "            for i in range(int(N_CATCHMENT / batch_size)):\n",
    "\n",
    "                # prepare data\n",
    "                ind_s = i * batch_size\n",
    "                ind_e = (i + 1) * batch_size\n",
    "\n",
    "                selected_catchments = catchment_index[ind_s:ind_e]\n",
    "\n",
    "                x_sub, y_sub = x_batch[ind_s:ind_e, :, :], y_batch[ind_s:ind_e, :]\n",
    "\n",
    "                # prepare training, put the models into training mode\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                    code = embedding(selected_catchments)\n",
    "                    out = decoder.decode(code, x_sub)\n",
    "\n",
    "                    # backprop\n",
    "                    loss = mse_loss_with_nans(out, y_sub)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "        # validate model after each epochs\n",
    "        decoder.eval()\n",
    "        embedding.eval()\n",
    "        \n",
    "        val_loss = val_model(embedding, decoder, dval).detach().cpu().numpy()\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Early stop using early_stopper, break for loop\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return early_stopper.min_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444ea97a-c3a7-4731-94ea-3e23ea9cc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-02 14:03:16,083]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 14:21:10,538]\u001b[0m Trial 0 finished with value: 4.212366580963135 and parameters: {'latent_dim_power': 2, 'kernel_size': 6, 'hidden_channel_dim': 157, 'drop_out_flag': False, 'lr_embedding': 0.0057860763442888815, 'lr_decoder': 8.69242418137445e-05, 'batch_size_power': 6}. Best is trial 0 with value: 4.212366580963135.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 15:15:26,707]\u001b[0m Trial 1 finished with value: 4.5039238929748535 and parameters: {'latent_dim_power': 2, 'kernel_size': 6, 'hidden_channel_dim': 134, 'drop_out_flag': True, 'dropout_rate': 0.1763872997141289, 'lr_embedding': 0.0033430453031401434, 'lr_decoder': 0.009263572184367292, 'batch_size_power': 4}. Best is trial 0 with value: 4.212366580963135.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 15:44:10,742]\u001b[0m Trial 2 finished with value: 4.057097911834717 and parameters: {'latent_dim_power': 1, 'kernel_size': 6, 'hidden_channel_dim': 123, 'drop_out_flag': True, 'dropout_rate': 0.19845586310146146, 'lr_embedding': 0.0005477526283652785, 'lr_decoder': 5.7310662165273254e-05, 'batch_size_power': 7}. Best is trial 2 with value: 4.057097911834717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 16:02:57,414]\u001b[0m Trial 3 finished with value: 4.583042144775391 and parameters: {'latent_dim_power': 1, 'kernel_size': 4, 'hidden_channel_dim': 110, 'drop_out_flag': False, 'lr_embedding': 0.0001853459092260986, 'lr_decoder': 0.005361440630190941, 'batch_size_power': 7}. Best is trial 2 with value: 4.057097911834717.\u001b[0m\n",
      "\u001b[33m[W 2022-12-02 20:28:59,719]\u001b[0m Trial 4 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_21560\\3289153609.py\", line 61, in objective\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"c:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbase_model\u001b[39m\u001b[39m\"\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mNopPruner()\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     _optimize(\n\u001b[0;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn [10], line 61\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[39m# backprop\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     loss \u001b[39m=\u001b[39m mse_loss_with_nans(out, y_sub)\n\u001b[1;32m---> 61\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     62\u001b[0m scaler\u001b[39m.\u001b[39mstep(embedding_optimizer)\n\u001b[0;32m     63\u001b[0m scaler\u001b[39m.\u001b[39mstep(decoder_optimizer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner()\n",
    ")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590e626-3ba7-43f8-9679-4af442102c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_TCN_study.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(study, \"base_TCN_study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b090000aab17214e4dd60cddf5bd8d4d225439b12a5c4dc7bae7f143c631854"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
