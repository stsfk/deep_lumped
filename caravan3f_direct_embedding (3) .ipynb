{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_SEQ = 1\n",
    "SEQ_LENGTH = 365*2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "N_CATCHMENT=2346\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "VAL_STEP_SIZE = 100\n",
    "\n",
    "PATIENCE = 25\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data_back(Dataset):\n",
    "    def __init__(\n",
    "        self, fpath=\"data/Caravan/data3f_train.csv\", record_length=5843, n_feature=3\n",
    "    ):\n",
    "        data_raw = np.genfromtxt(fpath, delimiter=\",\", skip_header=1)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = torch.from_numpy(data_raw[:, 0:n_feature]).to(dtype=torch.float32)\n",
    "        self.x = x.view(-1, record_length, n_feature).contiguous()\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = torch.from_numpy(data_raw[:, n_feature]).to(dtype=torch.float32)\n",
    "        self.y = y.view(-1, record_length).contiguous()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Index_Generator:\n",
    "    def __init__(self, catchment_number=749, record_length=5843):\n",
    "        self.catchment_number = catchment_number\n",
    "        self.record_length = record_length\n",
    "\n",
    "    def generate_random_consecutive_numbers(\n",
    "        self, n_seq=1, batch_size=64, seq_length=100\n",
    "    ):\n",
    "        selected_catchments = np.random.choice(\n",
    "            self.catchment_number, size=batch_size, replace=False\n",
    "        )\n",
    "\n",
    "        selected_starting_index = np.random.choice(\n",
    "            self.record_length - seq_length + 1, size=batch_size * n_seq, replace=True\n",
    "        )\n",
    "\n",
    "        selected_ending_index = selected_starting_index + seq_length\n",
    "\n",
    "        return selected_catchments, selected_starting_index, selected_ending_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Batch_Generator:\n",
    "    def __init__(self, dataset, index_gen, record_length):\n",
    "        self.dataset = dataset\n",
    "        self.index_gen = index_gen\n",
    "        self.record_length = record_length\n",
    "\n",
    "    def get_batch(self, n_seq=1, batch_size=64, seq_length=730, n_feature=3):\n",
    "\n",
    "        (\n",
    "            selected_catchments,\n",
    "            selected_starting_index,\n",
    "            selected_ending_index,\n",
    "        ) = self.index_gen.generate_random_consecutive_numbers(\n",
    "            n_seq, batch_size, seq_length\n",
    "        )\n",
    "\n",
    "        xs_batch = torch.ones([n_seq, batch_size, seq_length, n_feature])\n",
    "        ys_batch = torch.ones([n_seq, batch_size, seq_length])\n",
    "\n",
    "        for i in range(n_seq):\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                xs_batch[i, j, :, :] = self.dataset.x[\n",
    "                    selected_catchments[j],\n",
    "                    selected_starting_index[j + i * batch_size] : selected_ending_index[\n",
    "                        j + i * batch_size\n",
    "                    ],\n",
    "                    :,\n",
    "                ]\n",
    "                ys_batch[i, j, :] = self.dataset.y[\n",
    "                    selected_catchments[j],\n",
    "                    selected_starting_index[j + i * batch_size] : selected_ending_index[\n",
    "                        j + i * batch_size\n",
    "                    ],\n",
    "                ]\n",
    "\n",
    "        selected_catchments = torch.from_numpy(selected_catchments)\n",
    "\n",
    "        return xs_batch, ys_batch, selected_catchments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "        storge_device=\"cpu\",\n",
    "        seq_length=730,\n",
    "        target_seq_length=365,\n",
    "        base_length=365,\n",
    "    ):\n",
    "        data_raw = np.genfromtxt(fpath, delimiter=\",\", skip_header=1)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = torch.from_numpy(data_raw[:, 0:n_feature]).to(dtype=torch.float32)\n",
    "        x = x.view(-1, record_length, n_feature).contiguous()\n",
    "        self.x = x.to(storge_device)\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = torch.from_numpy(data_raw[:, n_feature]).to(dtype=torch.float32)\n",
    "        y = y.view(-1, record_length).contiguous()\n",
    "        self.y = y.to(storge_device)\n",
    "\n",
    "        self.n_catchment = y.shape[0]\n",
    "\n",
    "        self.n_feature = n_feature\n",
    "\n",
    "        self.record_length = self.x.shape[1]\n",
    "        self.seq_length = seq_length\n",
    "        self.target_seq_length = target_seq_length\n",
    "        self.base_length = base_length\n",
    "\n",
    "        self.storge_device = storge_device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self, batch_size=64):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        selected_catchment_index = torch.randint(\n",
    "            low=0, high=self.n_catchment, size=(batch_size,), device=self.storge_device\n",
    "        )\n",
    "\n",
    "        x_sub = torch.index_select(self.x, dim=0, index=selected_catchment_index)\n",
    "        y_sub = torch.index_select(self.y, dim=0, index=selected_catchment_index)\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - self.seq_length + 1,\n",
    "            size=(batch_size,),\n",
    "            device=self.storge_device,\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of seq_length, adding 0 to seq_length to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, self.seq_length) + torch.arange(\n",
    "            self.seq_length, device=self.storge_device\n",
    "        )\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, self.n_feature)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = x_sub.gather(dim=1, index=index_x), y_sub.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            x_batch,\n",
    "            y_batch[:, self.base_length :],\n",
    "            selected_catchment_index,\n",
    "        )\n",
    "\n",
    "    def get_val_batch(self):\n",
    "        n_years = math.ceil(\n",
    "            (self.record_length - self.base_length) / self.target_seq_length\n",
    "        )\n",
    "\n",
    "        out_x = (\n",
    "            torch.ones(\n",
    "                [n_years, self.n_catchment, self.seq_length, self.n_feature],\n",
    "                device=self.storge_device,\n",
    "            )\n",
    "            * torch.nan\n",
    "        )\n",
    "        out_y = (\n",
    "            torch.ones(\n",
    "                [n_years, self.n_catchment, self.seq_length], device=self.storge_device\n",
    "            )\n",
    "            * torch.nan\n",
    "        )\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = self.base_length * i\n",
    "\n",
    "            if i == n_years - 1:\n",
    "                end_record_ind = self.record_length\n",
    "\n",
    "                out_x[i, :, 0 : (end_record_ind - start_record_ind), :] = self.x[\n",
    "                    :, start_record_ind:end_record_ind, :\n",
    "                ]\n",
    "                out_y[i, :, 0 : (end_record_ind - start_record_ind)] = self.y[\n",
    "                    :, start_record_ind:end_record_ind\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                end_record_ind = start_record_ind + self.seq_length\n",
    "\n",
    "                out_x[i, :, :, :] = self.x[:, start_record_ind:end_record_ind, :]\n",
    "                out_y[i, :, :] = self.y[:, start_record_ind:end_record_ind]\n",
    "\n",
    "        return out_x, out_y[:, :, self.base_length :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Reference: https://stackoverflow.com/a/73704579/3361298\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    # Reference: https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(\n",
    "            -1, x.size(-1)\n",
    "        )  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(\n",
    "                x.size(0), -1, y.size(-1)\n",
    "            )  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        feature_dim,\n",
    "        lstm_hidden_dim,\n",
    "        fc_hidden_dims,\n",
    "        num_lstm_layers=1,\n",
    "        output_dim=1,\n",
    "        p=0.2,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.feature_dim + self.latent_dim,\n",
    "            self.lstm_hidden_dim,\n",
    "            num_layers=self.num_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # LSTM to latent code\n",
    "        self.fc_hidden_dims = fc_hidden_dims\n",
    "        self.fc_layers = []\n",
    "        self.p = p\n",
    "        for i in range((len(self.fc_hidden_dims))):\n",
    "            in_dim = self.lstm_hidden_dim if i == 0 else self.fc_hidden_dims[i - 1]\n",
    "            out_dim = self.fc_hidden_dims[i]\n",
    "\n",
    "            self.fc_layers += [nn.Linear(in_dim, out_dim)]\n",
    "            self.fc_layers += [nn.ReLU()]\n",
    "            self.fc_layers += [nn.Dropout(p=self.p)]\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.fc_layers += [nn.Linear(self.fc_hidden_dims[-1], self.output_dim)]\n",
    "\n",
    "        self.fc_layers = TimeDistributed(\n",
    "            nn.Sequential(*self.fc_layers), batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out, (_, _) = self.lstm(inputs)\n",
    "        out = self.fc_layers(out[:,BASE_LENGTH:,:])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def decode(self, code, x):\n",
    "        \n",
    "        code = code.expand(x.shape[1], -1, -1).transpose(0, 1)\n",
    "\n",
    "        x = torch.cat((code, x), 2)\n",
    "        out = self.forward(x).squeeze()\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch_gen = Random_Batch_Generator(\n",
    "#     Forcing_Data(\"./data/data_train_w_missing.csv\", record_length=7304),\n",
    "#     Random_Index_Generator(catchment_number=2346, record_length=7304),\n",
    "#     record_length=7304\n",
    "# )\n",
    "# train_val_batch_gen = Random_Batch_Generator(\n",
    "#     Forcing_Data(\"./data/Caravan/data3f_train_val.csv\", record_length=7669),\n",
    "#     Random_Index_Generator(catchment_number=749, record_length=7669),\n",
    "#     record_length=7669\n",
    "# )\n",
    "# val_batch_gen = Random_Batch_Generator(\n",
    "#     Forcing_Data(\"./data/data_val_w_missing.csv\", record_length=4017),\n",
    "#     Random_Index_Generator(catchment_number=2346, record_length=4017),\n",
    "#     record_length=4017\n",
    "# )\n",
    "# test_batch_gen = Random_Batch_Generator(\n",
    "#     Forcing_Data(\"./data/Caravan/data3f_test.csv\", record_length=4017),\n",
    "#     Random_Index_Generator(catchment_number=749, record_length=4017),\n",
    "#     record_length=4017\n",
    "# )\n",
    "\n",
    "dtrain = Forcing_Data(\n",
    "    \"data/data_train_w_missing.csv\",\n",
    "    record_length=7304,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dval = Forcing_Data(\n",
    "    \"data/data_val_w_missing.csv\",\n",
    "    record_length=4017,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_with_nans(input, target):\n",
    "    # Adapted from https://stackoverflow.com/a/59851632/3361298\n",
    "\n",
    "    # Missing data are nans\n",
    "    mask = torch.isnan(target)\n",
    "\n",
    "    out = (input[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def mse(target, pred): \n",
    "    mask = np.isnan(target)\n",
    "\n",
    "    out = (pred[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hydrogrpah(x, y, embedding, decoder):\n",
    "\n",
    "    # put model into evaluation mode\n",
    "    embedding, decoder = embedding.eval(), decoder.eval()\n",
    "\n",
    "    # prediction have the same shape as the observation y\n",
    "    preds = np.ones(y.shape)\n",
    "    record_length = y.shape[1]\n",
    "\n",
    "    # length of the target sequence in years\n",
    "    n_years = math.floor((record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "\n",
    "    for j in range(math.ceil(N_CATCHMENT / VAL_STEP_SIZE)):\n",
    "        # iterate over catchments, evaluate VAL_STEP_SIZE catchments each time\n",
    "        start_catchment_ind = j * VAL_STEP_SIZE\n",
    "        end_catchment_ind = min((j + 1) * VAL_STEP_SIZE, N_CATCHMENT)\n",
    "\n",
    "        # code of selected catchment\n",
    "        selected_catchments = torch.from_numpy(\n",
    "            np.arange(start_catchment_ind, end_catchment_ind)\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            code = embedding.forward(selected_catchments)\n",
    "\n",
    "        # iterate over different years\n",
    "        for k in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH + k * TARGET_SEQ_LENGTH\n",
    "            end_record_ind = start_record_ind + TARGET_SEQ_LENGTH\n",
    "\n",
    "            # dealing with the unique length of the last year record\n",
    "            if k == (n_years - 1):\n",
    "                end_record_ind = record_length\n",
    "\n",
    "            # subsetting, base length is included\n",
    "            x_sub = x[\n",
    "                start_catchment_ind:end_catchment_ind,\n",
    "                (start_record_ind - BASE_LENGTH) : end_record_ind,\n",
    "                :,\n",
    "            ].to(DEVICE)\n",
    "\n",
    "            # pass through decoder, and store the result into `preds`\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "                with torch.no_grad():\n",
    "                    pred = decoder.decode(code, x_sub)\n",
    "                    preds[\n",
    "                        start_catchment_ind:end_catchment_ind,\n",
    "                        start_record_ind:end_record_ind,\n",
    "                    ] = (\n",
    "                        pred.cpu().detach().numpy()\n",
    "                    )\n",
    "\n",
    "    return preds[:, BASE_LENGTH:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    \n",
    "    lstm_hidden_dim=trial.suggest_int(\"lstm_hidden_dim\", 4, 128)\n",
    "    n_lstm_layers=trial.suggest_int(\"n_lstm_layers\", 1, 2)\n",
    "    n_fc_layers=trial.suggest_int(\"n_fc_layers\", 1, 3)\n",
    "    LATENT_DIM_power = trial.suggest_int(\"LATENT_DIM_power\", 1, 2)\n",
    "    LATENT_DIM = 2**LATENT_DIM_power\n",
    "    \n",
    "    drop_out_flag = trial.suggest_categorical(\"drop_out_flag\", [True, False])\n",
    "    \n",
    "    if drop_out_flag:\n",
    "        p = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    else:\n",
    "        p = 0\n",
    "    \n",
    "    fc_hidden_dims = []\n",
    "    for i in range(n_fc_layers):\n",
    "        fc_dim = trial.suggest_int(f\"fc_dim{i}\", 4, 16)\n",
    "        fc_hidden_dims.append(fc_dim)\n",
    "        \n",
    "    decoder = Decoder(\n",
    "        latent_dim=LATENT_DIM,\n",
    "        feature_dim=FORCING_DIM,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        fc_hidden_dims=fc_hidden_dims,\n",
    "        num_lstm_layers=n_lstm_layers,\n",
    "        output_dim=1,\n",
    "        p=p,\n",
    "    )\n",
    "    \n",
    "    embedding = nn.Embedding(N_CATCHMENT, LATENT_DIM)\n",
    "    \n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(embedding, decoder, dataset, val_metric=mse):\n",
    "    \n",
    "    x, y = dataset.x, dataset.y\n",
    "    \n",
    "    preds = predict_hydrogrpah(x, y, embedding, decoder)\n",
    "    obs = y[:,BASE_LENGTH:].cpu().detach().numpy()\n",
    "\n",
    "    return val_metric(obs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(\n",
    "    embedding,\n",
    "    decoder,\n",
    "    dataset,\n",
    "    storge_device,\n",
    "    computing_device,\n",
    "    use_amp,\n",
    "    val_metric,\n",
    "    return_summary=True,\n",
    "):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = dataset.get_val_batch()\n",
    "\n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size=y.shape, device=storge_device)\n",
    "\n",
    "    n_catchments = y.shape[1]\n",
    "    selected_catchments = torch.arange(n_catchments, device=computing_device)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "        with torch.no_grad():\n",
    "            code = embedding(selected_catchments)\n",
    "            for i in range(x.shape[0]):\n",
    "                x_sub = x[i, :, :, :]\n",
    "                preds[i, :, :] = decoder.decode(code, x_sub)\n",
    "\n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out\n",
    "\n",
    "def val_model_mem_saving(\n",
    "    embedding,\n",
    "    decoder,\n",
    "    dataset,\n",
    "    storge_device,\n",
    "    computing_device,\n",
    "    use_amp,\n",
    "    val_metric,\n",
    "    return_summary,\n",
    "    val_steps,\n",
    "):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = dataset.get_val_batch()\n",
    "\n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size=y.shape, device=storge_device)\n",
    "    n_catchments = y.shape[1]\n",
    "\n",
    "    # iterate over years\n",
    "    for i in range(x.shape[0]):\n",
    "        # iterate over catchments\n",
    "        for j in range(math.ceil(n_catchments / val_steps)):\n",
    "            start_catchment_ind = j * val_steps\n",
    "            end_catchment_ind = min((j + 1) * val_steps, n_catchments)\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    code = embedding(\n",
    "                        torch.arange(\n",
    "                            start=start_catchment_ind,\n",
    "                            end=end_catchment_ind,\n",
    "                            device=computing_device,\n",
    "                        )\n",
    "                    )\n",
    "                    x_sub = x[i, start_catchment_ind:end_catchment_ind, :, :].to(\n",
    "                        computing_device\n",
    "                    )\n",
    "                    preds[i, start_catchment_ind:end_catchment_ind, :] = decoder.decode(\n",
    "                        code, x_sub\n",
    "                    ).to(storge_device)\n",
    "\n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # prepare early stopper\n",
    "    early_stopper = EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define optimizers\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 3, 8)\n",
    "    BATCH_SIZE = 2**batch_size_power\n",
    "    TRAIN_STEPS = round(\n",
    "        15 * N_CATCHMENT / BATCH_SIZE\n",
    "    )  # year * catchment number / batch size\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(TRAIN_STEPS):\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "\n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, selected_catchments) = dtrain.get_random_batch(\n",
    "                BATCH_SIZE\n",
    "            )\n",
    "            \n",
    "            x_batch, y_batch, selected_catchments = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "                selected_catchments.to(computing_device),\n",
    "            )\n",
    "\n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(selected_catchments)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                running_loss = running_loss + loss.cpu().detach().numpy()\n",
    "\n",
    "        losses.append(running_loss)\n",
    "\n",
    "        # # compute validation loss\n",
    "\n",
    "        # val_error = val_model(embedding, decoder, dval)\n",
    "        # val_losses.append(val_error)\n",
    "\n",
    "        # trial.report(val_error, epoch)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value.\n",
    "        # if trial.should_prune():\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # # Early stop using early_stopper, break for loop\n",
    "        # if early_stopper.early_stop(val_error):\n",
    "        #     break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return early_stopper.min_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    latent_dim=4,\n",
    "    feature_dim=FORCING_DIM,\n",
    "    lstm_hidden_dim=25,\n",
    "    fc_hidden_dims=[6],\n",
    "    num_lstm_layers=1,\n",
    "    output_dim=1,\n",
    "    p=0,\n",
    ")\n",
    "\n",
    "embedding = nn.Embedding(N_CATCHMENT, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val=5.017515659332275\n",
      "val=4.459843635559082\n",
      "val=4.1187262535095215\n",
      "val=4.044862270355225\n",
      "val=3.7769598960876465\n",
      "val=3.6994822025299072\n",
      "val=3.4810004234313965\n",
      "val=3.5082967281341553\n",
      "val=3.3294506072998047\n",
      "val=3.2971298694610596\n",
      "val=3.2659692764282227\n",
      "val=3.2660627365112305\n",
      "val=3.171774387359619\n",
      "val=3.209750175476074\n",
      "val=3.150386333465576\n",
      "val=3.0805978775024414\n",
      "val=3.1052029132843018\n",
      "val=3.0524260997772217\n",
      "val=3.075756072998047\n",
      "val=3.014307975769043\n",
      "val=3.0627918243408203\n",
      "val=3.010718822479248\n",
      "val=3.109849214553833\n",
      "val=2.924255609512329\n",
      "val=3.0679218769073486\n",
      "val=2.8861587047576904\n",
      "val=2.9174699783325195\n",
      "val=2.927917003631592\n",
      "val=2.9803483486175537\n",
      "val=2.9044899940490723\n",
      "val=2.8398683071136475\n",
      "val=2.899876356124878\n",
      "val=2.8597230911254883\n",
      "val=2.8272809982299805\n",
      "val=2.884828805923462\n",
      "val=2.9121720790863037\n",
      "val=2.7782766819000244\n",
      "val=2.8449289798736572\n",
      "val=2.8970274925231934\n",
      "val=2.777165412902832\n",
      "val=2.7890841960906982\n",
      "val=2.7966980934143066\n",
      "val=2.8034019470214844\n",
      "val=2.905522584915161\n",
      "val=2.791111707687378\n",
      "val=2.8000879287719727\n",
      "val=2.7663605213165283\n",
      "val=2.8679392337799072\n",
      "val=2.7630486488342285\n",
      "val=2.737060546875\n",
      "val=2.723775625228882\n",
      "val=2.742250919342041\n",
      "val=2.7635505199432373\n",
      "val=2.7494986057281494\n",
      "val=2.7410237789154053\n",
      "val=2.7613041400909424\n",
      "val=2.8715927600860596\n",
      "val=2.761867046356201\n",
      "val=2.7482903003692627\n",
      "val=2.8178930282592773\n",
      "val=2.7142319679260254\n",
      "val=2.7476372718811035\n",
      "val=2.7168843746185303\n",
      "val=2.7737958431243896\n",
      "val=2.7284419536590576\n",
      "val=2.744492530822754\n",
      "val=2.842780113220215\n",
      "val=2.7516543865203857\n",
      "val=2.739737033843994\n",
      "val=2.7464165687561035\n",
      "val=2.714879035949707\n",
      "val=2.7720377445220947\n",
      "val=2.727693796157837\n",
      "val=2.7684123516082764\n",
      "val=2.7054998874664307\n",
      "val=2.6989328861236572\n",
      "val=2.6826887130737305\n",
      "val=2.719835042953491\n",
      "val=2.7397642135620117\n",
      "val=2.7135510444641113\n",
      "val=2.745074510574341\n",
      "val=2.7403879165649414\n",
      "val=2.717996835708618\n",
      "val=2.7342731952667236\n",
      "val=2.727562427520752\n",
      "val=2.744229555130005\n",
      "val=2.7134885787963867\n",
      "val=2.716996192932129\n",
      "val=2.7214856147766113\n",
      "val=2.719160795211792\n",
      "val=2.748126983642578\n",
      "val=2.7350282669067383\n",
      "val=2.7234973907470703\n",
      "val=2.7894811630249023\n",
      "val=2.706756353378296\n",
      "val=2.716766834259033\n",
      "val=2.7305240631103516\n",
      "val=2.752450466156006\n",
      "val=2.705754041671753\n",
      "val=2.757385730743408\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# define model\n",
    "embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "# define batch size\n",
    "batch_size_power = 6\n",
    "BATCH_SIZE = 2** batch_size_power\n",
    "TRAIN_STEPS = round(15*N_CATCHMENT/BATCH_SIZE) # catchment number * year / batch si\n",
    "\n",
    "# define optimizers\n",
    "lr = 0.0017145938261308676\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "\n",
    "# train model\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i in range(TRAIN_STEPS):\n",
    "\n",
    "        decoder_optimizer.zero_grad()\n",
    "        embedding_optimizer.zero_grad()\n",
    "\n",
    "        # put the models into training mode\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "\n",
    "        # get training batch and pass to device\n",
    "        (x_batch, y_batch, selected_catchments) = dtrain.get_random_batch(\n",
    "            BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        x_batch, y_batch, selected_catchments = (\n",
    "            x_batch.to(computing_device),\n",
    "            y_batch.to(computing_device),\n",
    "            selected_catchments.to(computing_device),\n",
    "        )\n",
    "\n",
    "        # slice batch for training\n",
    "        with torch.autocast(\n",
    "            device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "        ):\n",
    "            code = embedding(selected_catchments)\n",
    "\n",
    "            # pass through decoder\n",
    "            out = decoder.decode(code, x_batch)\n",
    "\n",
    "            # compute loss\n",
    "            loss = mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(embedding_optimizer)\n",
    "        scaler.step(decoder_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = running_loss + loss.cpu().detach().numpy()\n",
    "\n",
    "    losses.append(running_loss)\n",
    "\n",
    "    # put the model in to evaluation model, compute validation loss\n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Handle pruning based on the intermediate value\n",
    "    if memory_saving:\n",
    "        val_loss = val_model_mem_saving(\n",
    "            embedding=embedding,\n",
    "            decoder=decoder,\n",
    "            dataset=dval,\n",
    "            storge_device=storge_device,\n",
    "            computing_device=computing_device,\n",
    "            use_amp=use_amp,\n",
    "            val_metric=mse_loss_with_nans,\n",
    "            return_summary=True,\n",
    "            val_steps=VAL_STEPS,\n",
    "        )\n",
    "    else:\n",
    "        val_loss = (\n",
    "            val_model(\n",
    "                embedding=embedding,\n",
    "                decoder=decoder,\n",
    "                dataset=dval,\n",
    "                storge_device=storge_device,\n",
    "                computing_device=computing_device,\n",
    "                use_amp=use_amp,\n",
    "                val_metric=mse_loss_with_nans,\n",
    "                return_summary=True,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    val_losses.append(val_loss)\n",
    "            \n",
    "    losses.append(val_loss)\n",
    "    \n",
    "    print(f\"val={val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 365])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_hidden_dim': 25,\n",
       " 'n_lstm_layers': 1,\n",
       " 'n_fc_layers': 1,\n",
       " 'LATENT_DIM_power': 2,\n",
       " 'drop_out_flag': False,\n",
       " 'fc_dim0': 6,\n",
       " 'lr': 0.0017145938261308676,\n",
       " 'batch_size_power': 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-17 09:27:49,833]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "\u001b[32m[I 2022-12-17 14:43:44,995]\u001b[0m Trial 0 finished with value: 3.853461273099789 and parameters: {'lstm_hidden_dim': 45, 'n_lstm_layers': 1, 'n_fc_layers': 3, 'LATENT_DIM_power': 1, 'drop_out_flag': True, 'dropout_rate': 0.41512299159634236, 'fc_dim0': 14, 'fc_dim1': 13, 'fc_dim2': 16, 'lr': 1.7873488949281057e-05, 'batch_size_power': 5}. Best is trial 0 with value: 3.853461273099789.\u001b[0m\n",
      "\u001b[33m[W 2022-12-17 15:02:21,274]\u001b[0m Trial 1 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_4187/3481565355.py\", line 61, in objective\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 363, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mNopPruner())\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     57\u001b[0m a, b \u001b[38;5;241m=\u001b[39m out[:, (SEQ_LENGTH \u001b[38;5;241m-\u001b[39m TARGET_SEQ_LENGTH) :], y0[:, (SEQ_LENGTH \u001b[38;5;241m-\u001b[39m TARGET_SEQ_LENGTH) :]\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss_with_nans(a, b)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m embedding_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     63\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner())\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_epochs(study):\n",
    "    \n",
    "    stats = study.best_trials[0].intermediate_values\n",
    "    EPOCHS = min(stats, key=lambda k: stats[k]) + 1\n",
    "    \n",
    "    return EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"data/Caravan/3f_study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_models(loaded_study):\n",
    "    \n",
    "    EPOCHS = get_optimal_epochs(loaded_study)\n",
    "    trial = loaded_study.best_trials[0]\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder  = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "    BATCH_SIZE = 2** batch_size_power\n",
    "    TRAIN_STEPS = round(15*N_CATCHMENT/BATCH_SIZE) # catchment number * year / batch si\n",
    "\n",
    "    # define optimizers\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(TRAIN_STEPS):\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "\n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            (xs_batch, ys_batch, selected_catchments) = train_val_batch_gen.get_batch(\n",
    "                n_seq=N_SEQ, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH\n",
    "            )\n",
    "\n",
    "            # divide batch and pass to device\n",
    "            xs_batch, ys_batch, selected_catchments = (\n",
    "                xs_batch.to(DEVICE),\n",
    "                ys_batch.to(DEVICE),\n",
    "                selected_catchments.to(DEVICE),\n",
    "            )\n",
    "            \n",
    "            # slice batch for training\n",
    "            x0, y0 = (xs_batch[0, :, :, :], ys_batch[0, :, :])\n",
    "            code = embedding(selected_catchments)\n",
    "\n",
    "            # pass through decoder\n",
    "            out = decoder.decode(code, x0)\n",
    "\n",
    "            # backprop\n",
    "            a, b = out[:, (SEQ_LENGTH - TARGET_SEQ_LENGTH) :], y0[:, (SEQ_LENGTH - TARGET_SEQ_LENGTH) :],\n",
    "            \n",
    "            loss = criterion(a,b)\n",
    "\n",
    "            loss.backward()\n",
    "            embedding_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                running_loss = running_loss + loss.cpu().detach().numpy()\n",
    "\n",
    "        # put the model in to evaluation model, compute validation loss\n",
    "        embedding.eval()\n",
    "        decoder.eval()\n",
    "                \n",
    "        losses.append(running_loss)\n",
    "\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, decoder = get_final_models(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding.cpu(), \"data/Caravan/embedding.pt\")\n",
    "torch.save(decoder.cpu(), \"data/Caravan/decoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lstm): LSTM(7, 25, batch_first=True)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): TimeDistributed(\n",
       "      (module): Linear(in_features=25, out_features=6, bias=True)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): TimeDistributed(\n",
       "      (module): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (3): TimeDistributed(\n",
       "      (module): Linear(in_features=6, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.to(DEVICE)\n",
    "decoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y_true = val_model(embedding, decoder, test_batch_gen, return_summary = False)\n",
    "preds, y_true = preds.cpu().numpy(), y_true.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8813870947642763"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_gen=val_batch_gen\n",
    "\n",
    "embedding = torch.load(\"data/Caravan/embedding.pt\", map_location=torch.device('cpu'))\n",
    "decoder = torch.load(\"data/Caravan/decoder.pt\", map_location=torch.device('cpu'))\n",
    "embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "val_model(embedding, decoder, batch_gen, val_metric=mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HydroErr\n",
    "\n",
    "gofs = np.ones(preds.shape[0])\n",
    "kges = np.ones(preds.shape[0])\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "    gofs[i] = r2_score(y_true = y_true[i,:],y_pred=preds[i,:])\n",
    "    kges[i] = HydroErr.kge_2009(simulated_array=preds[i,:], observed_array=y_true[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(gofs[gofs>0], bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(kges[kges>0], bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6425069517130905"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kges.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./data/CAMELS_US/kges.csv\", kges, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "plt.plot(y_true[i,:])\n",
    "plt.plot(preds[i,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(math.ceil(N_CATCHMENT / 100)):\n",
    "    # iterate over catchment\n",
    "    start_catchment_ind = j * 100  # evaluate 100 catchments each time\n",
    "    end_catchment_ind = min((j + 1) * 100, N_CATCHMENT)\n",
    "\n",
    "    x_val, y_val = val_batch_gen.get_batch(\n",
    "        selected_catchments=[\n",
    "            a for a in range(start_catchment_ind, end_catchment_ind)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\n",
    "\n",
    "    code = embedding(\n",
    "        torch.from_numpy(np.arange(start_catchment_ind, end_catchment_ind)).to(DEVICE)\n",
    "    )\n",
    "\n",
    "    x_val = x_val.transpose(0, 1)\n",
    "\n",
    "    for k in range(x_val.shape[0]):\n",
    "        # iterate over each year\n",
    "        start_record_ind = (k + 1) * (SEQ_LENGTH - TARGET_SEQ_LENGTH)\n",
    "        end_record_ind = start_record_ind + TARGET_SEQ_LENGTH\n",
    "\n",
    "        x_val_sub = x_val[k, :, :, :].squeeze()\n",
    "        preds[\n",
    "            start_catchment_ind:end_catchment_ind,\n",
    "            start_record_ind:end_record_ind,\n",
    "        ] = decoder.decode(code, x_val_sub)[\n",
    "            :, (SEQ_LENGTH - TARGET_SEQ_LENGTH) :\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[start_catchment_ind:end_catchment_ind, start_record_ind:end_record_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.ones(val_batch_gen.dataset.y.shape)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        (xs_batch, ys_batch, selected_catchments) = train_batch_gen.get_batch(\n",
    "            n_seq=n_seq, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        # divide batch and pass to device\n",
    "\n",
    "        xs_batch, ys_batch, selected_catchments = xs_batch.to(DEVICE), ys_batch.to(DEVICE), selected_catchments.to(DEVICE)\n",
    "        \n",
    "        x0, y0 = (\n",
    "            xs_batch[0, :, :, :],\n",
    "            ys_batch[0, :, :]\n",
    "        )\n",
    "\n",
    "        code = embedding(selected_catchments)\n",
    "\n",
    "        # pass through decoder\n",
    "        out = decoder.decode(code, x0).squeeze()\n",
    "    \n",
    "data_plot = out.cpu().detach().numpy()\n",
    "\n",
    "plt.plot(data_plot[1, (SEQ_LENGTH - TARGET_SEQ_LENGTH) : ])\n",
    "plt.plot(y0[1, (SEQ_LENGTH-TARGET_SEQ_LENGTH):].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_plot[24, (SEQ_LENGTH - TARGET_SEQ_LENGTH) : ])\n",
    "plt.plot(y0[24, (SEQ_LENGTH-TARGET_SEQ_LENGTH):].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y0[24, (SEQ_LENGTH-TARGET_SEQ_LENGTH):].cpu().detach().numpy(), y_pred=data_plot[24, (SEQ_LENGTH - TARGET_SEQ_LENGTH) : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(code[:,0].cpu().detach().numpy(), code[:,1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding, \"data/Caravan/embedding.pt\")\n",
    "torch.save(decoder, \"data/Caravan/decoder.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
