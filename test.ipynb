{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import tcn\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENT = 2346\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "TRAIN_YEAR = 19\n",
    "\n",
    "PATIENCE = 50\n",
    "\n",
    "dtypes = defaultdict(lambda: float)\n",
    "dtypes[\"catchment_id\"] = str\n",
    "\n",
    "# training hyperparameters\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "    ):\n",
    "        data_raw = np.genfromtxt(fpath, delimiter=\",\", skip_header=1)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = torch.from_numpy(data_raw[:, 0:3])\n",
    "        x = x.view(-1, record_length, n_feature).contiguous()\n",
    "        self.x = x.to(storge_device)\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = torch.from_numpy(data_raw[:,3])\n",
    "        y = y.view(-1, record_length).contiguous()\n",
    "        self.y = y.to(storge_device)\n",
    "\n",
    "        self.record_length = self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # SEQ_LENGTH, BASE_LENGTH, and DEVICE is from global\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - SEQ_LENGTH + 1,\n",
    "            size=(N_CATCHMENT,),\n",
    "            device=storge_device,\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of SEQ_LENGTH, adding 0 to SEQ_LENGTH to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, SEQ_LENGTH) + torch.arange(\n",
    "            SEQ_LENGTH, device=storge_device\n",
    "        )\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, FORCING_DIM)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = self.x.gather(dim=1, index=index_x), self.y.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return x_batch, y_batch[:, BASE_LENGTH:]\n",
    "\n",
    "    def get_val_batch(self):\n",
    "        n_years = math.ceil((self.record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "\n",
    "        out_x = (\n",
    "            torch.ones(\n",
    "                [n_years, N_CATCHMENT, SEQ_LENGTH, FORCING_DIM], device=storge_device\n",
    "            )\n",
    "            * torch.nan\n",
    "        )\n",
    "        out_y = (\n",
    "            torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH], device=storge_device)\n",
    "            * torch.nan\n",
    "        )\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH * i\n",
    "\n",
    "            if i == n_years - 1:\n",
    "                end_record_ind = self.record_length\n",
    "\n",
    "                out_x[i, :, 0 : (end_record_ind - start_record_ind), :] = self.x[\n",
    "                    :, start_record_ind:end_record_ind, :\n",
    "                ]\n",
    "                out_y[i, :, 0 : (end_record_ind - start_record_ind)] = self.y[\n",
    "                    :, start_record_ind:end_record_ind\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                end_record_ind = start_record_ind + SEQ_LENGTH\n",
    "\n",
    "                out_x[i, :, :, :] = self.x[:, start_record_ind:end_record_ind, :]\n",
    "                out_y[i, :, :] = self.y[:, start_record_ind:end_record_ind]\n",
    "\n",
    "        return out_x, out_y[:, :, BASE_LENGTH:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = Forcing_Data(\"data/data_train_w_missing.csv\", record_length=7304)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2346, 7304, 3]), torch.Size([2346, 7304]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.x.shape, dtrain.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3000, dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.y[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
