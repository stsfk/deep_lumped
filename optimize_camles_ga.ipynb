{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "95vm9FxGQ7KK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "import time\n",
        "\n",
        "import dataloader\n",
        "import models\n",
        "import training_fun\n",
        "\n",
        "import optuna\n",
        "\n",
        "import joblib\n",
        "\n",
        "import pygad\n",
        "\n",
        "import HydroErr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oJcpZUdpp9K3"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEQ_LENGTH = 365 * 2\n",
        "TARGET_SEQ_LENGTH = 365\n",
        "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
        "\n",
        "FORCING_DIM = 3\n",
        "\n",
        "N_CATCHMENTS = 559\n",
        "\n",
        "# training hyperparameters\n",
        "EPOCHS = 500\n",
        "TRAIN_YEAR = 19\n",
        "PATIENCE = 20\n",
        "\n",
        "use_amp = True\n",
        "compile_model = False\n",
        "\n",
        "if compile_model:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "memory_saving = False\n",
        "if memory_saving:\n",
        "    storge_device = \"cpu\"\n",
        "    computing_device = DEVICE\n",
        "    VAL_STEPS = 500\n",
        "else:\n",
        "    storge_device = DEVICE\n",
        "    computing_device = DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ceCEmnygRmye"
      },
      "outputs": [],
      "source": [
        "embedding = torch.load(\"data/final_lstm_embedding2.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "decoder = torch.load(\"data/final_lstm_decoder2.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "\n",
        "embedding.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# dimension of embedding\n",
        "catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
        "LATENT_dim = catchment_embeddings.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nGxVHBn3p9K4"
      },
      "outputs": [],
      "source": [
        "dtrain_val = dataloader.Forcing_Data(\n",
        "    \"data/camels_train_val.csv\",\n",
        "    record_length=3652,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dtrain = dataloader.Forcing_Data(\n",
        "    \"data/camels_train.csv\",\n",
        "    record_length=2922,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dval = dataloader.Forcing_Data(\n",
        "    \"data/camels_val.csv\",\n",
        "    record_length=1095,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dtest = dataloader.Forcing_Data(\n",
        "    \"data/camels_test.csv\",\n",
        "    record_length=4383,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dw8xH6sT3qQN"
      },
      "outputs": [],
      "source": [
        "class Objective_builder:\n",
        "    def __init__(self, x, y, eval_fun):\n",
        "        self.eval_fun = eval_fun\n",
        "        self.x = x.contiguous()\n",
        "        self.y = y.contiguous()\n",
        "    \n",
        "    def eval(self, ga_instance, solution, solution_idx):\n",
        "        \n",
        "        # numpy to torch tensor\n",
        "        solution = torch.from_numpy(solution).unsqueeze(0).to(dtype=torch.float32).to(computing_device)\n",
        "        solution = solution.expand(self.x.shape[0], -1)\n",
        "        \n",
        "        # BASE_LENGTH is from global\n",
        "        pred = decoder.decode(solution, self.x).view(-1).detach().cpu().numpy()\n",
        "\n",
        "        ob = self.y.view(-1).detach().cpu().numpy()\n",
        "        \n",
        "        gof = self.eval_fun(simulated_array=pred, observed_array=ob)\n",
        "        \n",
        "        return gof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_batch_train, y_batch_train = dtrain.get_val_batch()\n",
        "x_batch_val, y_batch_val = dval.get_val_batch()\n",
        "x_batch_train_val, y_batch_train_val = dtrain_val.get_val_batch()\n",
        "x_batch_test, y_batch_test = dtest.get_val_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters of GA\n",
        "num_generations = 500\n",
        "num_parents_mating = 10\n",
        "\n",
        "sol_per_pop = 100\n",
        "num_genes = LATENT_dim\n",
        "\n",
        "init_range_low = catchment_embeddings.detach().cpu().min().numpy().tolist()\n",
        "init_range_high = catchment_embeddings.detach().cpu().max().numpy().tolist()\n",
        "\n",
        "parent_selection_type = \"sss\"\n",
        "\n",
        "crossover_type = \"single_point\"\n",
        "\n",
        "mutation_type = \"random\"\n",
        "mutation_probability = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fitting_wrapper(selected_catchment):\n",
        "\n",
        "    # Subsetting training, validation, and test data of selected catchments\n",
        "    x_train = x_batch_train[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_train = y_batch_train[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_val = x_batch_val[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_val = y_batch_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_train_val = x_batch_train_val[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_train_val = y_batch_train_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_test = x_batch_test[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_test = y_batch_test[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    # Creating evaluation functions\n",
        "    fn_train = Objective_builder(x_train,y_train,HydroErr.kge_2009)\n",
        "    fn_val = Objective_builder(x_val,y_val,HydroErr.kge_2009)\n",
        "    fn_train_val = Objective_builder(x_train_val,y_train_val,HydroErr.kge_2009)\n",
        "    fn_test = Objective_builder(x_test,y_test,HydroErr.kge_2009)\n",
        "\n",
        "    # Setting up callback functions for early stop\n",
        "    early_stopper = training_fun.EarlyStopper(patience=20)\n",
        "    val_losses = []\n",
        "\n",
        "    def on_generation(instance):\n",
        "        \n",
        "        solution, solution_fitness, solution_idx = instance.best_solution()\n",
        "        val_loss = fn_val.eval(instance, solution, solution_idx)\n",
        "        \n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        if early_stopper.early_stop(-val_loss):\n",
        "            return \"stop\"\n",
        "        else:\n",
        "            return val_loss\n",
        "\n",
        "    # Identifying optimal number of generations\n",
        "    ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                        num_parents_mating=num_parents_mating,\n",
        "                        fitness_func=fn_train_val.eval,\n",
        "                        sol_per_pop=sol_per_pop,\n",
        "                        num_genes=num_genes,\n",
        "                        init_range_low=init_range_low,\n",
        "                        init_range_high=init_range_high,\n",
        "                        parent_selection_type=parent_selection_type,\n",
        "                        crossover_type=crossover_type,\n",
        "                        mutation_type=mutation_type,\n",
        "                        mutation_probability = mutation_probability,\n",
        "                        stop_criteria=\"saturation_5\")\n",
        "\n",
        "    ga_instance.run()\n",
        "\n",
        "    # Evaluating best solution\n",
        "    #solution = ga_instance.best_solutions[np.argmax(val_losses),:]\n",
        "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "\n",
        "    return fn_test.eval(ga_instance, solution, 1), solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i=0 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570\n",
            " 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584\n",
            " 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598\n",
            " 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612\n",
            " 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626\n",
            " 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640\n",
            " 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654\n",
            " 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668\n",
            " 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682\n",
            " 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696\n",
            " 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710\n",
            " 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724\n",
            " 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738\n",
            " 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752\n",
            " 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766\n",
            " 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780\n",
            " 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794\n",
            " 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808\n",
            " 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822\n",
            " 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836\n",
            " 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850\n",
            " 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864\n",
            " 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878\n",
            " 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892\n",
            " 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906\n",
            " 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.7434932789681079\n",
            "i=1 starts\n",
            "fit=0.8189707255719877\n",
            "i=2 starts\n",
            "fit=0.8963877406001399\n",
            "i=3 starts\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_CATCHMENTS):\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mi=\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m starts\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     calibrated_KGES[i], camels_embeddings[i,:]  \u001b[39m=\u001b[39m fitting_wrapper(i)\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfit=\u001b[39m\u001b[39m{\u001b[39;00mcalibrated_KGES[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mfitting_wrapper\u001b[0;34m(selected_catchment)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Identifying optimal number of generations\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ga_instance \u001b[39m=\u001b[39m pygad\u001b[39m.\u001b[39mGA(num_generations\u001b[39m=\u001b[39mnum_generations,\n\u001b[1;32m     40\u001b[0m                     num_parents_mating\u001b[39m=\u001b[39mnum_parents_mating,\n\u001b[1;32m     41\u001b[0m                     fitness_func\u001b[39m=\u001b[39mfn_train\u001b[39m.\u001b[39meval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     save_best_solutions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m                     suppress_warnings \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m ga_instance\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     56\u001b[0m \u001b[39m# Evaluating best solution\u001b[39;00m\n\u001b[1;32m     57\u001b[0m solution \u001b[39m=\u001b[39m ga_instance\u001b[39m.\u001b[39mbest_solutions[np\u001b[39m.\u001b[39margmax(val_losses),:]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/pygad/pygad.py:2084\u001b[0m, in \u001b[0;36mGA.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[39m# If the on_generation attribute is not None, then cal the callback function after the generation.\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_generation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 2084\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_generation(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   2085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(r) \u001b[39mis\u001b[39;00m \u001b[39mstr\u001b[39m \u001b[39mand\u001b[39;00m r\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2086\u001b[0m         \u001b[39m# Before aborting the loop, save the fitness value of the best solution.\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m         \u001b[39m# _, best_solution_fitness, _ = self.best_solution()\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_solutions_fitness\u001b[39m.\u001b[39mappend(best_solution_fitness)\n",
            "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mfitting_wrapper.<locals>.on_generation\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_generation\u001b[39m(instance):\n\u001b[0;32m---> 28\u001b[0m     solution, solution_fitness, solution_idx \u001b[39m=\u001b[39m instance\u001b[39m.\u001b[39;49mbest_solution()\n\u001b[1;32m     29\u001b[0m     val_loss \u001b[39m=\u001b[39m fn_val\u001b[39m.\u001b[39meval(instance, solution, solution_idx)\n\u001b[1;32m     31\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_loss)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/pygad/pygad.py:2154\u001b[0m, in \u001b[0;36mGA.best_solution\u001b[0;34m(self, pop_fitness)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2152\u001b[0m     \u001b[39mif\u001b[39;00m pop_fitness \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2153\u001b[0m         \u001b[39m# If the 'pop_fitness' parameter is not passed, then we have to call the 'cal_pop_fitness()' method to calculate the fitness of all solutions in the lastest population.\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m         pop_fitness \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcal_pop_fitness()\n\u001b[1;32m   2155\u001b[0m     \u001b[39m# Verify the type of the 'pop_fitness' parameter.\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(pop_fitness) \u001b[39min\u001b[39;00m [\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m, numpy\u001b[39m.\u001b[39mndarray]:\n\u001b[1;32m   2157\u001b[0m         \u001b[39m# Verify that the length of the passed population fitness matches the length of the 'self.population' attribute.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/pygad/pygad.py:1669\u001b[0m, in \u001b[0;36mGA.cal_pop_fitness\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# Check if batch processing is used. If not, then calculate this missing fitness value.\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfitness_batch_size \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m]:\n\u001b[0;32m-> 1669\u001b[0m         fitness \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitness_func(\u001b[39mself\u001b[39;49m, sol, sol_idx)\n\u001b[1;32m   1670\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(fitness) \u001b[39min\u001b[39;00m GA\u001b[39m.\u001b[39msupported_int_float_types:\n\u001b[1;32m   1671\u001b[0m             \u001b[39mpass\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mObjective_builder.eval\u001b[0;34m(self, ga_instance, solution, solution_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m solution \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# BASE_LENGTH is from global\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pred \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(solution, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m ob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m gof \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_fun(simulated_array\u001b[39m=\u001b[39mpred, observed_array\u001b[39m=\u001b[39mob)\n",
            "File \u001b[0;32m~/Documents/projects/deep_lumped/models.py:92\u001b[0m, in \u001b[0;36mLSTM_decoder.decode\u001b[0;34m(self, code, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m code \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mexpand(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((code, x), \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/Documents/projects/deep_lumped/models.py:82\u001b[0m, in \u001b[0;36mLSTM_decoder.forward\u001b[0;34m(self, inputs, base_length)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, base_length\u001b[39m=\u001b[39m\u001b[39m365\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     out, (_, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(inputs)\n\u001b[1;32m     83\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_layers(out[:, base_length:, :])\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "N_CATCHMENTS = 559\n",
        "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
        "camels_embeddings = np.ones([N_CATCHMENTS, LATENT_dim])\n",
        "\n",
        "for i in range(N_CATCHMENTS):\n",
        "    print(f'i={i} starts')\n",
        "    calibrated_KGES[i], camels_embeddings[i,:]  = fitting_wrapper(i)\n",
        "    print(f'fit={calibrated_KGES[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_catchment = 27\n",
        "\n",
        "# Subsetting training, validation, and test data of selected catchments\n",
        "x_train = x_batch_train[:,selected_catchment,:,:].to(computing_device)\n",
        "y_train = y_batch_train[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_val = x_batch_val[:,selected_catchment,:,:].to(computing_device)\n",
        "y_val = y_batch_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_train_val = x_batch_train_val[:,selected_catchment,:,:].to(computing_device)\n",
        "y_train_val = y_batch_train_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_test = x_batch_test[:,selected_catchment,:,:].to(computing_device)\n",
        "y_test = y_batch_test[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "# Creating evaluation functions\n",
        "fn_train = Objective_builder(x_train,y_train,HydroErr.kge_2009)\n",
        "fn_val = Objective_builder(x_val,y_val,HydroErr.kge_2009)\n",
        "fn_train_val = Objective_builder(x_train_val,y_train_val,HydroErr.kge_2009)\n",
        "fn_test = Objective_builder(x_test,y_test,HydroErr.kge_2009)\n",
        "\n",
        "# Setting up callback functions for early stop\n",
        "early_stopper = training_fun.EarlyStopper(patience=20)\n",
        "val_losses = []\n",
        "\n",
        "def on_generation(ga_instance):\n",
        "    \n",
        "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "    val_loss = fn_val.eval(ga_instance, solution, solution_idx)\n",
        "    \n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    if early_stopper.early_stop(-val_loss):\n",
        "        return \"stop\"\n",
        "    else:\n",
        "        return val_loss\n",
        "\n",
        "# Identifying optimal number of generations\n",
        "ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                    num_parents_mating=num_parents_mating,\n",
        "                    fitness_func=fn_train.eval,\n",
        "                    sol_per_pop=sol_per_pop,\n",
        "                    num_genes=num_genes,\n",
        "                    init_range_low=init_range_low,\n",
        "                    init_range_high=init_range_high,\n",
        "                    parent_selection_type=parent_selection_type,\n",
        "                    crossover_type=crossover_type,\n",
        "                    mutation_type=mutation_type,\n",
        "                    mutation_probability = mutation_probability,\n",
        "                    on_generation = on_generation,\n",
        "                    save_best_solutions=True,\n",
        "                    suppress_warnings = True)\n",
        "\n",
        "ga_instance.run()\n",
        "\n",
        "# Running optimization\n",
        "ga_instance2 = pygad.GA(num_generations= np.argmax(val_losses)+1,#np.argmax(val_losses)+1,\n",
        "                    num_parents_mating=num_parents_mating,\n",
        "                    fitness_func=fn_train_val.eval,\n",
        "                    sol_per_pop=sol_per_pop,\n",
        "                    num_genes=num_genes,\n",
        "                    init_range_low=init_range_low,\n",
        "                    init_range_high=init_range_high,\n",
        "                    parent_selection_type=parent_selection_type,\n",
        "                    crossover_type=crossover_type,\n",
        "                    mutation_type=mutation_type,\n",
        "                    mutation_probability = mutation_probability,\n",
        "                    suppress_warnings = True)\n",
        "\n",
        "ga_instance2.run()\n",
        "\n",
        "# Evaluating best solution\n",
        "solution, solution_fitness, solution_idx = ga_instance2.best_solution()\n",
        "\n",
        "fn_test.eval(ga_instance2, solution, solution_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fitting_wrapper(selected_catchment):\n",
        "\n",
        "    # Subsetting training, validation, and test data of selected catchments\n",
        "    x_train = x_batch_train[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_train = y_batch_train[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_val = x_batch_val[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_val = y_batch_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_train_val = x_batch_train_val[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_train_val = y_batch_train_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    x_test = x_batch_test[:,selected_catchment,:,:].to(computing_device)\n",
        "    y_test = y_batch_test[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "    # Creating evaluation functions\n",
        "    fn_train = Objective_builder(x_train,y_train,HydroErr.kge_2009)\n",
        "    fn_val = Objective_builder(x_val,y_val,HydroErr.kge_2009)\n",
        "    fn_train_val = Objective_builder(x_train_val,y_train_val,HydroErr.kge_2009)\n",
        "    fn_test = Objective_builder(x_test,y_test,HydroErr.kge_2009)\n",
        "\n",
        "    # Setting up callback functions for early stop\n",
        "    early_stopper = training_fun.EarlyStopper(patience=20)\n",
        "    val_losses = []\n",
        "\n",
        "    def on_generation(instance):\n",
        "        \n",
        "        solution, solution_fitness, solution_idx = instance.best_solution()\n",
        "        val_loss = fn_val.eval(instance, solution, solution_idx)\n",
        "        \n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        if early_stopper.early_stop(-val_loss):\n",
        "            return \"stop\"\n",
        "        else:\n",
        "            return val_loss\n",
        "\n",
        "    # Identifying optimal number of generations\n",
        "    ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                        num_parents_mating=num_parents_mating,\n",
        "                        fitness_func=fn_train.eval,\n",
        "                        sol_per_pop=sol_per_pop,\n",
        "                        num_genes=num_genes,\n",
        "                        init_range_low=init_range_low,\n",
        "                        init_range_high=init_range_high,\n",
        "                        parent_selection_type=parent_selection_type,\n",
        "                        crossover_type=crossover_type,\n",
        "                        mutation_type=mutation_type,\n",
        "                        mutation_probability = mutation_probability,\n",
        "                        on_generation = on_generation,\n",
        "                        save_best_solutions=True,\n",
        "                        suppress_warnings = True)\n",
        "\n",
        "    ga_instance.run()\n",
        "\n",
        "    # Evaluating best solution\n",
        "    solution = ga_instance.best_solutions[np.argmax(val_losses),:]\n",
        "\n",
        "    return fn_test.eval(ga_instance, solution, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5910463366282108"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution = ga_instance.best_solutions[np.argmax(val_losses),:]\n",
        "fn_test.eval(ga_instance2, solution, solution_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570\n",
            " 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584\n",
            " 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598\n",
            " 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612\n",
            " 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626\n",
            " 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640\n",
            " 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654\n",
            " 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668\n",
            " 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682\n",
            " 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696\n",
            " 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710\n",
            " 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724\n",
            " 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738\n",
            " 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752\n",
            " 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766\n",
            " 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780\n",
            " 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794\n",
            " 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808\n",
            " 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822\n",
            " 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836\n",
            " 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850\n",
            " 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864\n",
            " 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878\n",
            " 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892\n",
            " 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906\n",
            " 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        }
      ],
      "source": [
        "outs = []\n",
        "for i in range(5):\n",
        "    outs.append(fitting_wrapper(27))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5724215071521541,\n",
              " 0.5441049669838935,\n",
              " 0.6259962547732397,\n",
              " 0.7161652441021733,\n",
              " 0.6551835428923383]"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5793932045481927,\n",
              " 0.565785842066912,\n",
              " 0.38353897448440166,\n",
              " 0.07107572840859677,\n",
              " 0.22625580029192005]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.606491755884658"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(val_losses)+1\n",
        "max(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5024920678421618"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "selected_catchment = 28\n",
        "\n",
        "# Subsetting training, validation, and test data of selected catchments\n",
        "x_train = x_batch_train[:,selected_catchment,:,:].to(computing_device)\n",
        "y_train = y_batch_train[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_val = x_batch_val[:,selected_catchment,:,:].to(computing_device)\n",
        "y_val = y_batch_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_train_val = x_batch_train_val[:,selected_catchment,:,:].to(computing_device)\n",
        "y_train_val = y_batch_train_val[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "x_test = x_batch_test[:,selected_catchment,:,:].to(computing_device)\n",
        "y_test = y_batch_test[:,selected_catchment,:].to(computing_device)\n",
        "\n",
        "# Creating evaluation functions\n",
        "fn_train = Objective_builder(x_train,y_train,HydroErr.kge_2009)\n",
        "fn_val = Objective_builder(x_val,y_val,HydroErr.kge_2009)\n",
        "fn_train_val = Objective_builder(x_train_val,y_train_val,HydroErr.kge_2009)\n",
        "fn_test = Objective_builder(x_test,y_test,HydroErr.kge_2009)\n",
        "\n",
        "ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                    num_parents_mating=num_parents_mating,\n",
        "                    fitness_func=fn_train_val.eval,\n",
        "                    sol_per_pop=sol_per_pop,\n",
        "                    num_genes=num_genes,\n",
        "                    init_range_low=init_range_low,\n",
        "                    init_range_high=init_range_high,\n",
        "                    parent_selection_type=parent_selection_type,\n",
        "                    crossover_type=crossover_type,\n",
        "                    mutation_type=mutation_type,\n",
        "                    mutation_probability = mutation_probability,\n",
        "                    stop_criteria=[\"saturate_20\"])\n",
        "\n",
        "ga_instance.run()\n",
        "\n",
        "# Evaluating best solution\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "\n",
        "fn_test.eval(ga_instance, solution, solution_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5791900976269718,\n",
              " 0.5343538338998928,\n",
              " 0.5238328305861192,\n",
              " 0.5237228947342973,\n",
              " 0.5997991449632745,\n",
              " 0.606491755884658,\n",
              " 0.6004795200682627,\n",
              " 0.5993681202704366,\n",
              " 0.5662229381377364,\n",
              " 0.5662229381377364,\n",
              " 0.5609933956117865,\n",
              " 0.5609933956117865,\n",
              " 0.569457994463938,\n",
              " 0.5473970737031113,\n",
              " 0.5585322413240748,\n",
              " 0.5567413758561621,\n",
              " 0.5567413758561621,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.567862865501825,\n",
              " 0.5598073889676316,\n",
              " 0.5578061459833064]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300\n",
            " 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314\n",
            " 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328\n",
            " 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342\n",
            " 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356\n",
            " 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370\n",
            " 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384\n",
            " 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398\n",
            " 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412\n",
            " 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426\n",
            " 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440\n",
            " 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454\n",
            " 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468\n",
            " 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482\n",
            " 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496\n",
            " 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510\n",
            " 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524\n",
            " 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538\n",
            " 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552\n",
            " 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566\n",
            " 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580\n",
            " 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594\n",
            " 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608\n",
            " 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622\n",
            " 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636\n",
            " 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.275273582385805"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluating best solution\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "\n",
        "fn_test.eval(ga_instance, solution, solution_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yang/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/HydroErr/HydroErr.py:6248: UserWarning: Row(s) [2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570\n",
            " 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584\n",
            " 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598\n",
            " 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612\n",
            " 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626\n",
            " 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640\n",
            " 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654\n",
            " 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668\n",
            " 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682\n",
            " 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696\n",
            " 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710\n",
            " 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724\n",
            " 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738\n",
            " 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752\n",
            " 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766\n",
            " 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780\n",
            " 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794\n",
            " 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808\n",
            " 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822\n",
            " 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836\n",
            " 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850\n",
            " 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864\n",
            " 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878\n",
            " 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892\n",
            " 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906\n",
            " 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((array([-1.14443824,  7.60124301, -1.81857731,  4.68522915, -5.24913282,\n",
              "         -6.98782299,  0.04133971, -0.28031797]),\n",
              "  0.8867852106643422,\n",
              "  0),\n",
              " (array([ 3.30561581,  4.38723012, -4.78517276,  4.33073349, -1.11927776,\n",
              "         -2.58300541,  0.91813256, -6.23118088]),\n",
              "  0.9003130163408817,\n",
              "  0))"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ga_instance2.best_solution(),ga_instance.best_solution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nm0X9u8sp9K7"
      },
      "outputs": [],
      "source": [
        "num_generations = 200\n",
        "num_parents_mating = 10\n",
        "\n",
        "sol_per_pop = 100\n",
        "num_genes = LATENT_dim\n",
        "\n",
        "init_range_low = catchment_embeddings.detach().cpu().min().numpy().tolist()\n",
        "init_range_high = catchment_embeddings.detach().cpu().max().numpy().tolist()\n",
        "\n",
        "parent_selection_type = \"sss\"\n",
        "\n",
        "crossover_type = \"single_point\"\n",
        "\n",
        "mutation_type = \"random\"\n",
        "mutation_probability = 0.25\n",
        "\n",
        "x_batch_train_val, y_batch_train_val = dtrain_val.get_val_batch()\n",
        "x_batch_test, y_batch_test = dtest.get_val_batch()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_calibration(selected_catchment=0):\n",
        "    \n",
        "    x = x_batch_train_val[:,selected_catchment,:,:]\n",
        "    y = y_batch_train_val[:,selected_catchment,:]\n",
        "\n",
        "    x, y = x.to(computing_device), y.to(computing_device)\n",
        "\n",
        "    fn = Objective_builder(x,y,HydroErr.kge_2009)\n",
        "\n",
        "    def fitness_func(ga_instance, solution, solution_idx):\n",
        "        return fn.eval(solution)\n",
        "\n",
        "    ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                        num_parents_mating=num_parents_mating,\n",
        "                        fitness_func=fitness_func,\n",
        "                        sol_per_pop=sol_per_pop,\n",
        "                        num_genes=num_genes,\n",
        "                        init_range_low=init_range_low,\n",
        "                        init_range_high=init_range_high,\n",
        "                        parent_selection_type=parent_selection_type,\n",
        "                        crossover_type=crossover_type,\n",
        "                        mutation_type=mutation_type,\n",
        "                        mutation_probability = mutation_probability,\n",
        "                        stop_criteria=[\"saturate_10\"])\n",
        "\n",
        "    ga_instance.run()\n",
        "\n",
        "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "    \n",
        "    # evaluate on test dataset\n",
        "    x = x_batch_test[:,selected_catchment,:,:]\n",
        "    y = y_batch_test[:,selected_catchment,:]\n",
        "\n",
        "    x, y = x.to(computing_device), y.to(computing_device)\n",
        "\n",
        "    fn = Objective_builder(x,y,HydroErr.kge_2009)\n",
        "\n",
        "    return fn.eval(solution), solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoScuihK-7QO",
        "outputId": "76bab91f-81fa-4fa0-cd25-39e38a80534e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i=0 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300\n",
            " 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314\n",
            " 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328\n",
            " 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342\n",
            " 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356\n",
            " 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370\n",
            " 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384\n",
            " 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398\n",
            " 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412\n",
            " 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426\n",
            " 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440\n",
            " 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454\n",
            " 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468\n",
            " 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482\n",
            " 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496\n",
            " 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510\n",
            " 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524\n",
            " 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538\n",
            " 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552\n",
            " 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566\n",
            " 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580\n",
            " 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594\n",
            " 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608\n",
            " 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622\n",
            " 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636\n",
            " 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.8912936780968745\n",
            "i=1 starts\n",
            "fit=0.8823883546151197\n",
            "i=2 starts\n",
            "fit=0.8591966099423587\n",
            "i=3 starts\n",
            "fit=0.8969882666394903\n",
            "i=4 starts\n",
            "fit=0.8950657324625352\n",
            "i=5 starts\n",
            "fit=0.8751336477183165\n",
            "i=6 starts\n",
            "fit=0.7306446623826914\n",
            "i=7 starts\n",
            "fit=0.8140387171921166\n",
            "i=8 starts\n",
            "fit=0.8811242785604148\n",
            "i=9 starts\n",
            "fit=0.8467761182197655\n",
            "i=10 starts\n",
            "fit=0.8691801580391245\n",
            "i=11 starts\n",
            "fit=0.8185834030787769\n",
            "i=12 starts\n",
            "fit=0.7916241030151192\n",
            "i=13 starts\n",
            "fit=0.7746070914082531\n",
            "i=14 starts\n",
            "fit=0.8504181277292586\n",
            "i=15 starts\n",
            "fit=0.7668275913408619\n",
            "i=16 starts\n",
            "fit=0.6687283264957957\n",
            "i=17 starts\n",
            "fit=0.8831461433222995\n",
            "i=18 starts\n",
            "fit=0.6939680505308197\n",
            "i=19 starts\n",
            "fit=0.745730027738799\n",
            "i=20 starts\n",
            "fit=0.694823344883986\n",
            "i=21 starts\n",
            "fit=0.7698606886676037\n",
            "i=22 starts\n",
            "fit=0.8235664372536173\n",
            "i=23 starts\n",
            "fit=0.7002406510090196\n",
            "i=24 starts\n",
            "fit=0.798448970943919\n",
            "i=25 starts\n",
            "fit=0.5282203928539844\n",
            "i=26 starts\n",
            "fit=0.7446983078127021\n",
            "i=27 starts\n",
            "fit=0.19334870356349054\n",
            "i=28 starts\n",
            "fit=0.6632312703341063\n",
            "i=29 starts\n",
            "fit=0.6806077869833798\n",
            "i=30 starts\n",
            "fit=0.7343164216004061\n",
            "i=31 starts\n",
            "fit=0.6815783519146051\n",
            "i=32 starts\n",
            "fit=0.749060369818324\n",
            "i=33 starts\n",
            "fit=0.7561031106837506\n",
            "i=34 starts\n",
            "fit=0.7853473743353784\n",
            "i=35 starts\n",
            "fit=0.8013903580231844\n",
            "i=36 starts\n",
            "fit=0.721768109905655\n",
            "i=37 starts\n",
            "fit=0.7483163400525799\n",
            "i=38 starts\n",
            "fit=0.8625452018121562\n",
            "i=39 starts\n",
            "fit=0.7703597141510337\n",
            "i=40 starts\n",
            "fit=0.796406991052965\n",
            "i=41 starts\n",
            "fit=0.4293418636955163\n",
            "i=42 starts\n",
            "fit=0.6996000736962401\n",
            "i=43 starts\n",
            "fit=0.5608462543339927\n",
            "i=44 starts\n",
            "fit=0.6800683027238474\n",
            "i=45 starts\n",
            "fit=0.8292325289920731\n",
            "i=46 starts\n",
            "fit=0.8249053714578493\n",
            "i=47 starts\n",
            "fit=0.801497054189428\n",
            "i=48 starts\n",
            "fit=0.7921535578915742\n",
            "i=49 starts\n",
            "fit=0.7584823560564088\n",
            "i=50 starts\n",
            "fit=0.6615511434624604\n",
            "i=51 starts\n",
            "fit=0.7550528047615891\n",
            "i=52 starts\n",
            "fit=0.5855133258084455\n",
            "i=53 starts\n",
            "fit=0.5548569791948073\n",
            "i=54 starts\n",
            "fit=0.7463656689596164\n",
            "i=55 starts\n",
            "fit=0.7918640113482946\n",
            "i=56 starts\n",
            "fit=0.6967632843767388\n",
            "i=57 starts\n",
            "fit=0.6506362437020634\n",
            "i=58 starts\n",
            "fit=0.7670862936309859\n",
            "i=59 starts\n",
            "fit=0.6758726099945686\n",
            "i=60 starts\n",
            "fit=0.6465651305591593\n",
            "i=61 starts\n",
            "fit=0.5462952992459174\n",
            "i=62 starts\n",
            "fit=0.6185833459722244\n",
            "i=63 starts\n",
            "fit=0.7284223217506929\n",
            "i=64 starts\n",
            "fit=0.7851901938125222\n",
            "i=65 starts\n",
            "fit=0.7011684810208436\n",
            "i=66 starts\n",
            "fit=0.7388534415361456\n",
            "i=67 starts\n",
            "fit=0.6982552520388824\n",
            "i=68 starts\n",
            "fit=0.5504668529626502\n",
            "i=69 starts\n",
            "fit=0.5824009647063175\n",
            "i=70 starts\n",
            "fit=0.436107659410648\n",
            "i=71 starts\n",
            "fit=0.6539840630533612\n",
            "i=72 starts\n",
            "fit=0.7605700279882919\n",
            "i=73 starts\n",
            "fit=0.664383946900212\n",
            "i=74 starts\n",
            "fit=0.630439365437927\n",
            "i=75 starts\n",
            "fit=0.5144689488262516\n",
            "i=76 starts\n",
            "fit=0.558355349853624\n",
            "i=77 starts\n",
            "fit=0.8288174600270559\n",
            "i=78 starts\n",
            "fit=0.5686150655117066\n",
            "i=79 starts\n",
            "fit=0.7623056280680507\n",
            "i=80 starts\n",
            "fit=0.6535196624047301\n",
            "i=81 starts\n",
            "fit=0.7127345643899553\n",
            "i=82 starts\n",
            "fit=0.7661610466640238\n",
            "i=83 starts\n",
            "fit=0.8025831067471254\n",
            "i=84 starts\n",
            "fit=0.6694830972405992\n",
            "i=85 starts\n",
            "fit=0.6870455571629603\n",
            "i=86 starts\n",
            "fit=0.5580366508870891\n",
            "i=87 starts\n",
            "fit=0.18101393412687683\n",
            "i=88 starts\n",
            "fit=0.5514277338877813\n",
            "i=89 starts\n",
            "fit=0.562531640437947\n",
            "i=90 starts\n",
            "fit=0.7240111615721256\n",
            "i=91 starts\n",
            "fit=0.556158983399262\n",
            "i=92 starts\n",
            "fit=0.7762572536998389\n",
            "i=93 starts\n",
            "fit=0.7328162508700818\n",
            "i=94 starts\n",
            "fit=0.6885135291194515\n",
            "i=95 starts\n",
            "fit=0.6335822938369698\n",
            "i=96 starts\n",
            "fit=0.7503690009665297\n",
            "i=97 starts\n",
            "fit=0.6295971153357078\n",
            "i=98 starts\n",
            "fit=-0.15651498360464355\n",
            "i=99 starts\n",
            "fit=0.4078804289622343\n",
            "i=100 starts\n",
            "fit=0.7755190678232041\n",
            "i=101 starts\n",
            "fit=0.7522122326038063\n",
            "i=102 starts\n",
            "fit=0.6335559609951211\n",
            "i=103 starts\n",
            "fit=0.39387087465748305\n",
            "i=104 starts\n",
            "fit=0.6973927561863671\n",
            "i=105 starts\n",
            "fit=0.5079808408817109\n",
            "i=106 starts\n",
            "fit=0.5408998399386826\n",
            "i=107 starts\n",
            "fit=0.5877558424223044\n",
            "i=108 starts\n",
            "fit=0.7450515369556265\n",
            "i=109 starts\n",
            "fit=0.7649509446775882\n",
            "i=110 starts\n",
            "fit=0.6662836102747567\n",
            "i=111 starts\n",
            "fit=0.4489019259951774\n",
            "i=112 starts\n",
            "fit=0.7581028690653293\n",
            "i=113 starts\n",
            "fit=0.6488244725854122\n",
            "i=114 starts\n",
            "fit=0.6710190184922507\n",
            "i=115 starts\n",
            "fit=0.6036794734270481\n",
            "i=116 starts\n",
            "fit=0.6274248884782838\n",
            "i=117 starts\n",
            "fit=0.7443172884793966\n",
            "i=118 starts\n",
            "fit=0.7605474427661401\n",
            "i=119 starts\n",
            "fit=0.627538320417513\n",
            "i=120 starts\n",
            "fit=0.6956573130132291\n",
            "i=121 starts\n",
            "fit=0.5569673527467147\n",
            "i=122 starts\n",
            "fit=0.643206222468697\n",
            "i=123 starts\n",
            "fit=0.6904460974784274\n",
            "i=124 starts\n",
            "fit=0.6538512321970651\n",
            "i=125 starts\n",
            "fit=0.5637489228743956\n",
            "i=126 starts\n",
            "fit=0.375545002558186\n",
            "i=127 starts\n",
            "fit=0.4452534211670761\n",
            "i=128 starts\n",
            "fit=0.533849147093576\n",
            "i=129 starts\n",
            "fit=0.7078263705089585\n",
            "i=130 starts\n",
            "fit=0.3051530438961664\n",
            "i=131 starts\n",
            "fit=0.7645520980366095\n",
            "i=132 starts\n",
            "fit=0.6531484201776132\n",
            "i=133 starts\n",
            "fit=0.6651912626364442\n",
            "i=134 starts\n",
            "fit=0.6038674931917363\n",
            "i=135 starts\n",
            "fit=0.8255883104485686\n",
            "i=136 starts\n",
            "fit=0.5269834888990224\n",
            "i=137 starts\n",
            "fit=0.7278106290942135\n",
            "i=138 starts\n",
            "fit=0.7184871066730889\n",
            "i=139 starts\n",
            "fit=0.5855336395542612\n",
            "i=140 starts\n",
            "fit=0.7522648266377977\n",
            "i=141 starts\n",
            "fit=0.38602491098814695\n",
            "i=142 starts\n",
            "fit=0.7518771111490002\n",
            "i=143 starts\n",
            "fit=0.6273867172389422\n",
            "i=144 starts\n",
            "fit=0.7882567288242955\n",
            "i=145 starts\n",
            "fit=0.7669453771932961\n",
            "i=146 starts\n",
            "fit=0.8204816487247735\n",
            "i=147 starts\n",
            "fit=0.8158594288148746\n",
            "i=148 starts\n",
            "fit=0.7359717078269223\n",
            "i=149 starts\n",
            "fit=0.617355853925122\n",
            "i=150 starts\n",
            "fit=0.5322388351152902\n",
            "i=151 starts\n",
            "fit=0.7514035913180375\n",
            "i=152 starts\n",
            "fit=0.7605849661784645\n",
            "i=153 starts\n",
            "fit=0.844880863934022\n",
            "i=154 starts\n",
            "fit=0.8477573180689877\n",
            "i=155 starts\n",
            "fit=0.7251308619961581\n",
            "i=156 starts\n",
            "fit=0.7204600413811668\n",
            "i=157 starts\n",
            "fit=0.6703529003737032\n",
            "i=158 starts\n",
            "fit=0.8199753627034568\n",
            "i=159 starts\n",
            "fit=0.7928594993895162\n",
            "i=160 starts\n",
            "fit=0.8471121169788152\n",
            "i=161 starts\n",
            "fit=0.7327859883372636\n",
            "i=162 starts\n",
            "fit=0.8670523869480268\n",
            "i=163 starts\n",
            "fit=0.7707111362867959\n",
            "i=164 starts\n",
            "fit=0.8315310854565033\n",
            "i=165 starts\n",
            "fit=0.8826881368770567\n",
            "i=166 starts\n",
            "fit=0.837738179718601\n",
            "i=167 starts\n",
            "fit=0.7022920276355558\n",
            "i=168 starts\n",
            "fit=0.8371841065374794\n",
            "i=169 starts\n",
            "fit=0.756348196831307\n",
            "i=170 starts\n",
            "fit=0.7922326318813894\n",
            "i=171 starts\n",
            "fit=0.819757897775403\n",
            "i=172 starts\n",
            "fit=0.6579593645217792\n",
            "i=173 starts\n",
            "fit=0.4249674222689046\n",
            "i=174 starts\n",
            "fit=0.8834210577558748\n",
            "i=175 starts\n",
            "fit=0.8719084349384916\n",
            "i=176 starts\n",
            "fit=0.8336148990025251\n",
            "i=177 starts\n",
            "fit=0.6233222740300799\n",
            "i=178 starts\n",
            "fit=0.7896202872456384\n",
            "i=179 starts\n",
            "fit=0.8295220690686849\n",
            "i=180 starts\n",
            "fit=0.7857897201219273\n",
            "i=181 starts\n",
            "fit=0.7335071251587313\n",
            "i=182 starts\n",
            "fit=0.8483086106236211\n",
            "i=183 starts\n",
            "fit=0.8874184752270475\n",
            "i=184 starts\n",
            "fit=0.728205122489263\n",
            "i=185 starts\n",
            "fit=0.6872167520748966\n",
            "i=186 starts\n",
            "fit=0.7846847305089302\n",
            "i=187 starts\n",
            "fit=0.8492035279671646\n",
            "i=188 starts\n",
            "fit=0.8197295081114432\n",
            "i=189 starts\n",
            "fit=0.8355268232717618\n",
            "i=190 starts\n",
            "fit=0.7962058595884095\n",
            "i=191 starts\n",
            "fit=0.6110657084153001\n",
            "i=192 starts\n",
            "fit=0.6765847697321187\n",
            "i=193 starts\n",
            "fit=0.7695105470078634\n",
            "i=194 starts\n",
            "fit=0.8514968296873547\n",
            "i=195 starts\n",
            "fit=0.7329294766956236\n",
            "i=196 starts\n",
            "fit=0.7297016112477084\n",
            "i=197 starts\n",
            "fit=0.7437350259564374\n",
            "i=198 starts\n",
            "fit=0.8505044338517947\n",
            "i=199 starts\n",
            "fit=0.7571830842834184\n",
            "i=200 starts\n",
            "fit=0.7520428709623608\n",
            "i=201 starts\n",
            "fit=0.842596762987822\n",
            "i=202 starts\n",
            "fit=0.7682300296945082\n",
            "i=203 starts\n",
            "fit=0.8498847039260924\n",
            "i=204 starts\n",
            "fit=0.6248160584336425\n",
            "i=205 starts\n",
            "fit=0.7952738774626337\n",
            "i=206 starts\n",
            "fit=0.7412279720727553\n",
            "i=207 starts\n",
            "fit=0.7986500062444926\n",
            "i=208 starts\n",
            "fit=0.6711663586117329\n",
            "i=209 starts\n",
            "fit=0.7026319485201694\n",
            "i=210 starts\n",
            "fit=0.7643123425827525\n",
            "i=211 starts\n",
            "fit=0.6040215783296499\n",
            "i=212 starts\n",
            "fit=0.6499871066903371\n",
            "i=213 starts\n",
            "fit=0.806256193017046\n",
            "i=214 starts\n",
            "fit=0.65573156919484\n",
            "i=215 starts\n",
            "fit=0.7797180883101983\n",
            "i=216 starts\n",
            "fit=0.8359448059459093\n",
            "i=217 starts\n",
            "fit=0.7134716990196507\n",
            "i=218 starts\n",
            "fit=0.8872822519561759\n",
            "i=219 starts\n",
            "fit=0.8122347821760717\n",
            "i=220 starts\n",
            "fit=0.6937261465180853\n",
            "i=221 starts\n",
            "fit=0.8042325143202841\n",
            "i=222 starts\n",
            "fit=0.8267336166851218\n",
            "i=223 starts\n",
            "fit=0.8279222093767691\n",
            "i=224 starts\n",
            "fit=0.6022889129571078\n",
            "i=225 starts\n",
            "fit=0.8254063229375801\n",
            "i=226 starts\n",
            "fit=0.8533249246525184\n",
            "i=227 starts\n",
            "fit=0.5764037006998106\n",
            "i=228 starts\n",
            "fit=0.5510750766588635\n",
            "i=229 starts\n",
            "fit=0.5554932619219182\n",
            "i=230 starts\n",
            "fit=0.6600610381429972\n",
            "i=231 starts\n",
            "fit=0.6016957959978717\n",
            "i=232 starts\n",
            "fit=0.7159893711223957\n",
            "i=233 starts\n",
            "fit=0.6697730507622881\n",
            "i=234 starts\n",
            "fit=0.5837016338902286\n",
            "i=235 starts\n",
            "fit=0.6228378543376387\n",
            "i=236 starts\n",
            "fit=0.7429382772886406\n",
            "i=237 starts\n",
            "fit=0.5851525712103269\n",
            "i=238 starts\n",
            "fit=0.5283141717881263\n",
            "i=239 starts\n",
            "fit=0.5763038105410729\n",
            "i=240 starts\n",
            "fit=0.6611995365870207\n",
            "i=241 starts\n",
            "fit=0.7098205550407024\n",
            "i=242 starts\n",
            "fit=0.8215188846022569\n",
            "i=243 starts\n",
            "fit=0.7315670806615131\n",
            "i=244 starts\n",
            "fit=0.8376095064059614\n",
            "i=245 starts\n",
            "fit=0.5881813728190524\n",
            "i=246 starts\n",
            "fit=0.7891137224440543\n",
            "i=247 starts\n",
            "fit=0.759334933701\n",
            "i=248 starts\n",
            "fit=0.6461201472236396\n",
            "i=249 starts\n",
            "fit=0.7429767702256163\n",
            "i=250 starts\n",
            "fit=0.8656167608267693\n",
            "i=251 starts\n",
            "fit=0.8123529710836118\n",
            "i=252 starts\n",
            "fit=0.675009397680489\n",
            "i=253 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [1270 1271 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029\n",
            " 4030 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043\n",
            " 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057\n",
            " 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071\n",
            " 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085\n",
            " 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099\n",
            " 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113\n",
            " 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127\n",
            " 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141\n",
            " 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155\n",
            " 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169\n",
            " 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183\n",
            " 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197\n",
            " 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211\n",
            " 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225\n",
            " 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239\n",
            " 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253\n",
            " 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267\n",
            " 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281\n",
            " 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295\n",
            " 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309\n",
            " 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323\n",
            " 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337\n",
            " 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351\n",
            " 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365\n",
            " 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.4376339720912312\n",
            "i=254 starts\n",
            "fit=0.1908777463122604\n",
            "i=255 starts\n",
            "fit=0.6890376778550571\n",
            "i=256 starts\n",
            "fit=0.7272851630603216\n",
            "i=257 starts\n",
            "fit=0.3924217709978861\n",
            "i=258 starts\n",
            "fit=0.5921596227220116\n",
            "i=259 starts\n",
            "fit=0.6214434804869797\n",
            "i=260 starts\n",
            "fit=0.7655458527683116\n",
            "i=261 starts\n",
            "fit=0.6683052720616469\n",
            "i=262 starts\n",
            "fit=0.7756124094761123\n",
            "i=263 starts\n",
            "fit=0.7680373758280458\n",
            "i=264 starts\n",
            "fit=0.6675763299458621\n",
            "i=265 starts\n",
            "fit=0.47088449914333264\n",
            "i=266 starts\n",
            "fit=0.6068229197723416\n",
            "i=267 starts\n",
            "fit=0.804066535003602\n",
            "i=268 starts\n",
            "fit=0.7979689653311579\n",
            "i=269 starts\n",
            "fit=0.6146605590561185\n",
            "i=270 starts\n",
            "fit=0.7324815909153055\n",
            "i=271 starts\n",
            "fit=0.7362078440957034\n",
            "i=272 starts\n",
            "fit=0.8132401917302935\n",
            "i=273 starts\n",
            "fit=0.8236269440353516\n",
            "i=274 starts\n",
            "fit=0.6519981422096226\n",
            "i=275 starts\n",
            "fit=0.21411473671276182\n",
            "i=276 starts\n",
            "fit=0.7979295217459965\n",
            "i=277 starts\n",
            "fit=0.763938919972599\n",
            "i=278 starts\n",
            "fit=0.7729487640864963\n",
            "i=279 starts\n",
            "fit=0.7799999435728998\n",
            "i=280 starts\n",
            "fit=0.7562094109907583\n",
            "i=281 starts\n",
            "fit=0.8113687227706355\n",
            "i=282 starts\n",
            "fit=0.8145396602758627\n",
            "i=283 starts\n",
            "fit=0.7092357790412143\n",
            "i=284 starts\n",
            "fit=0.6492748930270535\n",
            "i=285 starts\n",
            "fit=0.628234825001267\n",
            "i=286 starts\n",
            "fit=0.7920727394776362\n",
            "i=287 starts\n",
            "fit=0.8217593507341096\n",
            "i=288 starts\n",
            "fit=0.7253725131473274\n",
            "i=289 starts\n",
            "fit=0.5553316784717546\n",
            "i=290 starts\n",
            "fit=0.9075722357207092\n",
            "i=291 starts\n",
            "fit=0.9122551265573708\n",
            "i=292 starts\n",
            "fit=0.8615458384501659\n",
            "i=293 starts\n",
            "fit=0.6729904050692165\n",
            "i=294 starts\n",
            "fit=0.7023654954403228\n",
            "i=295 starts\n",
            "fit=0.547408474159152\n",
            "i=296 starts\n",
            "fit=0.8015060449553985\n",
            "i=297 starts\n",
            "fit=0.7702454428912029\n",
            "i=298 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939\n",
            " 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953\n",
            " 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967\n",
            " 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981\n",
            " 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995\n",
            " 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009\n",
            " 4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023\n",
            " 4024 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037\n",
            " 4038 4039 4040 4041 4042 4043 4044 4045 4046 4047 4048 4049 4050 4051\n",
            " 4052 4053 4054 4055 4056 4057 4058 4059 4060 4061 4062 4063 4064 4065\n",
            " 4066 4067 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079\n",
            " 4080 4081 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093\n",
            " 4094 4095 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107\n",
            " 4108 4109 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121\n",
            " 4122 4123 4124 4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135\n",
            " 4136 4137 4138 4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149\n",
            " 4150 4151 4152 4153 4154 4155 4156 4157 4158 4159 4160 4161 4162 4163\n",
            " 4164 4165 4166 4167 4168 4169 4170 4171 4172 4173 4174 4175 4176 4177\n",
            " 4178 4179 4180 4181 4182 4183 4184 4185 4186 4187 4188 4189 4190 4191\n",
            " 4192 4193 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205\n",
            " 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4218 4219\n",
            " 4220 4221 4222 4223 4224 4225 4226 4227 4228 4229 4230 4231 4232 4233\n",
            " 4234 4235 4236 4237 4238 4239 4240 4241 4242 4243 4244 4245 4246 4247\n",
            " 4248 4249 4250 4251 4252 4253 4254 4255 4256 4257 4258 4259 4260 4261\n",
            " 4262 4263 4264 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275\n",
            " 4276 4277 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289\n",
            " 4290 4291 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303\n",
            " 4304 4305 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317\n",
            " 4318 4319 4320 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331\n",
            " 4332 4333 4334 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345\n",
            " 4346 4347 4348 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359\n",
            " 4360 4361 4362 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373\n",
            " 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.7167774816227493\n",
            "i=299 starts\n",
            "fit=0.6081839022478532\n",
            "i=300 starts\n",
            "fit=0.22845189083580764\n",
            "i=301 starts\n",
            "fit=0.6175544115001697\n",
            "i=302 starts\n",
            "fit=0.10420788234934875\n",
            "i=303 starts\n",
            "fit=0.3320898384208886\n",
            "i=304 starts\n",
            "fit=0.353985025675976\n",
            "i=305 starts\n",
            "fit=-0.023227331208844015\n",
            "i=306 starts\n",
            "fit=0.005029179533584172\n",
            "i=307 starts\n",
            "fit=0.5666394182528733\n",
            "i=308 starts\n",
            "fit=0.37346779866512425\n",
            "i=309 starts\n",
            "fit=0.5462901102591674\n",
            "i=310 starts\n",
            "fit=0.46440859325793704\n",
            "i=311 starts\n",
            "fit=0.38060437244551437\n",
            "i=312 starts\n",
            "fit=0.6395152009632411\n",
            "i=313 starts\n",
            "fit=-0.14219954838626903\n",
            "i=314 starts\n",
            "fit=-0.649110266532632\n",
            "i=315 starts\n",
            "fit=0.5736367871086894\n",
            "i=316 starts\n",
            "fit=0.5183318423461545\n",
            "i=317 starts\n",
            "fit=0.5525246022205977\n",
            "i=318 starts\n",
            "fit=0.5283957269337098\n",
            "i=319 starts\n",
            "fit=0.5739650760726615\n",
            "i=320 starts\n",
            "fit=0.6336801638718352\n",
            "i=321 starts\n",
            "fit=0.7752355367308482\n",
            "i=322 starts\n",
            "fit=0.7322970663654083\n",
            "i=323 starts\n",
            "fit=0.5333537478605256\n",
            "i=324 starts\n",
            "fit=0.25084182936555566\n",
            "i=325 starts\n",
            "fit=-0.24999601155408646\n",
            "i=326 starts\n",
            "fit=0.44909062126150057\n",
            "i=327 starts\n",
            "fit=0.8046687910833213\n",
            "i=328 starts\n",
            "fit=0.7585748347315647\n",
            "i=329 starts\n",
            "fit=0.831416158274068\n",
            "i=330 starts\n",
            "fit=0.7637494276340349\n",
            "i=331 starts\n",
            "fit=-0.02147431337360195\n",
            "i=332 starts\n",
            "fit=0.4335960878471439\n",
            "i=333 starts\n",
            "fit=0.32504923300167443\n",
            "i=334 starts\n",
            "fit=0.6807576538609963\n",
            "i=335 starts\n",
            "fit=-0.6483735138379263\n",
            "i=336 starts\n",
            "fit=0.49684497224869717\n",
            "i=337 starts\n",
            "fit=0.341525750212476\n",
            "i=338 starts\n",
            "fit=0.6155484373196847\n",
            "i=339 starts\n",
            "fit=0.26290059179394576\n",
            "i=340 starts\n",
            "fit=0.7365548839123148\n",
            "i=341 starts\n",
            "fit=0.5093833627297726\n",
            "i=342 starts\n",
            "fit=0.4758520134244618\n",
            "i=343 starts\n",
            "fit=0.6076355431195499\n",
            "i=344 starts\n",
            "fit=0.6487989756755281\n",
            "i=345 starts\n",
            "fit=0.4295918309326212\n",
            "i=346 starts\n",
            "fit=0.8160173373137142\n",
            "i=347 starts\n",
            "fit=0.7429290490976379\n",
            "i=348 starts\n",
            "fit=0.7141354252202603\n",
            "i=349 starts\n",
            "fit=0.6726699178715787\n",
            "i=350 starts\n",
            "fit=0.509838512140498\n",
            "i=351 starts\n",
            "fit=0.8054471989232278\n",
            "i=352 starts\n",
            "fit=0.5180149368477243\n",
            "i=353 starts\n",
            "fit=0.6566852305397827\n",
            "i=354 starts\n",
            "fit=0.6624572503636359\n",
            "i=355 starts\n",
            "fit=0.8446216612767367\n",
            "i=356 starts\n",
            "fit=0.7727458714455948\n",
            "i=357 starts\n",
            "fit=0.6482645966530483\n",
            "i=358 starts\n",
            "fit=0.7024036283431643\n",
            "i=359 starts\n",
            "fit=0.793160068323733\n",
            "i=360 starts\n",
            "fit=0.7918404073600693\n",
            "i=361 starts\n",
            "fit=0.9361320674780196\n",
            "i=362 starts\n",
            "fit=0.06007826810147132\n",
            "i=363 starts\n",
            "fit=0.7954820858159183\n",
            "i=364 starts\n",
            "fit=0.7567646510969739\n",
            "i=365 starts\n",
            "fit=0.21631124777826016\n",
            "i=366 starts\n",
            "fit=0.8056743644351413\n",
            "i=367 starts\n",
            "fit=0.6781220321796\n",
            "i=368 starts\n",
            "fit=0.6836204086011437\n",
            "i=369 starts\n",
            "fit=0.7657454666849496\n",
            "i=370 starts\n",
            "fit=0.6388816010322728\n",
            "i=371 starts\n",
            "fit=0.6451918723863598\n",
            "i=372 starts\n",
            "fit=0.771049665875741\n",
            "i=373 starts\n",
            "fit=0.009830543843341588\n",
            "i=374 starts\n",
            "fit=-2.085254192790617\n",
            "i=375 starts\n",
            "fit=0.6806152154761698\n",
            "i=376 starts\n",
            "fit=0.647116660075858\n",
            "i=377 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3941\n",
            " 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955\n",
            " 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971\n",
            " 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985\n",
            " 3986 3987 3988 3989 3990 3991 3992 3993 3994 3996 3997 3998 3999 4000\n",
            " 4003 4004 4006 4007 4008 4009 4010 4011 4012 4013 4014 4015 4016 4017\n",
            " 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.867699585532275\n",
            "i=378 starts\n",
            "fit=0.657441676222895\n",
            "i=379 starts\n",
            "fit=0.7465437622369426\n",
            "i=380 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939\n",
            " 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953\n",
            " 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967\n",
            " 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981\n",
            " 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995\n",
            " 3996 3997 3998 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 4010\n",
            " 4011 4012 4013 4014 4015 4016 4018 4019 4020 4021 4022 4023 4024 4025\n",
            " 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 4039\n",
            " 4040 4041 4042 4043 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053\n",
            " 4054 4055 4056 4057 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067\n",
            " 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081\n",
            " 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095\n",
            " 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109\n",
            " 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123\n",
            " 4124 4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137\n",
            " 4138 4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151\n",
            " 4152 4153 4154 4155 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165\n",
            " 4166 4167 4168 4169 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179\n",
            " 4180 4181 4182 4183 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193\n",
            " 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207\n",
            " 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221\n",
            " 4222 4223 4224 4225 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235\n",
            " 4236 4237 4238 4239 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249\n",
            " 4250 4251 4252 4253 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263\n",
            " 4264 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277\n",
            " 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291\n",
            " 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305\n",
            " 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319\n",
            " 4320 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333\n",
            " 4334 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347\n",
            " 4348 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361\n",
            " 4362 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375\n",
            " 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.7004370784241776\n",
            "i=381 starts\n",
            "fit=0.45694210285611103\n",
            "i=382 starts\n",
            "fit=0.3514361642636139\n",
            "i=383 starts\n",
            "fit=0.21548589190848433\n",
            "i=384 starts\n",
            "fit=-0.727655090768595\n",
            "i=385 starts\n",
            "fit=0.2908907730621898\n",
            "i=386 starts\n",
            "fit=0.4078489957645489\n",
            "i=387 starts\n",
            "fit=0.7643054901112037\n",
            "i=388 starts\n",
            "fit=0.6827606090109432\n",
            "i=389 starts\n",
            "fit=0.7669929187905458\n",
            "i=390 starts\n",
            "fit=0.7515494559529072\n",
            "i=391 starts\n",
            "fit=0.6752598153380243\n",
            "i=392 starts\n",
            "fit=0.6649043130871198\n",
            "i=393 starts\n",
            "fit=0.536988616928081\n",
            "i=394 starts\n",
            "fit=0.7566745707011044\n",
            "i=395 starts\n",
            "fit=0.7537522553029978\n",
            "i=396 starts\n",
            "fit=0.6932173419350453\n",
            "i=397 starts\n",
            "fit=-0.043543275977983464\n",
            "i=398 starts\n",
            "fit=0.49936165433135615\n",
            "i=399 starts\n",
            "fit=0.5893134840529228\n",
            "i=400 starts\n",
            "fit=0.2532615862252927\n",
            "i=401 starts\n",
            "fit=0.5761973210627492\n",
            "i=402 starts\n",
            "fit=0.4722534724215953\n",
            "i=403 starts\n",
            "fit=0.32295085511141797\n",
            "i=404 starts\n",
            "fit=-0.008744193885943163\n",
            "i=405 starts\n",
            "fit=0.5300053695904481\n",
            "i=406 starts\n",
            "fit=0.5736063960047102\n",
            "i=407 starts\n",
            "fit=0.22178841316035447\n",
            "i=408 starts\n",
            "fit=0.19373487198762107\n",
            "i=409 starts\n",
            "fit=0.6809249204160803\n",
            "i=410 starts\n",
            "fit=0.46233971868042256\n",
            "i=411 starts\n",
            "fit=0.5553293141727491\n",
            "i=412 starts\n",
            "fit=0.4833882265794752\n",
            "i=413 starts\n",
            "fit=0.47804801275711617\n",
            "i=414 starts\n",
            "fit=0.6106947900453215\n",
            "i=415 starts\n",
            "fit=0.7852830794377521\n",
            "i=416 starts\n",
            "fit=0.260310962178642\n",
            "i=417 starts\n",
            "fit=0.7120753063155034\n",
            "i=418 starts\n",
            "fit=0.6500958883352785\n",
            "i=419 starts\n",
            "fit=0.6953189783232815\n",
            "i=420 starts\n",
            "fit=0.261498253589459\n",
            "i=421 starts\n",
            "fit=0.7432763076657637\n",
            "i=422 starts\n",
            "fit=0.49160623398557723\n",
            "i=423 starts\n",
            "fit=-2.2704397815161665\n",
            "i=424 starts\n",
            "fit=0.18463710669079958\n",
            "i=425 starts\n",
            "fit=0.4973215200022796\n",
            "i=426 starts\n",
            "fit=0.3522633088838294\n",
            "i=427 starts\n",
            "fit=0.3928569342772559\n",
            "i=428 starts\n",
            "fit=0.4325098249987056\n",
            "i=429 starts\n",
            "fit=0.31035293991016155\n",
            "i=430 starts\n",
            "fit=0.6431264237574161\n",
            "i=431 starts\n",
            "fit=0.5420467064190633\n",
            "i=432 starts\n",
            "fit=0.6975447924567727\n",
            "i=433 starts\n",
            "fit=0.308914213511158\n",
            "i=434 starts\n",
            "fit=0.8566405611896235\n",
            "i=435 starts\n",
            "fit=0.36371647230567916\n",
            "i=436 starts\n",
            "fit=0.7786256630970996\n",
            "i=437 starts\n",
            "fit=0.9317034811469092\n",
            "i=438 starts\n",
            "fit=0.41725993656253724\n",
            "i=439 starts\n",
            "fit=0.8471920481682792\n",
            "i=440 starts\n",
            "fit=0.8414874209543167\n",
            "i=441 starts\n",
            "fit=0.7457134956664409\n",
            "i=442 starts\n",
            "fit=0.7653792011169364\n",
            "i=443 starts\n",
            "fit=0.9161911252800251\n",
            "i=444 starts\n",
            "fit=0.9203750547047856\n",
            "i=445 starts\n",
            "fit=0.8206602516663257\n",
            "i=446 starts\n",
            "fit=0.8219165803798923\n",
            "i=447 starts\n",
            "fit=-0.19293259237892602\n",
            "i=448 starts\n",
            "fit=0.5706083405654294\n",
            "i=449 starts\n",
            "fit=0.8200548674904642\n",
            "i=450 starts\n",
            "fit=0.5800135967189897\n",
            "i=451 starts\n",
            "fit=-0.4308932051490917\n",
            "i=452 starts\n",
            "fit=0.5117899025684127\n",
            "i=453 starts\n",
            "fit=0.7105752333745404\n",
            "i=454 starts\n",
            "fit=0.30582221855588043\n",
            "i=455 starts\n",
            "fit=-0.9175919532579953\n",
            "i=456 starts\n",
            "fit=0.3165506260245665\n",
            "i=457 starts\n",
            "fit=0.11404858363065995\n",
            "i=458 starts\n",
            "fit=0.6797871302914851\n",
            "i=459 starts\n",
            "fit=0.7346600904758488\n",
            "i=460 starts\n",
            "fit=0.663551176614896\n",
            "i=461 starts\n",
            "fit=0.7168946285940131\n",
            "i=462 starts\n",
            "fit=0.627935819033309\n",
            "i=463 starts\n",
            "fit=0.6219908757699588\n",
            "i=464 starts\n",
            "fit=0.5989256853233595\n",
            "i=465 starts\n",
            "fit=0.7437858677438413\n",
            "i=466 starts\n",
            "fit=0.1217267802125902\n",
            "i=467 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3199 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030\n",
            " 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044\n",
            " 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058\n",
            " 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072\n",
            " 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086\n",
            " 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100\n",
            " 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114\n",
            " 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128\n",
            " 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142\n",
            " 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156\n",
            " 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170\n",
            " 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184\n",
            " 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198\n",
            " 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212\n",
            " 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226\n",
            " 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240\n",
            " 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254\n",
            " 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268\n",
            " 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282\n",
            " 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296\n",
            " 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310\n",
            " 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324\n",
            " 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338\n",
            " 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352\n",
            " 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366\n",
            " 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.7218981395847633\n",
            "i=468 starts\n",
            "fit=0.710521243962417\n",
            "i=469 starts\n",
            "fit=0.8586489404408156\n",
            "i=470 starts\n",
            "fit=0.6350423425877771\n",
            "i=471 starts\n",
            "fit=0.7622315652254993\n",
            "i=472 starts\n",
            "fit=0.8127880530402427\n",
            "i=473 starts\n",
            "fit=0.6272892375180575\n",
            "i=474 starts\n",
            "fit=0.6056425666641906\n",
            "i=475 starts\n",
            "fit=0.8285231778932943\n",
            "i=476 starts\n",
            "fit=0.8123488367762032\n",
            "i=477 starts\n",
            "fit=0.5627929787069346\n",
            "i=478 starts\n",
            "fit=0.8027830163268601\n",
            "i=479 starts\n",
            "fit=0.6031421538242122\n",
            "i=480 starts\n",
            "fit=0.661370085917617\n",
            "i=481 starts\n",
            "fit=0.7075212689334192\n",
            "i=482 starts\n",
            "fit=0.9114666358146765\n",
            "i=483 starts\n",
            "fit=0.8905628544467683\n",
            "i=484 starts\n",
            "fit=0.7804976045290968\n",
            "i=485 starts\n",
            "fit=0.683176143241441\n",
            "i=486 starts\n",
            "fit=0.7789942176835327\n",
            "i=487 starts\n",
            "fit=0.7309849970274142\n",
            "i=488 starts\n",
            "fit=0.8913348486753188\n",
            "i=489 starts\n",
            "fit=0.8529563917044659\n",
            "i=490 starts\n",
            "fit=0.8315747447813058\n",
            "i=491 starts\n",
            "fit=0.765510740505595\n",
            "i=492 starts\n",
            "fit=0.5363584182364016\n",
            "i=493 starts\n",
            "fit=0.8044133419558368\n",
            "i=494 starts\n",
            "fit=0.6346583871875378\n",
            "i=495 starts\n",
            "fit=0.5118364099822517\n",
            "i=496 starts\n",
            "fit=0.7363862605791925\n",
            "i=497 starts\n",
            "fit=0.801739628990025\n",
            "i=498 starts\n",
            "fit=0.7758286886015883\n",
            "i=499 starts\n",
            "fit=0.8689291328414165\n",
            "i=500 starts\n",
            "fit=0.9150777682090313\n",
            "i=501 starts\n",
            "fit=0.8619015018602451\n",
            "i=502 starts\n",
            "fit=0.6297591842984438\n",
            "i=503 starts\n",
            "fit=0.9296339432306677\n",
            "i=504 starts\n",
            "fit=0.5919756118080262\n",
            "i=505 starts\n",
            "fit=0.8231154548604795\n",
            "i=506 starts\n",
            "fit=0.5521365379069207\n",
            "i=507 starts\n",
            "fit=0.868663571221725\n",
            "i=508 starts\n",
            "fit=0.8001174339323315\n",
            "i=509 starts\n",
            "fit=0.9050090805812505\n",
            "i=510 starts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\Lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [ 638  639  640  641  642  643  644  645  646  647  648  649  650  651\n",
            "  652  653  654  655  656  657  658  659  660  661  662  663  664  665\n",
            "  666  667  668  669  670  671  672  673  674  675  676  677  678  679\n",
            "  680  681  682  683  684  685  686  687  688  689  690  691  692  693\n",
            "  694  695  696  697  698  699  700  701  702  703  704  705  706  707\n",
            "  708  709  710  711  712  713  714  715  716  717  718  719  720  721\n",
            "  722  723  724  725  726  727  728  729  730  731  732  733  734  735\n",
            "  736  737  738  739  740  741  742  743  744  745  746  747  748  749\n",
            "  750  751  752  753  754  755  756  757  758  759  760  761  762  763\n",
            "  764  765  766  767  768  769  770  771  772  773  774  775  776  777\n",
            "  778  779  780  781  782  783  784  785  786  787  788  789  790  791\n",
            "  792  793  794  795  796  797  798  799  800  801  802  803  804  805\n",
            "  806  807  808  809  810  811  812  813  814  815  816  817  818  819\n",
            "  820  821  822  823  824  825  826  827  828  829  830  831  832  833\n",
            "  834  835  836  837  838  839  840  841  842  843  844  845  846  847\n",
            "  848  849  850  851  852  853  854  855  856  857  858  859  860  861\n",
            "  862  863  864  865  866  867  868  869  870  871  872  873  874  875\n",
            "  876  877  878  879  880  881  882  883  884  885  886  887  888  889\n",
            "  890  891  892  893  894  895  896  897  898  899  900  901  902  903\n",
            "  904  905  906  907  908  909  910  911  912  913  914  915  916  917\n",
            "  918  919  920  921  922  923  924  925  926  927  928  929  930  931\n",
            "  932  933  934  935  936  937  938  939  940  941  942  943  944  945\n",
            "  946  947  948  949  950  951  952  953  954  955  956  957  958  959\n",
            "  960  961  962  963  964  965  966  967  968  969  970  971  972  973\n",
            "  974  975  976  977  978  979  980  981  982  983  984  985  986  987\n",
            "  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001\n",
            " 1002 1003 3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298\n",
            " 3299 3300 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312\n",
            " 3313 3314 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326\n",
            " 3327 3328 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340\n",
            " 3341 3342 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354\n",
            " 3355 3356 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368\n",
            " 3369 3370 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382\n",
            " 3383 3384 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396\n",
            " 3397 3398 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410\n",
            " 3411 3412 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424\n",
            " 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438\n",
            " 3439 3440 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452\n",
            " 3453 3454 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466\n",
            " 3467 3468 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480\n",
            " 3481 3482 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494\n",
            " 3495 3496 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508\n",
            " 3509 3510 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522\n",
            " 3523 3524 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536\n",
            " 3537 3538 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550\n",
            " 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564\n",
            " 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578\n",
            " 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592\n",
            " 3593 3594 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606\n",
            " 3607 3608 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620\n",
            " 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634\n",
            " 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648\n",
            " 3649] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit=0.8258814979279985\n",
            "i=511 starts\n",
            "fit=0.8204225853480238\n",
            "i=512 starts\n",
            "fit=0.7553665299473517\n",
            "i=513 starts\n",
            "fit=0.953819551262836\n",
            "i=514 starts\n",
            "fit=0.7986440080654245\n",
            "i=515 starts\n",
            "fit=0.9060115189549103\n",
            "i=516 starts\n",
            "fit=0.5490400635121606\n",
            "i=517 starts\n",
            "fit=0.7594315524164135\n",
            "i=518 starts\n",
            "fit=0.799473162346217\n",
            "i=519 starts\n",
            "fit=0.5211217354048752\n",
            "i=520 starts\n",
            "fit=0.628847612993779\n",
            "i=521 starts\n",
            "fit=0.8199416549128884\n",
            "i=522 starts\n",
            "fit=0.7314696569439858\n",
            "i=523 starts\n",
            "fit=0.8559750205832237\n",
            "i=524 starts\n",
            "fit=0.7301673349395086\n",
            "i=525 starts\n",
            "fit=0.883885545082419\n",
            "i=526 starts\n",
            "fit=0.757270453753514\n",
            "i=527 starts\n",
            "fit=0.6755736883917733\n",
            "i=528 starts\n",
            "fit=0.854255513753211\n",
            "i=529 starts\n",
            "fit=0.8504594939173958\n",
            "i=530 starts\n",
            "fit=0.9204855823276671\n",
            "i=531 starts\n",
            "fit=0.871633736613657\n",
            "i=532 starts\n",
            "fit=0.9095473807743244\n",
            "i=533 starts\n",
            "fit=0.8503566082240861\n",
            "i=534 starts\n",
            "fit=0.748759519443867\n",
            "i=535 starts\n",
            "fit=0.8627049493964447\n",
            "i=536 starts\n",
            "fit=0.8592485928565233\n",
            "i=537 starts\n",
            "fit=0.8366135946889782\n",
            "i=538 starts\n",
            "fit=0.852302636850615\n",
            "i=539 starts\n",
            "fit=0.8335045431228921\n",
            "i=540 starts\n",
            "fit=0.8972136480098605\n",
            "i=541 starts\n",
            "fit=0.7594857053755097\n",
            "i=542 starts\n",
            "fit=0.6334867157149655\n",
            "i=543 starts\n",
            "fit=0.8031104403601237\n",
            "i=544 starts\n",
            "fit=0.885050373079239\n",
            "i=545 starts\n",
            "fit=0.8742314546091691\n",
            "i=546 starts\n",
            "fit=0.9166778509334601\n",
            "i=547 starts\n",
            "fit=0.9142683474247958\n",
            "i=548 starts\n",
            "fit=0.9295381007546072\n",
            "i=549 starts\n",
            "fit=0.8518123176638934\n",
            "i=550 starts\n",
            "fit=0.8513365929421168\n",
            "i=551 starts\n",
            "fit=0.8650397043312454\n",
            "i=552 starts\n",
            "fit=0.9055864833710077\n",
            "i=553 starts\n",
            "fit=0.8901975310001256\n",
            "i=554 starts\n",
            "fit=0.8274075735824966\n",
            "i=555 starts\n",
            "fit=0.866401007346755\n",
            "i=556 starts\n",
            "fit=0.9030229415396893\n",
            "i=557 starts\n",
            "fit=0.9210108337454436\n",
            "i=558 starts\n",
            "fit=0.9041023200193959\n"
          ]
        }
      ],
      "source": [
        "N_CATCHMENTS = 559\n",
        "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
        "camels_embeddings = np.ones([N_CATCHMENTS, LATENT_dim])\n",
        "\n",
        "for i in range(N_CATCHMENTS):\n",
        "    print(f'i={i} starts')\n",
        "    calibrated_KGES[i], camels_embeddings[i,:]  = evaluate_calibration(i)\n",
        "    print(f'fit={calibrated_KGES[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VzoYzMSXDLtm"
      },
      "outputs": [],
      "source": [
        "np.savetxt(\"data/ga_KGEs.csv\", calibrated_KGES, delimiter=\",\")\n",
        "np.savetxt(\"data/ga_camels_embeddings.csv\", camels_embeddings, delimiter=\",\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5dla3EMJDzFA"
      },
      "source": [
        "# Result check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.91941096,  0.85682169,  0.91637485,  0.88217712,  0.88228154,\n",
              "        0.88692423,  0.74929147,  0.78510157,  0.86397878,  0.84344607,\n",
              "        0.76471023,  0.83853438,  0.83633127,  0.78910078,  0.84521933,\n",
              "        0.81248433,  0.80268453,  0.84948467,  0.74234173,  0.76419514,\n",
              "        0.76399213,  0.83128169,  0.76749481,  0.7045009 ,  0.78371101,\n",
              "        0.50837783,  0.77227529,  0.26538183,  0.57192447,  0.6664474 ,\n",
              "        0.62655947,  0.76148306,  0.71783067,  0.76387803,  0.79151405,\n",
              "        0.80246345,  0.75337292,  0.78052573,  0.83671804,  0.80459067,\n",
              "        0.81376294,  0.4992642 ,  0.7323576 ,  0.54922338,  0.80322473,\n",
              "        0.80011686,  0.86071515,  0.8383424 ,  0.77542572,  0.71476613,\n",
              "        0.68841337,  0.77964997,  0.51909695,  0.58665682,  0.75903779,\n",
              "        0.79007346,  0.70983101,  0.58668493,  0.79720012,  0.69344743,\n",
              "        0.65657411,  0.58694882,  0.69307331,  0.72728967,  0.79509138,\n",
              "        0.71894977,  0.75483379,  0.68432546,  0.55662402,  0.76278659,\n",
              "        0.39593781,  0.65615555,  0.74794464,  0.68333587,  0.70178364,\n",
              "        0.40697932,  0.61528125,  0.82687356,  0.65255826,  0.82787613,\n",
              "        0.63362776,  0.74538318,  0.73447039,  0.7825076 ,  0.68140171,\n",
              "        0.77140607,  0.6380635 ,  0.20024766,  0.42963008,  0.52971751,\n",
              "        0.6719701 ,  0.60099438,  0.80752196,  0.72892372,  0.69117261,\n",
              "        0.69611122,  0.76547868,  0.63872182, -0.11519627,  0.4860116 ,\n",
              "        0.800024  ,  0.78789842,  0.6612794 ,  0.44902973,  0.74939338,\n",
              "        0.51798368,  0.41078897,  0.58069804,  0.76383509,  0.69565273,\n",
              "        0.74485204,  0.64750086,  0.805574  ,  0.71960594,  0.62877262,\n",
              "        0.5836393 ,  0.67657159,  0.76312647,  0.73282007,  0.69718688,\n",
              "        0.7761632 ,  0.60193611,  0.7059993 ,  0.7715166 ,  0.59769292,\n",
              "        0.3441966 ,  0.51040031,  0.24102388,  0.64247312,  0.57392257,\n",
              "        0.38786003,  0.6611121 ,  0.37198241,  0.74592679,  0.59846122,\n",
              "        0.70251622,  0.73492927,  0.67689356,  0.71690588,  0.59795736,\n",
              "        0.73300535,  0.40050024,  0.66473986,  0.62052494,  0.77961191,\n",
              "        0.76834316,  0.71230815,  0.7280521 ,  0.72004147,  0.76340558,\n",
              "        0.71374059,  0.82062759,  0.74521085,  0.8669458 ,  0.82252881,\n",
              "        0.7336983 ,  0.75425873,  0.70288106,  0.84010339,  0.68098926,\n",
              "        0.7201963 ,  0.82231133,  0.7994239 ,  0.79321425,  0.83145289,\n",
              "        0.8515744 ,  0.85055076,  0.8052184 ,  0.83548787,  0.73452006,\n",
              "        0.80122317,  0.82810313,  0.70573904,  0.42361549,  0.84942962,\n",
              "        0.84081893,  0.81121902,  0.67205209,  0.7845977 ,  0.7212419 ,\n",
              "        0.77314804,  0.72063559,  0.85554111,  0.80042074,  0.78176203,\n",
              "        0.68695959,  0.8132754 ,  0.84377055,  0.8177617 ,  0.81214913,\n",
              "        0.82246987,  0.53969286,  0.66687526,  0.7458828 ,  0.74793674,\n",
              "        0.79191592,  0.64567794,  0.6469284 ,  0.82163772,  0.66315604,\n",
              "        0.83273323,  0.80931477,  0.72716919,  0.770134  ,  0.57770304,\n",
              "        0.77183441,  0.79923023,  0.79479594,  0.69940441,  0.68663195,\n",
              "        0.80398073,  0.62653959,  0.76841969,  0.78728059,  0.7310606 ,\n",
              "        0.80408147,  0.82308273,  0.65653169,  0.87923794,  0.7808387 ,\n",
              "        0.63723231,  0.7177967 ,  0.80920951,  0.87298911,  0.77234718,\n",
              "        0.78699512,  0.8540085 ,  0.66328523,  0.61134755,  0.71422559,\n",
              "        0.59235529,  0.52435937,  0.70486377,  0.71723667,  0.36131162,\n",
              "        0.75021068,  0.64943309,  0.81305739,  0.73621462,  0.4362296 ,\n",
              "        0.71317615,  0.7736917 ,  0.81945665,  0.77322614,  0.81510141,\n",
              "        0.58788488,  0.7880409 ,  0.84235667,  0.7342695 ,  0.79974474,\n",
              "        0.86163299,  0.85753881,  0.29171516,  0.61868524,  0.28944174,\n",
              "        0.67046013,  0.75092808,  0.17034675,  0.6858873 ,  0.67057141,\n",
              "        0.78353178,  0.6593783 ,  0.72854327,  0.72969661,  0.60647781,\n",
              "        0.46324869,  0.5986977 ,  0.75862397,  0.78650734,  0.56274371,\n",
              "        0.70876318,  0.71402015,  0.84328325,  0.65059414,  0.66794822,\n",
              "        0.37370717,  0.77020333,  0.72474588,  0.74934765,  0.84116089,\n",
              "        0.7195804 ,  0.73674984,  0.71969591,  0.72739284,  0.66451058,\n",
              "        0.62813259,  0.80259202,  0.79903452,  0.84866589,  0.78279906,\n",
              "        0.89923192,  0.914948  ,  0.9137207 ,  0.82258443,  0.86115085,\n",
              "        0.77165027,  0.88986706,  0.79083062,  0.69386732,  0.69214525,\n",
              "        0.1221648 ,  0.67361741,  0.39800786,  0.45627447,  0.52819251,\n",
              "        0.60182398,  0.61420173,  0.73701324,  0.32324371,  0.32032418,\n",
              "        0.3338584 ,  0.57983593,  0.66467604, -0.53347539, -0.77916522,\n",
              "        0.49231771,  0.24596427,  0.17699822,  0.58668613,  0.479291  ,\n",
              "        0.6221163 ,  0.69209199,  0.4790768 ,  0.7383808 , -0.15451924,\n",
              "       -0.29396229,  0.38796394,  0.83557095,  0.9029518 ,  0.81982132,\n",
              "        0.88559738,  0.06042302,  0.05468455,  0.40803702,  0.63159649,\n",
              "       -3.21306982, -0.42124001,  0.33627795,  0.70906647,  0.19768644,\n",
              "        0.76387015,  0.55994912,  0.50294428,  0.66834091,  0.70103322,\n",
              "        0.40526508,  0.73992273,  0.76987355,  0.69867323,  0.69825273,\n",
              "        0.4624396 ,  0.81352287,  0.47299275,  0.75578586,  0.73324486,\n",
              "        0.85506525,  0.67403278,  0.70994717,  0.69812294,  0.89670899,\n",
              "        0.79907003,  0.92885066, -0.81278593,  0.61655241,  0.57911228,\n",
              "        0.13960474,  0.80853184,  0.66676686,  0.51376719,  0.72713499,\n",
              "        0.51769168,  0.66470911,  0.78874377,  0.13556319, -0.07324176,\n",
              "        0.74368356,  0.67438866,  0.89954279,  0.64772151,  0.7213874 ,\n",
              "        0.67899435,  0.16287941,  0.23592526,  0.41262948, -0.76162158,\n",
              "       -0.59042373,  0.40076877,  0.7731125 ,  0.75390234,  0.77999434,\n",
              "        0.81311815,  0.67826166,  0.63984392,  0.56275195,  0.8047991 ,\n",
              "        0.81619315,  0.73642421, -0.16109983,  0.52173931,  0.59457149,\n",
              "        0.35685496,  0.42213994,  0.31439609,  0.41240864, -0.04470742,\n",
              "        0.47810438,  0.61220637,  0.36593469,  0.27405874,  0.5797445 ,\n",
              "        0.63725465,  0.59190753,  0.55982766,  0.56417574,  0.48744129,\n",
              "        0.78964921,  0.40404145,  0.6873886 ,  0.52958862,  0.66173348,\n",
              "        0.3022803 ,  0.71956229,  0.59604024, -1.52422093,  0.13413323,\n",
              "        0.5203036 ,  0.34086044,  0.32677319,  0.44044134,  0.40508588,\n",
              "        0.73027592,  0.75014598,  0.79458861,  0.62235415,  0.79674646,\n",
              "        0.71427114,  0.73907256,  0.92849822,  0.62132847,  0.91527526,\n",
              "        0.92204264,  0.81502002,  0.85546758,  0.94949747,  0.92836226,\n",
              "        0.71526301,  0.85665105, -0.31446978,  0.88272819,  0.91022162,\n",
              "        0.61967056, -0.93347162,  0.52861445,  0.50401264,  0.29982149,\n",
              "        0.01182777,  0.2302663 , -0.00489074,  0.60811637,  0.53904763,\n",
              "        0.60491536,  0.65143804,  0.55451037,  0.5805563 ,  0.6226916 ,\n",
              "        0.72562412, -0.05955173,  0.73620334,  0.84192303,  0.74433085,\n",
              "        0.65168561,  0.78386373,  0.80307725,  0.53566598,  0.70342163,\n",
              "        0.56503604,  0.81244425,  0.02801929,  0.6630147 ,  0.30617911,\n",
              "        0.66248627,  0.78174409,  0.83717879,  0.86448875,  0.7400771 ,\n",
              "        0.74108488,  0.81301268,  0.7423835 ,  0.89087058,  0.80904241,\n",
              "        0.78308961,  0.65669813,  0.5887841 ,  0.81898816,  0.59213189,\n",
              "        0.48059431,  0.69839707,  0.64962555,  0.69277236,  0.93228245,\n",
              "        0.90869752,  0.84480855,  0.62216996,  0.93397444,  0.62347771,\n",
              "        0.74147842,  0.64449642,  0.8960605 ,  0.87572828,  0.88802278,\n",
              "        0.84582648,  0.81432098,  0.85065143,  0.92631717,  0.81182305,\n",
              "        0.90836883,  0.60153123,  0.8002772 ,  0.84273776,  0.72939165,\n",
              "        0.64054561,  0.81267633,  0.75303366,  0.84359061,  0.80120621,\n",
              "        0.9228345 ,  0.88222069,  0.87602614,  0.887157  ,  0.87362374,\n",
              "        0.94694415,  0.93311091,  0.94540468,  0.80507133,  0.77788528,\n",
              "        0.87413583,  0.91948687,  0.86261909,  0.90104465,  0.915395  ,\n",
              "        0.8767495 ,  0.7805991 ,  0.80691232,  0.8109702 ,  0.88317847,\n",
              "        0.89530966,  0.90155368,  0.91312674,  0.90371858,  0.85219745,\n",
              "        0.87097784,  0.87296341,  0.90480454,  0.90352674,  0.7921505 ,\n",
              "        0.91245779,  0.86514961,  0.89443528,  0.89847546])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calibrated_KGES"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KcwBoioFp9K9",
        "outputId": "e9aa9d7e-2972-4b4e-f519-4e10ab766559"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.hist(calibrated_KGES[calibrated_KGES>0], bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fBoQkf-Cp9K9",
        "outputId": "4856c289-13ec-4e46-d43d-559f73f757df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6496525777566913"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calibrated_KGES.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TNPOIEy_p9K-",
        "outputId": "dac5fcb6-05dd-456b-89b1-4018cffaa686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7168946285940131"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(calibrated_KGES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t5aRLriip9K7",
        "outputId": "83c37e57-c570-4994-b0e2-39ae8c766cb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.89129368,  0.88238835,  0.85919661,  0.89698827,  0.89506573,\n",
              "        0.87513365,  0.73064466,  0.81403872,  0.88112428,  0.84677612,\n",
              "        0.86918016,  0.8185834 ,  0.7916241 ,  0.77460709,  0.85041813,\n",
              "        0.76682759,  0.66872833,  0.88314614,  0.69396805,  0.74573003,\n",
              "        0.69482334,  0.76986069,  0.82356644,  0.70024065,  0.79844897,\n",
              "        0.52822039,  0.74469831,  0.1933487 ,  0.66323127,  0.68060779,\n",
              "        0.73431642,  0.68157835,  0.74906037,  0.75610311,  0.78534737,\n",
              "        0.80139036,  0.72176811,  0.74831634,  0.8625452 ,  0.77035971,\n",
              "        0.79640699,  0.42934186,  0.69960007,  0.56084625,  0.6800683 ,\n",
              "        0.82923253,  0.82490537,  0.80149705,  0.79215356,  0.75848236,\n",
              "        0.66155114,  0.7550528 ,  0.58551333,  0.55485698,  0.74636567,\n",
              "        0.79186401,  0.69676328,  0.65063624,  0.76708629,  0.67587261,\n",
              "        0.64656513,  0.5462953 ,  0.61858335,  0.72842232,  0.78519019,\n",
              "        0.70116848,  0.73885344,  0.69825525,  0.55046685,  0.58240096,\n",
              "        0.43610766,  0.65398406,  0.76057003,  0.66438395,  0.63043937,\n",
              "        0.51446895,  0.55835535,  0.82881746,  0.56861507,  0.76230563,\n",
              "        0.65351966,  0.71273456,  0.76616105,  0.80258311,  0.6694831 ,\n",
              "        0.68704556,  0.55803665,  0.18101393,  0.55142773,  0.56253164,\n",
              "        0.72401116,  0.55615898,  0.77625725,  0.73281625,  0.68851353,\n",
              "        0.63358229,  0.750369  ,  0.62959712, -0.15651498,  0.40788043,\n",
              "        0.77551907,  0.75221223,  0.63355596,  0.39387087,  0.69739276,\n",
              "        0.50798084,  0.54089984,  0.58775584,  0.74505154,  0.76495094,\n",
              "        0.66628361,  0.44890193,  0.75810287,  0.64882447,  0.67101902,\n",
              "        0.60367947,  0.62742489,  0.74431729,  0.76054744,  0.62753832,\n",
              "        0.69565731,  0.55696735,  0.64320622,  0.6904461 ,  0.65385123,\n",
              "        0.56374892,  0.375545  ,  0.44525342,  0.53384915,  0.70782637,\n",
              "        0.30515304,  0.7645521 ,  0.65314842,  0.66519126,  0.60386749,\n",
              "        0.82558831,  0.52698349,  0.72781063,  0.71848711,  0.58553364,\n",
              "        0.75226483,  0.38602491,  0.75187711,  0.62738672,  0.78825673,\n",
              "        0.76694538,  0.82048165,  0.81585943,  0.73597171,  0.61735585,\n",
              "        0.53223884,  0.75140359,  0.76058497,  0.84488086,  0.84775732,\n",
              "        0.72513086,  0.72046004,  0.6703529 ,  0.81997536,  0.7928595 ,\n",
              "        0.84711212,  0.73278599,  0.86705239,  0.77071114,  0.83153109,\n",
              "        0.88268814,  0.83773818,  0.70229203,  0.83718411,  0.7563482 ,\n",
              "        0.79223263,  0.8197579 ,  0.65795936,  0.42496742,  0.88342106,\n",
              "        0.87190843,  0.8336149 ,  0.62332227,  0.78962029,  0.82952207,\n",
              "        0.78578972,  0.73350713,  0.84830861,  0.88741848,  0.72820512,\n",
              "        0.68721675,  0.78468473,  0.84920353,  0.81972951,  0.83552682,\n",
              "        0.79620586,  0.61106571,  0.67658477,  0.76951055,  0.85149683,\n",
              "        0.73292948,  0.72970161,  0.74373503,  0.85050443,  0.75718308,\n",
              "        0.75204287,  0.84259676,  0.76823003,  0.8498847 ,  0.62481606,\n",
              "        0.79527388,  0.74122797,  0.79865001,  0.67116636,  0.70263195,\n",
              "        0.76431234,  0.60402158,  0.64998711,  0.80625619,  0.65573157,\n",
              "        0.77971809,  0.83594481,  0.7134717 ,  0.88728225,  0.81223478,\n",
              "        0.69372615,  0.80423251,  0.82673362,  0.82792221,  0.60228891,\n",
              "        0.82540632,  0.85332492,  0.5764037 ,  0.55107508,  0.55549326,\n",
              "        0.66006104,  0.6016958 ,  0.71598937,  0.66977305,  0.58370163,\n",
              "        0.62283785,  0.74293828,  0.58515257,  0.52831417,  0.57630381,\n",
              "        0.66119954,  0.70982056,  0.82151888,  0.73156708,  0.83760951,\n",
              "        0.58818137,  0.78911372,  0.75933493,  0.64612015,  0.74297677,\n",
              "        0.86561676,  0.81235297,  0.6750094 ,  0.43763397,  0.19087775,\n",
              "        0.68903768,  0.72728516,  0.39242177,  0.59215962,  0.62144348,\n",
              "        0.76554585,  0.66830527,  0.77561241,  0.76803738,  0.66757633,\n",
              "        0.4708845 ,  0.60682292,  0.80406654,  0.79796897,  0.61466056,\n",
              "        0.73248159,  0.73620784,  0.81324019,  0.82362694,  0.65199814,\n",
              "        0.21411474,  0.79792952,  0.76393892,  0.77294876,  0.77999994,\n",
              "        0.75620941,  0.81136872,  0.81453966,  0.70923578,  0.64927489,\n",
              "        0.62823483,  0.79207274,  0.82175935,  0.72537251,  0.55533168,\n",
              "        0.90757224,  0.91225513,  0.86154584,  0.67299041,  0.7023655 ,\n",
              "        0.54740847,  0.80150604,  0.77024544,  0.71677748,  0.6081839 ,\n",
              "        0.22845189,  0.61755441,  0.10420788,  0.33208984,  0.35398503,\n",
              "       -0.02322733,  0.00502918,  0.56663942,  0.3734678 ,  0.54629011,\n",
              "        0.46440859,  0.38060437,  0.6395152 , -0.14219955, -0.64911027,\n",
              "        0.57363679,  0.51833184,  0.5525246 ,  0.52839573,  0.57396508,\n",
              "        0.63368016,  0.77523554,  0.73229707,  0.53335375,  0.25084183,\n",
              "       -0.24999601,  0.44909062,  0.80466879,  0.75857483,  0.83141616,\n",
              "        0.76374943, -0.02147431,  0.43359609,  0.32504923,  0.68075765,\n",
              "       -0.64837351,  0.49684497,  0.34152575,  0.61554844,  0.26290059,\n",
              "        0.73655488,  0.50938336,  0.47585201,  0.60763554,  0.64879898,\n",
              "        0.42959183,  0.81601734,  0.74292905,  0.71413543,  0.67266992,\n",
              "        0.50983851,  0.8054472 ,  0.51801494,  0.65668523,  0.66245725,\n",
              "        0.84462166,  0.77274587,  0.6482646 ,  0.70240363,  0.79316007,\n",
              "        0.79184041,  0.93613207,  0.06007827,  0.79548209,  0.75676465,\n",
              "        0.21631125,  0.80567436,  0.67812203,  0.68362041,  0.76574547,\n",
              "        0.6388816 ,  0.64519187,  0.77104967,  0.00983054, -2.08525419,\n",
              "        0.68061522,  0.64711666,  0.86769959,  0.65744168,  0.74654376,\n",
              "        0.70043708,  0.4569421 ,  0.35143616,  0.21548589, -0.72765509,\n",
              "        0.29089077,  0.407849  ,  0.76430549,  0.68276061,  0.76699292,\n",
              "        0.75154946,  0.67525982,  0.66490431,  0.53698862,  0.75667457,\n",
              "        0.75375226,  0.69321734, -0.04354328,  0.49936165,  0.58931348,\n",
              "        0.25326159,  0.57619732,  0.47225347,  0.32295086, -0.00874419,\n",
              "        0.53000537,  0.5736064 ,  0.22178841,  0.19373487,  0.68092492,\n",
              "        0.46233972,  0.55532931,  0.48338823,  0.47804801,  0.61069479,\n",
              "        0.78528308,  0.26031096,  0.71207531,  0.65009589,  0.69531898,\n",
              "        0.26149825,  0.74327631,  0.49160623, -2.27043978,  0.18463711,\n",
              "        0.49732152,  0.35226331,  0.39285693,  0.43250982,  0.31035294,\n",
              "        0.64312642,  0.54204671,  0.69754479,  0.30891421,  0.85664056,\n",
              "        0.36371647,  0.77862566,  0.93170348,  0.41725994,  0.84719205,\n",
              "        0.84148742,  0.7457135 ,  0.7653792 ,  0.91619113,  0.92037505,\n",
              "        0.82066025,  0.82191658, -0.19293259,  0.57060834,  0.82005487,\n",
              "        0.5800136 , -0.43089321,  0.5117899 ,  0.71057523,  0.30582222,\n",
              "       -0.91759195,  0.31655063,  0.11404858,  0.67978713,  0.73466009,\n",
              "        0.66355118,  0.71689463,  0.62793582,  0.62199088,  0.59892569,\n",
              "        0.74378587,  0.12172678,  0.72189814,  0.71052124,  0.85864894,\n",
              "        0.63504234,  0.76223157,  0.81278805,  0.62728924,  0.60564257,\n",
              "        0.82852318,  0.81234884,  0.56279298,  0.80278302,  0.60314215,\n",
              "        0.66137009,  0.70752127,  0.91146664,  0.89056285,  0.7804976 ,\n",
              "        0.68317614,  0.77899422,  0.730985  ,  0.89133485,  0.85295639,\n",
              "        0.83157474,  0.76551074,  0.53635842,  0.80441334,  0.63465839,\n",
              "        0.51183641,  0.73638626,  0.80173963,  0.77582869,  0.86892913,\n",
              "        0.91507777,  0.8619015 ,  0.62975918,  0.92963394,  0.59197561,\n",
              "        0.82311545,  0.55213654,  0.86866357,  0.80011743,  0.90500908,\n",
              "        0.8258815 ,  0.82042259,  0.75536653,  0.95381955,  0.79864401,\n",
              "        0.90601152,  0.54904006,  0.75943155,  0.79947316,  0.52112174,\n",
              "        0.62884761,  0.81994165,  0.73146966,  0.85597502,  0.73016733,\n",
              "        0.88388555,  0.75727045,  0.67557369,  0.85425551,  0.85045949,\n",
              "        0.92048558,  0.87163374,  0.90954738,  0.85035661,  0.74875952,\n",
              "        0.86270495,  0.85924859,  0.83661359,  0.85230264,  0.83350454,\n",
              "        0.89721365,  0.75948571,  0.63348672,  0.80311044,  0.88505037,\n",
              "        0.87423145,  0.91667785,  0.91426835,  0.9295381 ,  0.85181232,\n",
              "        0.85133659,  0.8650397 ,  0.90558648,  0.89019753,  0.82740757,\n",
              "        0.86640101,  0.90302294,  0.92101083,  0.90410232])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calibrated_KGES"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
