{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import HydroErr\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 5\n",
    "\n",
    "N_CATCHMENTS = 671\n",
    "\n",
    "# training hyperparameters\n",
    "EPOCHS = 500\n",
    "TRAIN_VAL_YEAR = 14\n",
    "TRAIN_YEAR = 10\n",
    "PATIENCE = 20\n",
    "\n",
    "use_amp = True\n",
    "compile_model = True\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_val_test_data(\n",
    "    forcing_dataset,\n",
    "    train_val_length=5478,\n",
    "    train_length=4017,\n",
    "    val_length=1826,\n",
    "    test_length=5844,\n",
    "):\n",
    "    dtrain_val = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train_val.csv\",\n",
    "        record_length=train_val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtrain = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train.csv\",\n",
    "        record_length=train_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dval = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_val.csv\",\n",
    "        record_length=val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtest = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_test.csv\",\n",
    "        record_length=test_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    return dtrain_val, dtrain, dval, dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_study(forcing_dataset):\n",
    "    study = joblib.load(f\"data/{forcing_dataset}_671_study.pkl\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_epochs(study):\n",
    "    \n",
    "    stats = study.best_trials[0].intermediate_values\n",
    "    epochs = min(stats, key=lambda k: stats[k]) + 1\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model(study, dataset, n_catchments =N_CATCHMENTS, epoch_scale = TRAIN_YEAR/TRAIN_VAL_YEAR): #19/39\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    # define model\n",
    "    model_builder = training_fun.LSTM_model_builder(\n",
    "        n_catchments, base_length=365, forcing_dim=FORCING_DIM\n",
    "    )\n",
    "\n",
    "    embedding, decoder = model_builder.define_model(trial)\n",
    "\n",
    "    embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "        computing_device\n",
    "    )\n",
    "\n",
    "    if compile_model:\n",
    "        # pytorch2.0 new feature, complile model for fast training\n",
    "        embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "\n",
    "    # define model training hyperparameters\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # define optimal epochs\n",
    "    epochs = round(get_optimal_epochs(study)*epoch_scale)\n",
    "\n",
    "        # steps per epoch\n",
    "    steps = round(N_CATCHMENTS * TRAIN_VAL_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_VAL_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "\n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, selected_catchments) = dataset.get_random_batch(\n",
    "                batch_size\n",
    "            )\n",
    "\n",
    "            x_batch, y_batch, selected_catchments = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "                selected_catchments.to(computing_device),\n",
    "            )\n",
    "\n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(selected_catchments)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Python 3.11+ not yet supported for torch.compile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m study \u001b[39m=\u001b[39m load_study(forcing_dataset)\n\u001b[1;32m      7\u001b[0m dtrain_val, dtrain, dval, dtest \u001b[39m=\u001b[39m read_train_val_test_data(forcing_dataset)\n\u001b[0;32m----> 9\u001b[0m embedding, decoder \u001b[39m=\u001b[39m get_final_model(study, dtrain_val)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mget_final_model\u001b[0;34m(study, dataset, n_catchments, epoch_scale)\u001b[0m\n\u001b[1;32m     12\u001b[0m embedding, decoder \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39mto(computing_device), decoder\u001b[39m.\u001b[39mto(\n\u001b[1;32m     13\u001b[0m     computing_device\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m compile_model:\n\u001b[1;32m     17\u001b[0m     \u001b[39m# pytorch2.0 new feature, complile model for fast training\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     embedding, decoder \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcompile(embedding), torch\u001b[39m.\u001b[39mcompile(decoder)\n\u001b[1;32m     21\u001b[0m \u001b[39m# define model training hyperparameters\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# define optimizers\u001b[39;00m\n\u001b[1;32m     23\u001b[0m lr_embedding \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m\"\u001b[39m\u001b[39mlr_embedding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m5e-5\u001b[39m, \u001b[39m1e-2\u001b[39m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/__init__.py:1441\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minductor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1440\u001b[0m     backend \u001b[39m=\u001b[39m _TorchCompileInductorWrapper(mode, options, dynamic)\n\u001b[0;32m-> 1441\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49moptimize(backend\u001b[39m=\u001b[39;49mbackend, nopython\u001b[39m=\u001b[39;49mfullgraph, dynamic\u001b[39m=\u001b[39;49mdynamic, disable\u001b[39m=\u001b[39;49mdisable)(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:413\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    381\u001b[0m     backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minductor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     dynamic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m ):\n\u001b[1;32m    389\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m    The main entrypoint of TorchDynamo.  Do graph capture and call\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m    backend() to optimize extracted graphs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39m            ...\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     check_if_dynamo_supported()\n\u001b[1;32m    414\u001b[0m     \u001b[39m# Note: The hooks object could be global instead of passed around, *however* that would make\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[39m# for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[39m# There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[39m# compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# easier to understand UX at the cost of a little more plumbing on our end.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     hooks \u001b[39m=\u001b[39m Hooks(guard_export_fn\u001b[39m=\u001b[39mguard_export_fn, guard_fail_fn\u001b[39m=\u001b[39mguard_fail_fn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:377\u001b[0m, in \u001b[0;36mcheck_if_dynamo_supported\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWindows not yet supported for torch.compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m11\u001b[39m):\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPython 3.11+ not yet supported for torch.compile\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Python 3.11+ not yet supported for torch.compile"
     ]
    }
   ],
   "source": [
    "forcing_datasets = [\"nldas\", \"daymet\", \"maurer\"]\n",
    "\n",
    "for i in range(len(forcing_datasets)):\n",
    "    forcing_dataset = forcing_datasets[i]\n",
    "    \n",
    "    study = load_study(forcing_dataset)\n",
    "    dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "    \n",
    "    embedding, decoder = get_final_model(study, dtrain_val)\n",
    "    \n",
    "    torch.save(embedding.cpu(), f\"data/671_camels_{forcing_dataset}_embedding.pt\")\n",
    "    torch.save(decoder.cpu(), f\"data/671_camels_{forcing_dataset}_decoder.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding, decoder = get_final_model(study, dtrain_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, model_builder):\n",
    "        self.model_builder = model_builder\n",
    "\n",
    "    def objective(self, trial):\n",
    "\n",
    "        # prepare early stopper\n",
    "        early_stopper = training_fun.EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "        # define model\n",
    "        embedding, decoder = self.model_builder.define_model(trial)\n",
    "        embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "            computing_device\n",
    "        )\n",
    "\n",
    "        if compile_model:\n",
    "            # pytorch2.0 new feature, complile model for fast training\n",
    "            embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "        # define optimizers\n",
    "        lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "        lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "        # define batch size\n",
    "        batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "        batch_size = 2**batch_size_power\n",
    "\n",
    "        # steps per epoch\n",
    "        steps = round(N_CATCHMENTS * TRAIN_VAL_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "            # repeat TRAIN_VAL_YEAR times to finish an epoch\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            for step in range(steps):\n",
    "\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # put the models into training mode\n",
    "                decoder.train()\n",
    "                embedding.train()\n",
    "\n",
    "                # get training batch and pass to device\n",
    "                invalid_batch = True\n",
    "                while invalid_batch:\n",
    "                    (x_batch, y_batch, selected_catchments) = dtrain.get_random_batch(\n",
    "                        batch_size\n",
    "                    )\n",
    "\n",
    "                    if len(x_batch) > 0:\n",
    "                        invalid_batch = False\n",
    "\n",
    "                x_batch, y_batch, selected_catchments = (\n",
    "                    x_batch.to(computing_device),\n",
    "                    y_batch.to(computing_device),\n",
    "                    selected_catchments.to(computing_device),\n",
    "                )\n",
    "\n",
    "                # slice batch for training\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "                ):\n",
    "                    code = embedding(selected_catchments)\n",
    "\n",
    "                    # pass through decoder\n",
    "                    out = decoder.decode(code, x_batch)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            # validate model after each epochs\n",
    "            decoder.eval()\n",
    "            embedding.eval()\n",
    "\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if memory_saving:\n",
    "                val_loss = training_fun.val_model_mem_saving(\n",
    "                    embedding=embedding,\n",
    "                    decoder=decoder,\n",
    "                    dataset=dval,\n",
    "                    storge_device=storge_device,\n",
    "                    computing_device=computing_device,\n",
    "                    use_amp=use_amp,\n",
    "                    val_metric=training_fun.mse_loss_with_nans,\n",
    "                    return_summary=True,\n",
    "                    val_steps=VAL_STEPS,\n",
    "                )\n",
    "            else:\n",
    "                val_loss = (\n",
    "                    training_fun.val_model(\n",
    "                        embedding=embedding,\n",
    "                        decoder=decoder,\n",
    "                        dataset=dval,\n",
    "                        storge_device=storge_device,\n",
    "                        computing_device=computing_device,\n",
    "                        use_amp=use_amp,\n",
    "                        val_metric=training_fun.mse_loss_with_nans,\n",
    "                        return_summary=True,\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                torch.cuda.empty_cache()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Early stop using early_stopper, break for loop\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return early_stopper.min_validation_loss\n",
    "\n",
    "\n",
    "forcing_datasets = [\"nldas\", \"daymet\", \"maurer\"]\n",
    "\n",
    "for i in range(len(forcing_datasets)):\n",
    "    forcing_dataset = forcing_datasets[i]\n",
    "\n",
    "    dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "\n",
    "    LSTM_model_builder = training_fun.LSTM_model_builder(\n",
    "        n_catchments=N_CATCHMENTS, base_length=BASE_LENGTH, forcing_dim=FORCING_DIM\n",
    "    )\n",
    "\n",
    "    LSTM_objective = Objective(LSTM_model_builder).objective\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{forcing_dataset}_671_study\",\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.NopPruner(),\n",
    "    )\n",
    "    study.optimize(LSTM_objective, n_trials=200)\n",
    "\n",
    "    joblib.dump(study, f\"data/{forcing_dataset}_671_study.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
