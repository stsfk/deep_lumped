{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import HydroErr\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 5\n",
    "\n",
    "N_CATCHMENTS = 671\n",
    "\n",
    "# training hyperparameters\n",
    "EPOCHS = 500\n",
    "TRAIN_VAL_YEAR = 14\n",
    "TRAIN_YEAR = 14\n",
    "PATIENCE = 20\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_val_test_data(\n",
    "    forcing_dataset,\n",
    "    train_val_length=5478,\n",
    "    train_length=4017,\n",
    "    val_length=1826,\n",
    "    test_length=5844,\n",
    "):\n",
    "    dtrain_val = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train_val.csv\",\n",
    "        record_length=train_val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtrain = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train.csv\",\n",
    "        record_length=train_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dval = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_val.csv\",\n",
    "        record_length=val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtest = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_test.csv\",\n",
    "        record_length=test_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    return dtrain_val, dtrain, dval, dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_study(forcing_dataset):\n",
    "    study = joblib.load(f\"data/{forcing_dataset}_671_study.pkl\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_epochs(study):\n",
    "    \n",
    "    stats = study.best_trials[0].intermediate_values\n",
    "    epochs = min(stats, key=lambda k: stats[k]) + 1\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model(study, dataset, n_catchments =N_CATCHMENTS, epoch_scale = TRAIN_YEAR/TRAIN_VAL_YEAR): #19/39\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    # define model\n",
    "    model_builder = training_fun.LSTM_model_builder(\n",
    "        n_catchments, base_length=365, forcing_dim=FORCING_DIM\n",
    "    )\n",
    "\n",
    "    embedding, decoder = model_builder.define_model(trial)\n",
    "\n",
    "    embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "        computing_device\n",
    "    )\n",
    "\n",
    "    if compile_model:\n",
    "        # pytorch2.0 new feature, complile model for fast training\n",
    "        embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "\n",
    "    # define model training hyperparameters\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # define optimal epochs\n",
    "    epochs = round(get_optimal_epochs(study)*epoch_scale)\n",
    "\n",
    "        # steps per epoch\n",
    "    steps = round(N_CATCHMENTS * TRAIN_VAL_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_VAL_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "\n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, selected_catchments) = dataset.get_random_batch(\n",
    "                batch_size\n",
    "            )\n",
    "\n",
    "            x_batch, y_batch, selected_catchments = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "                selected_catchments.to(computing_device),\n",
    "            )\n",
    "\n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(selected_catchments)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_datasets = [\"nldas\", \"daymet\", \"maurer\"]\n",
    "\n",
    "for i in range(len(forcing_datasets)):\n",
    "    forcing_dataset = forcing_datasets[i]\n",
    "    \n",
    "    study = load_study(forcing_dataset)\n",
    "    dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "    \n",
    "    embedding, decoder = get_final_model(study, dtrain_val)\n",
    "    \n",
    "    torch.save(embedding.cpu(), f\"data/671_camels_{forcing_dataset}_embedding.pt\")\n",
    "    torch.save(decoder.cpu(), f\"data/671_camels_{forcing_dataset}_decoder.pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective_builder:\n",
    "    def __init__(self, x, y, eval_fun):\n",
    "        self.eval_fun = eval_fun\n",
    "        self.x = x.contiguous()\n",
    "        self.y = y.contiguous()\n",
    "    \n",
    "    def eval(self, code, return_summary = True):\n",
    "        \n",
    "        # numpy to torch tensor\n",
    "        code = torch.from_numpy(code).unsqueeze(0).to(dtype=torch.float32).to(computing_device)\n",
    "        code = code.expand(self.x.shape[0], -1)\n",
    "        \n",
    "        # BASE_LENGTH is from global\n",
    "        pred = decoder.decode(code, self.x).view(-1).detach().cpu().numpy()\n",
    "\n",
    "        ob = self.y.view(-1).detach().cpu().numpy()\n",
    "        \n",
    "        if return_summary:\n",
    "          gof = self.eval_fun(simulated_array=pred, observed_array=ob)\n",
    "          return gof\n",
    "        else:\n",
    "          return pred, ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_catchment(selected_catchment):\n",
    "    \n",
    "    x = x_batch_test[:,selected_catchment,:,:]\n",
    "    y = y_batch_test[:,selected_catchment,:]\n",
    "\n",
    "    x, y = x.to(computing_device), y.to(computing_device)\n",
    "\n",
    "    fn_kge = Objective_builder(x,y,HydroErr.kge_2009)\n",
    "    fn_nse = Objective_builder(x,y,HydroErr.nse)\n",
    "    \n",
    "    kge = fn_kge.eval(catchment_embeddings[selected_catchment,:].cpu().detach().numpy())\n",
    "    nse = fn_nse.eval(catchment_embeddings[selected_catchment,:].cpu().detach().numpy())\n",
    "    \n",
    "    return  kge, nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491 5492\n",
      " 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505 5506\n",
      " 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519 5520\n",
      " 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533 5534\n",
      " 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547 5548\n",
      " 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561 5562\n",
      " 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575 5576\n",
      " 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589 5590\n",
      " 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603 5604\n",
      " 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617 5618\n",
      " 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631 5632\n",
      " 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645 5646\n",
      " 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659 5660\n",
      " 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673 5674\n",
      " 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687 5688\n",
      " 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701 5702\n",
      " 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715 5716\n",
      " 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729 5730\n",
      " 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743 5744\n",
      " 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757 5758\n",
      " 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771 5772\n",
      " 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785 5786\n",
      " 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799 5800\n",
      " 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813 5814\n",
      " 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827 5828\n",
      " 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [1827 1828 1829 ... 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5328 5329 5330 5331 5332 5333 5334 5335 5336 5337 5338 5339 5340 5341\n",
      " 5342 5343 5344 5345 5346 5347 5348 5349 5350 5351 5352 5353 5354 5355\n",
      " 5356 5357 5358 5359 5360 5361 5362 5363 5364 5365 5366 5367 5368 5369\n",
      " 5370 5371 5372 5373 5374 5375 5376 5377 5378 5379 5380 5381 5382 5383\n",
      " 5384 5385 5386 5387 5388 5389 5390 5391 5392 5393 5394 5395 5396 5397\n",
      " 5398 5399 5400 5401 5402 5403 5404 5405 5406 5407 5408 5409 5410 5411\n",
      " 5412 5413 5414 5415 5416 5417 5418 5419 5420 5421 5422 5423 5424 5425\n",
      " 5426 5427 5428 5429 5430 5431 5432 5433 5434 5435 5436 5437 5438 5439\n",
      " 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449 5450 5451 5452 5453\n",
      " 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463 5464 5465 5466 5467\n",
      " 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477 5478 5479 5480 5481\n",
      " 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491 5492 5493 5494 5495\n",
      " 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505 5506 5507 5508 5509\n",
      " 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519 5520 5521 5522 5523\n",
      " 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533 5534 5535 5536 5537\n",
      " 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547 5548 5549 5550 5551\n",
      " 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561 5562 5563 5564 5565\n",
      " 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575 5576 5577 5578 5579\n",
      " 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589 5590 5591 5592 5593\n",
      " 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603 5604 5605 5606 5607\n",
      " 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617 5618 5619 5620 5621\n",
      " 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631 5632 5633 5634 5635\n",
      " 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645 5646 5647 5648 5649\n",
      " 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659 5660 5661 5662 5663\n",
      " 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673 5674 5675 5676 5677\n",
      " 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687 5688 5689 5690 5691\n",
      " 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701 5702 5703 5704 5705\n",
      " 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715 5716 5717 5718 5719\n",
      " 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729 5730 5731 5732 5733\n",
      " 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743 5744 5745 5746 5747\n",
      " 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757 5758 5759 5760 5761\n",
      " 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771 5772 5773 5774 5775\n",
      " 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785 5786 5787 5788 5789\n",
      " 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799 5800 5801 5802 5803\n",
      " 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813 5814 5815 5816 5817\n",
      " 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827 5828 5829 5830 5831\n",
      " 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5114 5115 5116 5117 5118 5119 5120 5121 5122 5123 5124 5125 5126 5129\n",
      " 5130 5131 5132 5133 5134 5135 5136 5137 5138 5139 5140 5141 5142 5143\n",
      " 5146 5147 5148 5149 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159\n",
      " 5160 5161 5162 5163 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173\n",
      " 5174 5175 5176 5177 5178 5179 5180 5181 5182 5184 5185 5186 5187 5188\n",
      " 5191 5192 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203 5204 5205\n",
      " 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217 5218 5219\n",
      " 5220 5221 5223 5224 5225 5226 5228 5229 5230 5231 5232 5233 5234 5237\n",
      " 5238 5239 5240 5243 5244 5245 5246 5247 5248 5249 5250 5251 5252 5253\n",
      " 5254 5255 5256 5257 5258 5259 5260 5261 5262 5263 5264 5265 5266 5268\n",
      " 5269 5270 5271 5272 5273 5274 5276 5277 5278 5279 5280 5281 5282 5283\n",
      " 5284 5285 5286 5287 5288 5289 5290 5291 5292 5293 5294 5295 5296 5297\n",
      " 5298 5299 5300 5301 5302 5303 5304 5305 5306 5307 5308 5309 5310 5311\n",
      " 5312 5313 5314 5315 5316 5317 5318 5319 5320 5321 5322 5323 5324 5325\n",
      " 5326 5327 5328 5329 5330 5331 5332 5333 5334 5335 5336 5337 5338 5339\n",
      " 5340 5341 5342 5343 5344 5345 5346 5347 5348 5349 5350 5351 5352 5353\n",
      " 5354 5355 5356 5357 5358 5359 5360 5361 5362 5363 5364 5365 5366 5367\n",
      " 5368 5369 5370 5371 5372 5373 5374 5375 5376 5377 5378 5379 5380 5381\n",
      " 5382 5383 5384 5385 5386 5387 5388 5389 5390 5391 5392 5393 5394 5395\n",
      " 5396 5397 5398 5399 5400 5401 5402 5403 5404 5405 5406 5407 5408 5409\n",
      " 5410 5411 5412 5413 5414 5415 5416 5417 5418 5419 5420 5421 5422 5423\n",
      " 5424 5425 5426 5427 5428 5429 5430 5431 5432 5433 5434 5435 5436 5437\n",
      " 5438 5439 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449 5450 5451\n",
      " 5452 5453 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463 5464 5465\n",
      " 5466 5467 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477 5478 5479\n",
      " 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491 5492 5493\n",
      " 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505 5506 5507\n",
      " 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519 5520 5521\n",
      " 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533 5534 5535\n",
      " 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547 5548 5549\n",
      " 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561 5562 5563\n",
      " 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575 5576 5577\n",
      " 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589 5590 5591\n",
      " 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603 5604 5605\n",
      " 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617 5618 5619\n",
      " 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631 5632 5633\n",
      " 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645 5646 5647\n",
      " 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659 5660 5661\n",
      " 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673 5674 5675\n",
      " 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687 5688 5689\n",
      " 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701 5702 5703\n",
      " 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715 5716 5717\n",
      " 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729 5730 5731\n",
      " 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743 5744 5745\n",
      " 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757 5758 5759\n",
      " 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771 5772 5773\n",
      " 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785 5786 5787\n",
      " 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799 5800 5801\n",
      " 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813 5814 5815\n",
      " 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827 5828 5829\n",
      " 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5114 5115 5116 5117 5118 5119 5120 5121 5122 5123 5124 5125 5126 5127\n",
      " 5128 5129 5130 5131 5132 5133 5134 5135 5136 5137 5138 5139 5140 5141\n",
      " 5142 5143 5144 5145 5146 5147 5148 5149 5150 5151 5152 5153 5154 5155\n",
      " 5156 5157 5158 5159 5160 5161 5162 5163 5164 5165 5166 5167 5168 5169\n",
      " 5170 5171 5172 5173 5174 5175 5176 5177 5178 5179 5180 5181 5182 5183\n",
      " 5184 5185 5186 5188 5189 5190 5191 5192 5193 5194 5195 5196 5197 5198\n",
      " 5199 5200 5201 5202 5203 5204 5206 5207 5208 5209 5210 5211 5212 5213\n",
      " 5214 5215 5216 5217 5218 5219 5220 5221 5222 5223 5224 5225 5227 5228\n",
      " 5229 5230 5231 5232 5233 5234 5236 5237 5238 5239 5240 5244 5246 5247\n",
      " 5248 5250 5252 5253 5254 5255 5256 5257 5258 5259 5260 5261 5262 5263\n",
      " 5264 5265 5266 5267 5269 5270 5271 5272 5273 5275 5276 5277 5278 5279\n",
      " 5280 5281 5282 5283 5284 5285 5286 5287 5288 5289 5290 5291 5292 5293\n",
      " 5294 5295 5296 5297 5298 5299 5300 5301 5302 5303 5304 5305 5306 5307\n",
      " 5308 5309 5310 5311 5312 5313 5314 5315 5316 5317 5318 5319 5320 5321\n",
      " 5322 5323 5324 5325 5326 5327 5328 5329 5330 5331 5332 5333 5334 5335\n",
      " 5336 5337 5338 5339 5340 5341 5342 5343 5344 5345 5346 5347 5348 5349\n",
      " 5350 5351 5352 5353 5354 5355 5356 5357 5358 5359 5360 5361 5362 5363\n",
      " 5364 5365 5366 5367 5368 5369 5370 5371 5372 5373 5374 5375 5376 5377\n",
      " 5378 5379 5380 5381 5382 5383 5384 5385 5386 5387 5388 5389 5390 5391\n",
      " 5392 5393 5394 5395 5396 5397 5398 5399 5400 5401 5402 5403 5404 5405\n",
      " 5406 5407 5408 5409 5410 5411 5412 5413 5414 5415 5416 5417 5418 5419\n",
      " 5420 5421 5422 5423 5424 5425 5426 5427 5428 5429 5430 5431 5432 5433\n",
      " 5434 5437 5438 5439 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449\n",
      " 5450 5451 5452 5453 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463\n",
      " 5464 5465 5466 5467 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477\n",
      " 5478 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491\n",
      " 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505\n",
      " 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519\n",
      " 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533\n",
      " 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547\n",
      " 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561\n",
      " 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575\n",
      " 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589\n",
      " 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603\n",
      " 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617\n",
      " 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631\n",
      " 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645\n",
      " 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659\n",
      " 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673\n",
      " 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687\n",
      " 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701\n",
      " 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715\n",
      " 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729\n",
      " 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743\n",
      " 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757\n",
      " 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771\n",
      " 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785\n",
      " 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799\n",
      " 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813\n",
      " 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827\n",
      " 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [   0 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491\n",
      " 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505\n",
      " 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519\n",
      " 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533\n",
      " 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547\n",
      " 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561\n",
      " 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575\n",
      " 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589\n",
      " 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603\n",
      " 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617\n",
      " 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631\n",
      " 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645\n",
      " 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659\n",
      " 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673\n",
      " 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687\n",
      " 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701\n",
      " 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715\n",
      " 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729\n",
      " 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743\n",
      " 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757\n",
      " 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771\n",
      " 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785\n",
      " 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799\n",
      " 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813\n",
      " 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827\n",
      " 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [2458 2459 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490\n",
      " 5491 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504\n",
      " 5505 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518\n",
      " 5519 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532\n",
      " 5533 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546\n",
      " 5547 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560\n",
      " 5561 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574\n",
      " 5575 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588\n",
      " 5589 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602\n",
      " 5603 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616\n",
      " 5617 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630\n",
      " 5631 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644\n",
      " 5645 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658\n",
      " 5659 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672\n",
      " 5673 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686\n",
      " 5687 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700\n",
      " 5701 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714\n",
      " 5715 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728\n",
      " 5729 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742\n",
      " 5743 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756\n",
      " 5757 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770\n",
      " 5771 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784\n",
      " 5785 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798\n",
      " 5799 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812\n",
      " 5813 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826\n",
      " 5827 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5114 5115 5116 5117 5118 5119 5120 5121 5122 5123 5124 5125 5126 5127\n",
      " 5128 5129 5130 5131 5132 5133 5134 5135 5136 5137 5138 5139 5140 5141\n",
      " 5142 5143 5144 5145 5146 5147 5148 5149 5150 5151 5152 5153 5154 5155\n",
      " 5156 5157 5158 5159 5160 5161 5162 5163 5164 5165 5166 5167 5168 5169\n",
      " 5170 5171 5172 5173 5174 5175 5176 5177 5178 5179 5180 5181 5182 5183\n",
      " 5184 5185 5186 5187 5188 5189 5190 5191 5192 5193 5194 5195 5196 5197\n",
      " 5198 5199 5200 5201 5202 5203 5204 5205 5206 5207 5208 5209 5210 5211\n",
      " 5212 5213 5214 5215 5216 5217 5218 5219 5220 5221 5222 5223 5224 5225\n",
      " 5226 5227 5228 5229 5230 5231 5232 5233 5234 5235 5236 5237 5238 5239\n",
      " 5240 5241 5242 5243 5244 5245 5246 5247 5248 5249 5250 5251 5252 5253\n",
      " 5254 5255 5256 5257 5258 5259 5260 5261 5262 5263 5264 5265 5266 5267\n",
      " 5268 5269 5270 5271 5272 5273 5274 5275 5276 5277 5278 5279 5280 5281\n",
      " 5282 5283 5284 5285 5286 5287 5288 5289 5290 5291 5292 5293 5294 5295\n",
      " 5296 5297 5298 5299 5300 5301 5302 5303 5304 5305 5306 5307 5308 5309\n",
      " 5310 5311 5312 5313 5314 5315 5316 5317 5318 5319 5320 5321 5322 5323\n",
      " 5324 5325 5326 5327 5328 5329 5330 5331 5332 5333 5334 5335 5336 5337\n",
      " 5338 5339 5340 5341 5342 5343 5344 5345 5346 5347 5348 5349 5350 5351\n",
      " 5352 5353 5354 5355 5356 5357 5358 5359 5360 5361 5362 5363 5364 5365\n",
      " 5366 5367 5368 5369 5370 5371 5372 5373 5374 5375 5376 5377 5378 5379\n",
      " 5380 5381 5382 5383 5384 5385 5386 5387 5388 5389 5390 5391 5392 5393\n",
      " 5394 5395 5396 5397 5398 5399 5400 5401 5402 5403 5404 5405 5406 5407\n",
      " 5408 5409 5410 5411 5412 5413 5414 5415 5416 5417 5418 5419 5420 5421\n",
      " 5422 5423 5424 5425 5426 5427 5428 5429 5430 5431 5432 5433 5434 5435\n",
      " 5436 5437 5438 5439 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449\n",
      " 5450 5451 5452 5453 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463\n",
      " 5464 5465 5466 5467 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477\n",
      " 5478 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491\n",
      " 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505\n",
      " 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519\n",
      " 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533\n",
      " 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547\n",
      " 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561\n",
      " 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575\n",
      " 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589\n",
      " 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603\n",
      " 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617\n",
      " 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631\n",
      " 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645\n",
      " 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659\n",
      " 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673\n",
      " 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687\n",
      " 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701\n",
      " 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715\n",
      " 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729\n",
      " 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743\n",
      " 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757\n",
      " 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771\n",
      " 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785\n",
      " 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799\n",
      " 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813\n",
      " 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827\n",
      " 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [4387 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491\n",
      " 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505\n",
      " 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519\n",
      " 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533\n",
      " 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547\n",
      " 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561\n",
      " 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575\n",
      " 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589\n",
      " 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603\n",
      " 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617\n",
      " 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631\n",
      " 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645\n",
      " 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659\n",
      " 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673\n",
      " 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687\n",
      " 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701\n",
      " 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715\n",
      " 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729\n",
      " 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743\n",
      " 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757\n",
      " 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771\n",
      " 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785\n",
      " 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799\n",
      " 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813\n",
      " 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827\n",
      " 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [4841 4842 4843 4844 4845 4846 4847 4848 4849 4850 4851 4852 4853 4854\n",
      " 4855 4856 4857 4858 4859 4860 4861 4862 4863 4864 4865 4866 4867 4868\n",
      " 4869 4870 4871 4872 4873 4874 4875 4876 4877 4878 4879 4880 4881 4882\n",
      " 4883 4884 4885 4886 4887 4888 4889 4890 4891 4892 4893 4894 4895 4896\n",
      " 4897 4898 4899 4900 4901 4902 4903 4904 4905 4906 4907 4908 4909 4910\n",
      " 4911 4912 4913 4914 4915 4916 4917 4918 4919 4920 4921 4922 4923 4924\n",
      " 4925 4926 4927 4928 4929 4930 4931 4932 4933 4934 4935 4936 4937 4938\n",
      " 4939 4940 4941 4942 4943 4944 4945 4946 4947 4948 4949 4950 4951 4952\n",
      " 4953 4954 4955 4956 4957 4958 4959 4960 4961 4962 4963 4964 4965 4966\n",
      " 4967 4968 4969 4970 4971 4972 4973 4974 4975 4976 4977 4978 4979 4980\n",
      " 4981 4982 4983 4984 4985 4986 4987 4988 4989 4990 4991 4992 4993 4994\n",
      " 4995 4996 4997 4998 4999 5000 5001 5002 5003 5004 5005 5006 5007 5008\n",
      " 5009 5010 5011 5012 5013 5014 5015 5016 5017 5018 5019 5020 5021 5022\n",
      " 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036\n",
      " 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050\n",
      " 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064\n",
      " 5065 5066 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 5077 5078\n",
      " 5079 5080 5081 5082 5083 5084 5085 5086 5087 5088 5089 5090 5091 5092\n",
      " 5093 5094 5095 5096 5097 5098 5099 5100 5101 5102 5103 5104 5105 5106\n",
      " 5107 5108 5109 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119 5120\n",
      " 5121 5122 5123 5124 5125 5126 5127 5128 5129 5130 5131 5132 5133 5134\n",
      " 5135 5136 5137 5138 5139 5140 5141 5142 5143 5144 5145 5146 5147 5148\n",
      " 5149 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159 5160 5161 5162\n",
      " 5163 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173 5174 5175 5176\n",
      " 5177 5178 5179 5180 5181 5182 5183 5184 5185 5186 5187 5188 5189 5190\n",
      " 5191 5192 5193 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203 5204\n",
      " 5205 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217 5218\n",
      " 5219 5220 5221 5222 5223 5224 5225 5226 5227 5228 5229 5230 5231 5232\n",
      " 5233 5234 5235 5236 5237 5238 5239 5240 5241 5242 5243 5244 5245 5246\n",
      " 5247 5248 5249 5250 5251 5252 5253 5254 5255 5256 5257 5258 5259 5260\n",
      " 5261 5262 5263 5264 5265 5266 5267 5268 5269 5270 5271 5272 5273 5274\n",
      " 5275 5276 5277 5278 5279 5280 5281 5282 5283 5284 5285 5286 5287 5288\n",
      " 5289 5290 5291 5292 5293 5294 5295 5296 5297 5298 5299 5300 5301 5302\n",
      " 5303 5304 5305 5306 5307 5308 5309 5310 5311 5312 5313 5314 5315 5316\n",
      " 5317 5318 5319 5320 5321 5322 5323 5324 5325 5326 5327 5328 5329 5330\n",
      " 5331 5332 5333 5334 5335 5336 5337 5338 5339 5340 5341 5342 5343 5344\n",
      " 5345 5346 5347 5348 5349 5350 5351 5352 5353 5354 5355 5356 5357 5358\n",
      " 5359 5360 5361 5362 5363 5364 5365 5366 5367 5368 5369 5370 5371 5372\n",
      " 5373 5374 5375 5376 5377 5378 5379 5380 5381 5382 5383 5384 5385 5386\n",
      " 5387 5388 5389 5390 5391 5392 5393 5394 5395 5396 5397 5398 5399 5400\n",
      " 5401 5402 5403 5404 5405 5406 5407 5408 5409 5410 5411 5412 5413 5414\n",
      " 5415 5416 5417 5418 5419 5420 5421 5422 5423 5424 5425 5426 5427 5428\n",
      " 5429 5430 5431 5432 5433 5434 5435 5436 5437 5438 5439 5440 5441 5442\n",
      " 5443 5444 5445 5446 5447 5448 5449 5450 5451 5452 5453 5454 5455 5456\n",
      " 5457 5458 5459 5460 5461 5462 5463 5464 5465 5466 5467 5468 5469 5470\n",
      " 5471 5472 5473 5474 5475 5476 5477 5478 5479 5480 5481 5482 5483 5484\n",
      " 5485 5486 5487 5488 5489 5490 5491 5492 5493 5494 5495 5496 5497 5498\n",
      " 5499 5500 5501 5502 5503 5504 5505 5506 5507 5508 5509 5510 5511 5512\n",
      " 5513 5514 5515 5516 5517 5518 5519 5520 5521 5522 5523 5524 5525 5526\n",
      " 5527 5528 5529 5530 5531 5532 5533 5534 5535 5536 5537 5538 5539 5540\n",
      " 5541 5542 5543 5544 5545 5546 5547 5548 5549 5550 5551 5552 5553 5554\n",
      " 5555 5556 5557 5558 5559 5560 5561 5562 5563 5564 5565 5566 5567 5568\n",
      " 5569 5570 5571 5572 5573 5574 5575 5576 5577 5578 5579 5580 5581 5582\n",
      " 5583 5584 5585 5586 5587 5588 5589 5590 5591 5592 5593 5594 5595 5596\n",
      " 5597 5598 5599 5600 5601 5602 5603 5604 5605 5606 5607 5608 5609 5610\n",
      " 5611 5612 5613 5614 5615 5616 5617 5618 5619 5620 5621 5622 5623 5624\n",
      " 5625 5626 5627 5628 5629 5630 5631 5632 5633 5634 5635 5636 5637 5638\n",
      " 5639 5640 5641 5642 5643 5644 5645 5646 5647 5648 5649 5650 5651 5652\n",
      " 5653 5654 5655 5656 5657 5658 5659 5660 5661 5662 5663 5664 5665 5666\n",
      " 5667 5668 5669 5670 5671 5672 5673 5674 5675 5676 5677 5678 5679 5680\n",
      " 5681 5682 5683 5684 5685 5686 5687 5688 5689 5690 5691 5692 5693 5694\n",
      " 5695 5696 5697 5698 5699 5700 5701 5702 5703 5704 5705 5706 5707 5708\n",
      " 5709 5710 5711 5712 5713 5714 5715 5716 5717 5718 5719 5720 5721 5722\n",
      " 5723 5724 5725 5726 5727 5728 5729 5730 5731 5732 5733 5734 5735 5736\n",
      " 5737 5738 5739 5740 5741 5742 5743 5744 5745 5746 5747 5748 5749 5750\n",
      " 5751 5752 5753 5754 5755 5756 5757 5758 5759 5760 5761 5762 5763 5764\n",
      " 5765 5766 5767 5768 5769 5770 5771 5772 5773 5774 5775 5776 5777 5778\n",
      " 5779 5780 5781 5782 5783 5784 5785 5786 5787 5788 5789 5790 5791 5792\n",
      " 5793 5794 5795 5796 5797 5798 5799 5800 5801 5802 5803 5804 5805 5806\n",
      " 5807 5808 5809 5810 5811 5812 5813 5814 5815 5816 5817 5818 5819 5820\n",
      " 5821 5822 5823 5824 5825 5826 5827 5828 5829 5830 5831 5832 5833 5834\n",
      " 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [   0 4841 4842 4843 4844 4845 4846 4847 4848 4849 4850 4851 4852 4853\n",
      " 4854 4855 4856 4857 4858 4859 4860 4861 4862 4863 4864 4865 4866 4867\n",
      " 4868 4869 4870 4871 4872 4873 4874 4875 4876 4877 4878 4879 4880 4881\n",
      " 4882 4883 4884 4885 4886 4887 4888 4889 4890 4891 4892 4893 4894 4895\n",
      " 4896 4897 4898 4899 4900 4901 4902 4903 4904 4905 4906 4907 4908 4909\n",
      " 4910 4911 4912 4913 4914 4915 4916 4917 4918 4919 4920 4921 4922 4923\n",
      " 4924 4925 4926 4927 4928 4929 4930 4931 4932 4933 4934 4935 4936 4937\n",
      " 4938 4939 4940 4941 4942 4943 4944 4945 4946 4947 4948 4949 4950 4951\n",
      " 4952 4953 4954 4955 4956 4957 4958 4959 4960 4961 4962 4963 4964 4965\n",
      " 4966 4967 4968 4969 4970 4971 4972 4973 4974 4975 4976 4977 4978 4979\n",
      " 4980 4981 4982 4983 4984 4985 4986 4987 4988 4989 4990 4991 4992 4993\n",
      " 4994 4995 4996 4997 4998 4999 5000 5001 5002 5003 5004 5005 5006 5007\n",
      " 5008 5009 5010 5011 5012 5013 5014 5015 5016 5017 5018 5019 5020 5021\n",
      " 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035\n",
      " 5036 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049\n",
      " 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063\n",
      " 5064 5065 5066 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 5077\n",
      " 5078 5079 5080 5081 5082 5083 5084 5085 5086 5087 5088 5089 5090 5091\n",
      " 5092 5093 5094 5095 5096 5097 5098 5099 5100 5101 5102 5103 5104 5105\n",
      " 5106 5107 5108 5109 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119\n",
      " 5120 5121 5122 5123 5124 5125 5126 5127 5128 5129 5130 5131 5132 5133\n",
      " 5134 5135 5136 5137 5138 5139 5140 5141 5142 5143 5144 5145 5146 5147\n",
      " 5148 5149 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159 5160 5161\n",
      " 5162 5163 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173 5174 5175\n",
      " 5176 5177 5178 5179 5180 5181 5182 5183 5184 5185 5186 5187 5188 5189\n",
      " 5190 5191 5192 5193 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203\n",
      " 5204 5205 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217\n",
      " 5218 5219 5220 5221 5222 5223 5224 5225 5226 5227 5228 5229 5230 5231\n",
      " 5232 5233 5234 5235 5236 5237 5238 5239 5240 5241 5242 5243 5244 5245\n",
      " 5246 5247 5248 5249 5250 5251 5252 5253 5254 5255 5256 5257 5258 5259\n",
      " 5260 5261 5262 5263 5264 5265 5266 5267 5268 5269 5270 5271 5272 5273\n",
      " 5274 5275 5276 5277 5278 5279 5280 5281 5282 5283 5284 5285 5286 5287\n",
      " 5288 5289 5290 5291 5292 5293 5294 5295 5296 5297 5298 5299 5300 5301\n",
      " 5302 5303 5304 5305 5306 5307 5308 5309 5310 5311 5312 5313 5314 5315\n",
      " 5316 5317 5318 5319 5320 5321 5322 5323 5324 5325 5326 5327 5328 5329\n",
      " 5330 5331 5332 5333 5334 5335 5336 5337 5338 5339 5340 5341 5342 5343\n",
      " 5344 5345 5346 5347 5348 5349 5350 5351 5352 5353 5354 5355 5356 5357\n",
      " 5358 5359 5360 5361 5362 5363 5364 5365 5366 5367 5368 5369 5370 5371\n",
      " 5372 5373 5374 5375 5376 5377 5378 5379 5380 5381 5382 5383 5384 5385\n",
      " 5386 5387 5388 5389 5390 5391 5392 5393 5394 5395 5396 5397 5398 5399\n",
      " 5400 5401 5402 5403 5404 5405 5406 5407 5408 5409 5410 5411 5412 5413\n",
      " 5414 5415 5416 5417 5418 5419 5420 5421 5422 5423 5424 5425 5426 5427\n",
      " 5428 5429 5430 5431 5432 5433 5434 5435 5436 5437 5438 5439 5440 5441\n",
      " 5442 5443 5444 5445 5446 5447 5448 5449 5450 5451 5452 5453 5454 5455\n",
      " 5456 5457 5458 5459 5460 5461 5462 5463 5464 5465 5466 5467 5468 5469\n",
      " 5470 5471 5472 5473 5474 5475 5476 5477 5478 5479 5480 5481 5482 5483\n",
      " 5484 5485 5486 5487 5488 5489 5490 5491 5492 5493 5494 5495 5496 5497\n",
      " 5498 5499 5500 5501 5502 5503 5504 5505 5506 5507 5508 5509 5510 5511\n",
      " 5512 5513 5514 5515 5516 5517 5518 5519 5520 5521 5522 5523 5524 5525\n",
      " 5526 5527 5528 5529 5530 5531 5532 5533 5534 5535 5536 5537 5538 5539\n",
      " 5540 5541 5542 5543 5544 5545 5546 5547 5548 5549 5550 5551 5552 5553\n",
      " 5554 5555 5556 5557 5558 5559 5560 5561 5562 5563 5564 5565 5566 5567\n",
      " 5568 5569 5570 5571 5572 5573 5574 5575 5576 5577 5578 5579 5580 5581\n",
      " 5582 5583 5584 5585 5586 5587 5588 5589 5590 5591 5592 5593 5594 5595\n",
      " 5596 5597 5598 5599 5600 5601 5602 5603 5604 5605 5606 5607 5608 5609\n",
      " 5610 5611 5612 5613 5614 5615 5616 5617 5618 5619 5620 5621 5622 5623\n",
      " 5624 5625 5626 5627 5628 5629 5630 5631 5632 5633 5634 5635 5636 5637\n",
      " 5638 5639 5640 5641 5642 5643 5644 5645 5646 5647 5648 5649 5650 5651\n",
      " 5652 5653 5654 5655 5656 5657 5658 5659 5660 5661 5662 5663 5664 5665\n",
      " 5666 5667 5668 5669 5670 5671 5672 5673 5674 5675 5676 5677 5678 5679\n",
      " 5680 5681 5682 5683 5684 5685 5686 5687 5688 5689 5690 5691 5692 5693\n",
      " 5694 5695 5696 5697 5698 5699 5700 5701 5702 5703 5704 5705 5706 5707\n",
      " 5708 5709 5710 5711 5712 5713 5714 5715 5716 5717 5718 5719 5720 5721\n",
      " 5722 5723 5724 5725 5726 5727 5728 5729 5730 5731 5732 5733 5734 5735\n",
      " 5736 5737 5738 5739 5740 5741 5742 5743 5744 5745 5746 5747 5748 5749\n",
      " 5750 5751 5752 5753 5754 5755 5756 5757 5758 5759 5760 5761 5762 5763\n",
      " 5764 5765 5766 5767 5768 5769 5770 5771 5772 5773 5774 5775 5776 5777\n",
      " 5778 5779 5780 5781 5782 5783 5784 5785 5786 5787 5788 5789 5790 5791\n",
      " 5792 5793 5794 5795 5796 5797 5798 5799 5800 5801 5802 5803 5804 5805\n",
      " 5806 5807 5808 5809 5810 5811 5812 5813 5814 5815 5816 5817 5818 5819\n",
      " 5820 5821 5822 5823 5824 5825 5826 5827 5828 5829 5830 5831 5832 5833\n",
      " 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [2458 2459 4841 ... 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [4387 4841 4842 4843 4844 4845 4846 4847 4848 4849 4850 4851 4852 4853\n",
      " 4854 4855 4856 4857 4858 4859 4860 4861 4862 4863 4864 4865 4866 4867\n",
      " 4868 4869 4870 4871 4872 4873 4874 4875 4876 4877 4878 4879 4880 4881\n",
      " 4882 4883 4884 4885 4886 4887 4888 4889 4890 4891 4892 4893 4894 4895\n",
      " 4896 4897 4898 4899 4900 4901 4902 4903 4904 4905 4906 4907 4908 4909\n",
      " 4910 4911 4912 4913 4914 4915 4916 4917 4918 4919 4920 4921 4922 4923\n",
      " 4924 4925 4926 4927 4928 4929 4930 4931 4932 4933 4934 4935 4936 4937\n",
      " 4938 4939 4940 4941 4942 4943 4944 4945 4946 4947 4948 4949 4950 4951\n",
      " 4952 4953 4954 4955 4956 4957 4958 4959 4960 4961 4962 4963 4964 4965\n",
      " 4966 4967 4968 4969 4970 4971 4972 4973 4974 4975 4976 4977 4978 4979\n",
      " 4980 4981 4982 4983 4984 4985 4986 4987 4988 4989 4990 4991 4992 4993\n",
      " 4994 4995 4996 4997 4998 4999 5000 5001 5002 5003 5004 5005 5006 5007\n",
      " 5008 5009 5010 5011 5012 5013 5014 5015 5016 5017 5018 5019 5020 5021\n",
      " 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035\n",
      " 5036 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049\n",
      " 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063\n",
      " 5064 5065 5066 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 5077\n",
      " 5078 5079 5080 5081 5082 5083 5084 5085 5086 5087 5088 5089 5090 5091\n",
      " 5092 5093 5094 5095 5096 5097 5098 5099 5100 5101 5102 5103 5104 5105\n",
      " 5106 5107 5108 5109 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119\n",
      " 5120 5121 5122 5123 5124 5125 5126 5127 5128 5129 5130 5131 5132 5133\n",
      " 5134 5135 5136 5137 5138 5139 5140 5141 5142 5143 5144 5145 5146 5147\n",
      " 5148 5149 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159 5160 5161\n",
      " 5162 5163 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173 5174 5175\n",
      " 5176 5177 5178 5179 5180 5181 5182 5183 5184 5185 5186 5187 5188 5189\n",
      " 5190 5191 5192 5193 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203\n",
      " 5204 5205 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217\n",
      " 5218 5219 5220 5221 5222 5223 5224 5225 5226 5227 5228 5229 5230 5231\n",
      " 5232 5233 5234 5235 5236 5237 5238 5239 5240 5241 5242 5243 5244 5245\n",
      " 5246 5247 5248 5249 5250 5251 5252 5253 5254 5255 5256 5257 5258 5259\n",
      " 5260 5261 5262 5263 5264 5265 5266 5267 5268 5269 5270 5271 5272 5273\n",
      " 5274 5275 5276 5277 5278 5279 5280 5281 5282 5283 5284 5285 5286 5287\n",
      " 5288 5289 5290 5291 5292 5293 5294 5295 5296 5297 5298 5299 5300 5301\n",
      " 5302 5303 5304 5305 5306 5307 5308 5309 5310 5311 5312 5313 5314 5315\n",
      " 5316 5317 5318 5319 5320 5321 5322 5323 5324 5325 5326 5327 5328 5329\n",
      " 5330 5331 5332 5333 5334 5335 5336 5337 5338 5339 5340 5341 5342 5343\n",
      " 5344 5345 5346 5347 5348 5349 5350 5351 5352 5353 5354 5355 5356 5357\n",
      " 5358 5359 5360 5361 5362 5363 5364 5365 5366 5367 5368 5369 5370 5371\n",
      " 5372 5373 5374 5375 5376 5377 5378 5379 5380 5381 5382 5383 5384 5385\n",
      " 5386 5387 5388 5389 5390 5391 5392 5393 5394 5395 5396 5397 5398 5399\n",
      " 5400 5401 5402 5403 5404 5405 5406 5407 5408 5409 5410 5411 5412 5413\n",
      " 5414 5415 5416 5417 5418 5419 5420 5421 5422 5423 5424 5425 5426 5427\n",
      " 5428 5429 5430 5431 5432 5433 5434 5435 5436 5437 5438 5439 5440 5441\n",
      " 5442 5443 5444 5445 5446 5447 5448 5449 5450 5451 5452 5453 5454 5455\n",
      " 5456 5457 5458 5459 5460 5461 5462 5463 5464 5465 5466 5467 5468 5469\n",
      " 5470 5471 5472 5473 5474 5475 5476 5477 5478 5479 5480 5481 5482 5483\n",
      " 5484 5485 5486 5487 5488 5489 5490 5491 5492 5493 5494 5495 5496 5497\n",
      " 5498 5499 5500 5501 5502 5503 5504 5505 5506 5507 5508 5509 5510 5511\n",
      " 5512 5513 5514 5515 5516 5517 5518 5519 5520 5521 5522 5523 5524 5525\n",
      " 5526 5527 5528 5529 5530 5531 5532 5533 5534 5535 5536 5537 5538 5539\n",
      " 5540 5541 5542 5543 5544 5545 5546 5547 5548 5549 5550 5551 5552 5553\n",
      " 5554 5555 5556 5557 5558 5559 5560 5561 5562 5563 5564 5565 5566 5567\n",
      " 5568 5569 5570 5571 5572 5573 5574 5575 5576 5577 5578 5579 5580 5581\n",
      " 5582 5583 5584 5585 5586 5587 5588 5589 5590 5591 5592 5593 5594 5595\n",
      " 5596 5597 5598 5599 5600 5601 5602 5603 5604 5605 5606 5607 5608 5609\n",
      " 5610 5611 5612 5613 5614 5615 5616 5617 5618 5619 5620 5621 5622 5623\n",
      " 5624 5625 5626 5627 5628 5629 5630 5631 5632 5633 5634 5635 5636 5637\n",
      " 5638 5639 5640 5641 5642 5643 5644 5645 5646 5647 5648 5649 5650 5651\n",
      " 5652 5653 5654 5655 5656 5657 5658 5659 5660 5661 5662 5663 5664 5665\n",
      " 5666 5667 5668 5669 5670 5671 5672 5673 5674 5675 5676 5677 5678 5679\n",
      " 5680 5681 5682 5683 5684 5685 5686 5687 5688 5689 5690 5691 5692 5693\n",
      " 5694 5695 5696 5697 5698 5699 5700 5701 5702 5703 5704 5705 5706 5707\n",
      " 5708 5709 5710 5711 5712 5713 5714 5715 5716 5717 5718 5719 5720 5721\n",
      " 5722 5723 5724 5725 5726 5727 5728 5729 5730 5731 5732 5733 5734 5735\n",
      " 5736 5737 5738 5739 5740 5741 5742 5743 5744 5745 5746 5747 5748 5749\n",
      " 5750 5751 5752 5753 5754 5755 5756 5757 5758 5759 5760 5761 5762 5763\n",
      " 5764 5765 5766 5767 5768 5769 5770 5771 5772 5773 5774 5775 5776 5777\n",
      " 5778 5779 5780 5781 5782 5783 5784 5785 5786 5787 5788 5789 5790 5791\n",
      " 5792 5793 5794 5795 5796 5797 5798 5799 5800 5801 5802 5803 5804 5805\n",
      " 5806 5807 5808 5809 5810 5811 5812 5813 5814 5815 5816 5817 5818 5819\n",
      " 5820 5821 5822 5823 5824 5825 5826 5827 5828 5829 5830 5831 5832 5833\n",
      " 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
     ]
    }
   ],
   "source": [
    "forcing_datasets = [\"nldas\", \"daymet\", \"maurer\"]\n",
    "\n",
    "for i in range(len(forcing_datasets)):\n",
    "    forcing_dataset = forcing_datasets[i]\n",
    "\n",
    "    embedding = torch.load(f\"data/671_camels_{forcing_dataset}_embedding.pt\").to(computing_device)\n",
    "    decoder = torch.load(f\"data/671_camels_{forcing_dataset}_decoder.pt\").to(computing_device)\n",
    "\n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # read data\n",
    "    dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "    x_batch_test, y_batch_test = dtest.get_val_batch()\n",
    "\n",
    "    # dimension of embedding\n",
    "    catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
    "    LATENT_dim = catchment_embeddings.shape[1]\n",
    "    \n",
    "    calibrated_KGES = np.ones(N_CATCHMENTS)\n",
    "    calibrated_NSES = np.ones(N_CATCHMENTS)\n",
    "\n",
    "    for i in range(N_CATCHMENTS):\n",
    "        #print(f'i={i} starts')\n",
    "        calibrated_KGES[i], calibrated_NSES[i]  = eval_catchment(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5267606937600486, -3.9365335540275512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(calibrated_KGES),calibrated_KGES.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "forcing_dataset = forcing_datasets[i]\n",
    "\n",
    "embedding = torch.load(f\"data/671_camels_{forcing_dataset}_embedding.pt\").to(computing_device)\n",
    "decoder = torch.load(f\"data/671_camels_{forcing_dataset}_decoder.pt\").to(computing_device)\n",
    "\n",
    "embedding.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# read data\n",
    "dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "x_batch_test, y_batch_test = dtest.get_val_batch()\n",
    "\n",
    "# dimension of embedding\n",
    "catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
    "LATENT_dim = catchment_embeddings.shape[1]\n",
    "\n",
    "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
    "calibrated_NSES = np.ones(N_CATCHMENTS)\n",
    "\n",
    "for i in range(N_CATCHMENTS):\n",
    "    #print(f'i={i} starts')\n",
    "    calibrated_KGES[i], calibrated_NSES[i]  = eval_catchment(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.08752167224884033, -11.256974341794738)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(calibrated_NSES),calibrated_NSES.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
