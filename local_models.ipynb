{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import HydroErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENTS = 559\n",
    "\n",
    "# training hyperparameters\n",
    "UPDATES = 1000\n",
    "TRAIN_YEAR = 8\n",
    "PATIENCE = 10\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.load(\"data/final_lstm_embedding.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
    "decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
    "\n",
    "embedding.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# dimension of embedding\n",
    "catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
    "LATENT_dim = catchment_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_val = dataloader.Forcing_Data(\n",
    "    \"data/camels_train_val.csv\",\n",
    "    record_length=3652,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtrain = dataloader.Forcing_Data(\n",
    "    \"data/camels_train.csv\",\n",
    "    record_length=2922,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dval = dataloader.Forcing_Data(\n",
    "    \"data/camels_val.csv\",\n",
    "    record_length=1095,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtest = dataloader.Forcing_Data(\n",
    "    \"data/camels_test.csv\",\n",
    "    record_length=4383,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_update(study):\n",
    "    \n",
    "    stats = study.best_trials[0].intermediate_values\n",
    "    steps = min(stats, key=lambda k: stats[k]) + 1\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FINE_TUNE:\n",
    "    def __init__(self, selected_catchment=0, eval_fun=HydroErr.kge_2009):\n",
    "        self.selected_catchment = selected_catchment\n",
    "        self.eval_fun=eval_fun\n",
    "        \n",
    "    def fine_tune(self,trial):\n",
    "       \n",
    "        # define batch size\n",
    "        batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "        batch_size = 2**batch_size_power\n",
    "        \n",
    "        # load model\n",
    "        decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
    "        \n",
    "        # define new embeding for the selected catchment\n",
    "        embedding = nn.Embedding(1, LATENT_dim).to(computing_device)\n",
    "        embedding_input = torch.zeros(size = (batch_size,), dtype=torch.long, device=computing_device)\n",
    "\n",
    "        # validation data\n",
    "        x_val, y_val = dval.get_catchment_val_batch(self.selected_catchment)\n",
    "        x_val, y_val = x_val.to(computing_device), y_val.to(computing_device)\n",
    "        \n",
    "        # define optimizers\n",
    "        lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "        lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "        \n",
    "        # model training\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "        \n",
    "        # define early stopper\n",
    "        early_stopper = training_fun.EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "        \n",
    "        for update in range(UPDATES):\n",
    "            \n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "            \n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "            \n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "            \n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, _) = dtrain.get_catchment_random_batch(\n",
    "                selected_catchment=self.selected_catchment, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            x_batch, y_batch = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "            )\n",
    "            \n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(embedding_input)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # validate model after each update\n",
    "            decoder.eval()\n",
    "            embedding.eval()\n",
    "            \n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                with torch.no_grad():                    \n",
    "                    code = embedding(torch.zeros(size = (x_val.shape[0],), dtype=torch.long, device=computing_device))\n",
    "                    out = decoder.decode(code, x_val)\n",
    "                    \n",
    "                    val_loss = training_fun.mse_loss_with_nans(out, y_val).detach().cpu().numpy()\n",
    "            \n",
    "            # Handle pruning based on the intermediate value\n",
    "            trial.report(val_loss, update)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                torch.cuda.empty_cache()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Early stop using early_stopper, break for loop\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "        return early_stopper.min_validation_loss\n",
    "\n",
    "    def test_final_model(self, n_trials=200, return_model = False):\n",
    "        \n",
    "        self.study = optuna.create_study(study_name=\"fine_tune\", direction=\"minimize\", pruner=optuna.pruners.NopPruner())\n",
    "        \n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        self.study.optimize(self.fine_tune, n_trials = n_trials)\n",
    "        \n",
    "        # optimal parameters\n",
    "        updates = get_optimal_update(self.study)\n",
    "        \n",
    "        lr_decoder = self.study.best_params[\"lr_decoder\"]\n",
    "        lr_embedding = self.study.best_params[\"lr_embedding\"]\n",
    "        batch_size_power = self.study.best_params[\"batch_size_power\"]\n",
    "        batch_size = 2 ** batch_size_power\n",
    "\n",
    "        # load model\n",
    "        decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
    "        \n",
    "        # define new embedding for the selected catchment\n",
    "        embedding = nn.Embedding(1, LATENT_dim).to(computing_device)\n",
    "        embedding_input = torch.zeros(size = (batch_size,), dtype=torch.long, device=computing_device)\n",
    "\n",
    "        # define model optimizer\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "        # validation data\n",
    "        x_test, y_test = dtest.get_catchment_val_batch(self.selected_catchment)\n",
    "        x_test, y_test = x_test.to(computing_device).contiguous(), y_test.to(computing_device).contiguous()\n",
    "        \n",
    "        # start training\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "        for update in range(updates):\n",
    "            \n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "            \n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "            \n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "            \n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, _) = dtrain_val.get_catchment_random_batch(\n",
    "                selected_catchment=self.selected_catchment, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            x_batch, y_batch = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "            )\n",
    "            \n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(embedding_input)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        decoder.eval()\n",
    "        embedding.eval()\n",
    "        \n",
    "        with torch.autocast(\n",
    "            device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "        ):\n",
    "            with torch.no_grad():                    \n",
    "                code = embedding(torch.zeros(size = (x_test.shape[0],), dtype=torch.long, device=computing_device))\n",
    "                pred = decoder.decode(code, x_test).view(-1).detach().cpu().numpy()\n",
    "                \n",
    "                ob = y_test.view(-1).detach().cpu().numpy()\n",
    "                \n",
    "                gof = self.eval_fun(simulated_array=pred, observed_array=ob)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if return_model:\n",
    "            return gof, embedding, decoder\n",
    "        else:\n",
    "            return gof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
    "\n",
    "for i in range(N_CATCHMENTS):\n",
    "\n",
    "    fine_tune = FINE_TUNE(i)\n",
    "    calibrated_KGES[i], embedding, decoder = fine_tune.test_final_model(n_trials=200, return_model=True)\n",
    "        \n",
    "    torch.save(embedding.cpu(), f\"data/fine_tune/embedding{i}.pt\")\n",
    "    torch.save(decoder.cpu(), f\"data/fine_tune/ecoder{i}.pt\")\n",
    "    \n",
    "    joblib.dump(fine_tune.study, f\"data/fine_tune/study{i}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
