{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import HydroErr\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import HydroErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 5\n",
    "\n",
    "N_CATCHMENTS = 531\n",
    "\n",
    "# training hyperparameters\n",
    "EPOCHS = 500\n",
    "TRAIN_YEAR = 14\n",
    "PATIENCE = 20\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load(\"data/531_nldas_original_lstm_study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_val = dataloader.Forcing_Data(\n",
    "    \"data/531_nldas_original_camels_train_val.csv\",\n",
    "    record_length=5478,\n",
    "    n_feature=FORCING_DIM,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtrain = dataloader.Forcing_Data(\n",
    "    \"data/531_nldas_original_camels_train.csv\",\n",
    "    record_length=4017,\n",
    "    n_feature=FORCING_DIM,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dval = dataloader.Forcing_Data(\n",
    "    \"data/531_nldas_original_camels_val.csv\",\n",
    "    record_length=1826,\n",
    "    n_feature=FORCING_DIM,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtest = dataloader.Forcing_Data(\n",
    "    \"data/531_nldas_original_camels_test.csv\",\n",
    "    record_length=5844,\n",
    "    n_feature=FORCING_DIM,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_epochs(study):\n",
    "    \n",
    "    stats = study.best_trials[0].intermediate_values\n",
    "    epochs = min(stats, key=lambda k: stats[k]) + 1\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model(study, dataset, n_catchments =N_CATCHMENTS, epoch_scale = 10/TRAIN_YEAR): #19/39\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    # define model\n",
    "    model_builder = training_fun.LSTM_model_builder(\n",
    "        n_catchments, base_length=365, forcing_dim=FORCING_DIM\n",
    "    )\n",
    "\n",
    "    embedding, decoder = model_builder.define_model(trial)\n",
    "\n",
    "    embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "        computing_device\n",
    "    )\n",
    "\n",
    "    if compile_model:\n",
    "        # pytorch2.0 new feature, complile model for fast training\n",
    "        embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "\n",
    "    # define model training hyperparameters\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # define optimal epochs\n",
    "    epochs = round(get_optimal_epochs(study)*epoch_scale)\n",
    "\n",
    "        # steps per epoch\n",
    "    steps = round(N_CATCHMENTS * TRAIN_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            embedding_optimizer.zero_grad()\n",
    "\n",
    "            # put the models into training mode\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            # get training batch and pass to device\n",
    "            (x_batch, y_batch, selected_catchments) = dataset.get_random_batch(\n",
    "                batch_size\n",
    "            )\n",
    "\n",
    "            x_batch, y_batch, selected_catchments = (\n",
    "                x_batch.to(computing_device),\n",
    "                y_batch.to(computing_device),\n",
    "                selected_catchments.to(computing_device),\n",
    "            )\n",
    "\n",
    "            # slice batch for training\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                code = embedding(selected_catchments)\n",
    "\n",
    "                # pass through decoder\n",
    "                out = decoder.decode(code, x_batch)\n",
    "\n",
    "                # compute loss\n",
    "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(embedding_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, decoder = get_final_model(study, dtrain_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding.cpu(), \"data/531_nldas_camles_final_lstm_embedding.pt\")\n",
    "torch.save(decoder.cpu(), \"data/531_nldas_camels_final_lstm_decoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.load(\"data/531_nldas_camles_final_lstm_embedding.pt\").to(computing_device)\n",
    "decoder = torch.load(\"data/531_nldas_camels_final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
    "\n",
    "embedding.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# dimension of embedding\n",
    "catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
    "LATENT_dim = catchment_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective_builder:\n",
    "    def __init__(self, x, y, eval_fun):\n",
    "        self.eval_fun = eval_fun\n",
    "        self.x = x.contiguous()\n",
    "        self.y = y.contiguous()\n",
    "    \n",
    "    def eval(self, code, return_summary = True):\n",
    "        \n",
    "        # numpy to torch tensor\n",
    "        code = torch.from_numpy(code).unsqueeze(0).to(dtype=torch.float32).to(computing_device)\n",
    "        code = code.expand(self.x.shape[0], -1)\n",
    "        \n",
    "        # BASE_LENGTH is from global\n",
    "        pred = decoder.decode(code, self.x).view(-1).detach().cpu().numpy()\n",
    "\n",
    "        ob = self.y.view(-1).detach().cpu().numpy()\n",
    "        \n",
    "        if return_summary:\n",
    "          gof = self.eval_fun(simulated_array=pred, observed_array=ob)\n",
    "          return gof\n",
    "        else:\n",
    "          return pred, ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 531, 730, 5]), torch.Size([16, 531, 365]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_test, y_batch_test = dtest.get_val_batch()\n",
    "x_batch_test.shape, y_batch_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_catchment(selected_catchment):\n",
    "    \n",
    "    x = x_batch_test[:,selected_catchment,:,:]\n",
    "    y = y_batch_test[:,selected_catchment,:]\n",
    "\n",
    "    x, y = x.to(computing_device), y.to(computing_device)\n",
    "\n",
    "    fn_kge = Objective_builder(x,y,HydroErr.kge_2009)\n",
    "    fn_nse = Objective_builder(x,y,HydroErr.nse)\n",
    "    \n",
    "    kge = fn_kge.eval(catchment_embeddings[selected_catchment,:].cpu().detach().numpy())\n",
    "    nse = fn_nse.eval(catchment_embeddings[selected_catchment,:].cpu().detach().numpy())\n",
    "    \n",
    "    return  kge, nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491 5492\n",
      " 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505 5506\n",
      " 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519 5520\n",
      " 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533 5534\n",
      " 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547 5548\n",
      " 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561 5562\n",
      " 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575 5576\n",
      " 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589 5590\n",
      " 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603 5604\n",
      " 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617 5618\n",
      " 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631 5632\n",
      " 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645 5646\n",
      " 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659 5660\n",
      " 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673 5674\n",
      " 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687 5688\n",
      " 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701 5702\n",
      " 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715 5716\n",
      " 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729 5730\n",
      " 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743 5744\n",
      " 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757 5758\n",
      " 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771 5772\n",
      " 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785 5786\n",
      " 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799 5800\n",
      " 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813 5814\n",
      " 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827 5828\n",
      " 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [5114 5115 5116 5117 5118 5119 5120 5121 5122 5123 5124 5125 5126 5127\n",
      " 5128 5129 5130 5131 5132 5133 5134 5135 5136 5137 5138 5139 5140 5141\n",
      " 5142 5143 5144 5145 5146 5147 5148 5149 5150 5151 5152 5153 5154 5155\n",
      " 5156 5157 5158 5159 5160 5161 5162 5163 5164 5165 5166 5167 5168 5169\n",
      " 5170 5171 5172 5173 5174 5175 5176 5177 5178 5179 5180 5181 5182 5183\n",
      " 5184 5185 5186 5187 5188 5189 5190 5191 5192 5193 5194 5195 5196 5197\n",
      " 5198 5199 5200 5201 5202 5203 5204 5205 5206 5207 5208 5209 5210 5211\n",
      " 5212 5213 5214 5215 5216 5217 5218 5219 5220 5221 5222 5223 5224 5225\n",
      " 5226 5227 5228 5229 5230 5231 5232 5233 5234 5235 5236 5237 5238 5239\n",
      " 5240 5241 5242 5243 5244 5245 5246 5247 5248 5249 5250 5251 5252 5253\n",
      " 5254 5255 5256 5257 5258 5259 5260 5261 5262 5263 5264 5265 5266 5267\n",
      " 5268 5269 5270 5271 5272 5273 5274 5275 5276 5277 5278 5279 5280 5281\n",
      " 5282 5283 5284 5285 5286 5287 5288 5289 5290 5291 5292 5293 5294 5295\n",
      " 5296 5297 5298 5299 5300 5301 5302 5303 5304 5305 5306 5307 5308 5309\n",
      " 5310 5311 5312 5313 5314 5315 5316 5317 5318 5319 5320 5321 5322 5323\n",
      " 5324 5325 5326 5327 5328 5329 5330 5331 5332 5333 5334 5335 5336 5337\n",
      " 5338 5339 5340 5341 5342 5343 5344 5345 5346 5347 5348 5349 5350 5351\n",
      " 5352 5353 5354 5355 5356 5357 5358 5359 5360 5361 5362 5363 5364 5365\n",
      " 5366 5367 5368 5369 5370 5371 5372 5373 5374 5375 5376 5377 5378 5379\n",
      " 5380 5381 5382 5383 5384 5385 5386 5387 5388 5389 5390 5391 5392 5393\n",
      " 5394 5395 5396 5397 5398 5399 5400 5401 5402 5403 5404 5405 5406 5407\n",
      " 5408 5409 5410 5411 5412 5413 5414 5415 5416 5417 5418 5419 5420 5421\n",
      " 5422 5423 5424 5425 5426 5427 5428 5429 5430 5431 5432 5433 5434 5435\n",
      " 5436 5437 5438 5439 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449\n",
      " 5450 5451 5452 5453 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463\n",
      " 5464 5465 5466 5467 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477\n",
      " 5478 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491\n",
      " 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505\n",
      " 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519\n",
      " 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533\n",
      " 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547\n",
      " 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561\n",
      " 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575\n",
      " 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589\n",
      " 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603\n",
      " 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617\n",
      " 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631\n",
      " 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645\n",
      " 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659\n",
      " 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673\n",
      " 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687\n",
      " 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701\n",
      " 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715\n",
      " 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729\n",
      " 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743\n",
      " 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757\n",
      " 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771\n",
      " 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785\n",
      " 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799\n",
      " 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813\n",
      " 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827\n",
      " 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
      "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
     ]
    }
   ],
   "source": [
    "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
    "calibrated_NSES = np.ones(N_CATCHMENTS)\n",
    "\n",
    "\n",
    "for i in range(N_CATCHMENTS):\n",
    "    #print(f'i={i} starts')\n",
    "    calibrated_KGES[i], calibrated_NSES[i]  = eval_catchment(i)\n",
    "    #print(f'fit={calibrated_KGES[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(calibrated_NSES[calibrated_NSES>0], bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6945471218645058, 0.5047193609812083)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(calibrated_KGES),calibrated_KGES.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6882016956806183, 0.4921213486240633)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(calibrated_NSES),calibrated_NSES.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
