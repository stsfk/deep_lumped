{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "95vm9FxGQ7KK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "import time\n",
        "\n",
        "import dataloader\n",
        "import models\n",
        "import training_fun\n",
        "\n",
        "import optuna\n",
        "\n",
        "import joblib\n",
        "\n",
        "import HydroErr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEQ_LENGTH = 365 * 2\n",
        "TARGET_SEQ_LENGTH = 365\n",
        "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
        "\n",
        "FORCING_DIM = 3\n",
        "\n",
        "N_CATCHMENTS = 559\n",
        "\n",
        "# training hyperparameters\n",
        "UPDATES = 1000\n",
        "TRAIN_YEAR = 8\n",
        "PATIENCE = 10\n",
        "\n",
        "use_amp = True\n",
        "compile_model = False\n",
        "\n",
        "if compile_model:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "memory_saving = False\n",
        "if memory_saving:\n",
        "    storge_device = \"cpu\"\n",
        "    computing_device = DEVICE\n",
        "    VAL_STEPS = 500\n",
        "else:\n",
        "    storge_device = DEVICE\n",
        "    computing_device = DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding = torch.load(\"data/final_lstm_embedding.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "\n",
        "embedding.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# dimension of embedding\n",
        "catchment_embeddings=[x.data for x in embedding.parameters()][0]\n",
        "LATENT_dim = catchment_embeddings.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtrain_val = dataloader.Forcing_Data(\n",
        "    \"data/camels_train_val.csv\",\n",
        "    record_length=3652,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dtrain = dataloader.Forcing_Data(\n",
        "    \"data/camels_train.csv\",\n",
        "    record_length=2922,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dval = dataloader.Forcing_Data(\n",
        "    \"data/camels_val.csv\",\n",
        "    record_length=1095,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")\n",
        "\n",
        "dtest = dataloader.Forcing_Data(\n",
        "    \"data/camels_test.csv\",\n",
        "    record_length=4383,\n",
        "    storge_device=storge_device,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    target_seq_length=TARGET_SEQ_LENGTH,\n",
        "    base_length=BASE_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimal_update(study):\n",
        "    \n",
        "    stats = study.best_trials[0].intermediate_values\n",
        "    steps = min(stats, key=lambda k: stats[k]) + 1\n",
        "    \n",
        "    return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FINE_TUNE:\n",
        "    def __init__(self, selected_catchment=0, eval_fun=HydroErr.kge_2009):\n",
        "        self.selected_catchment = selected_catchment\n",
        "        self.eval_fun=eval_fun\n",
        "        \n",
        "    def fine_tune(self,trial):\n",
        "       \n",
        "        # define batch size\n",
        "        batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
        "        batch_size = 2**batch_size_power\n",
        "        \n",
        "        # load model\n",
        "        decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "        \n",
        "        # define new embeding for the selected catchment\n",
        "        embedding = nn.Embedding(1, LATENT_dim).to(computing_device)\n",
        "        embedding_input = torch.zeros(size = (batch_size,), dtype=torch.long, device=computing_device)\n",
        "\n",
        "        # validation data\n",
        "        x_val, y_val = dval.get_catchment_val_batch(self.selected_catchment)\n",
        "        x_val, y_val = x_val.to(computing_device), y_val.to(computing_device)\n",
        "        \n",
        "        # define optimizers\n",
        "        lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
        "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
        "\n",
        "        lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
        "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
        "        \n",
        "        # model training\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        \n",
        "        # define early stopper\n",
        "        early_stopper = training_fun.EarlyStopper(patience=PATIENCE, min_delta=0)\n",
        "        \n",
        "        for update in range(UPDATES):\n",
        "            \n",
        "            decoder.train()\n",
        "            embedding.train()\n",
        "            \n",
        "            decoder_optimizer.zero_grad()\n",
        "            embedding_optimizer.zero_grad()\n",
        "            \n",
        "            # put the models into training mode\n",
        "            decoder.train()\n",
        "            embedding.train()\n",
        "            \n",
        "            # get training batch and pass to device\n",
        "            (x_batch, y_batch, _) = dtrain.get_catchment_random_batch(\n",
        "                selected_catchment=self.selected_catchment, batch_size=batch_size\n",
        "            )\n",
        "            \n",
        "            x_batch, y_batch = (\n",
        "                x_batch.to(computing_device),\n",
        "                y_batch.to(computing_device),\n",
        "            )\n",
        "            \n",
        "            # slice batch for training\n",
        "            with torch.autocast(\n",
        "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
        "            ):\n",
        "                code = embedding(embedding_input)\n",
        "\n",
        "                # pass through decoder\n",
        "                out = decoder.decode(code, x_batch)\n",
        "\n",
        "                # compute loss\n",
        "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
        "                \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(embedding_optimizer)\n",
        "            scaler.step(decoder_optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # validate model after each update\n",
        "            decoder.eval()\n",
        "            embedding.eval()\n",
        "            \n",
        "            with torch.autocast(\n",
        "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
        "            ):\n",
        "                with torch.no_grad():                    \n",
        "                    code = embedding(torch.zeros(size = (x_val.shape[0],), dtype=torch.long, device=computing_device))\n",
        "                    out = decoder.decode(code, x_val)\n",
        "                    \n",
        "                    val_loss = training_fun.mse_loss_with_nans(out, y_val).detach().cpu().numpy()\n",
        "            \n",
        "            # Handle pruning based on the intermediate value\n",
        "            trial.report(val_loss, update)\n",
        "\n",
        "            if trial.should_prune():\n",
        "                torch.cuda.empty_cache()\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "            # Early stop using early_stopper, break for loop\n",
        "            if early_stopper.early_stop(val_loss):\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            \n",
        "        return early_stopper.min_validation_loss\n",
        "\n",
        "    def test_final_model(self, n_trials=200, return_model = False):\n",
        "        \n",
        "        self.study = optuna.create_study(study_name=\"fine_tune\", direction=\"minimize\", pruner=optuna.pruners.NopPruner())\n",
        "        \n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "        self.study.optimize(self.fine_tune, n_trials = n_trials)\n",
        "        \n",
        "        # optimal parameters\n",
        "        updates = get_optimal_update(self.study)\n",
        "        \n",
        "        lr_decoder = self.study.best_params[\"lr_decoder\"]\n",
        "        lr_embedding = self.study.best_params[\"lr_embedding\"]\n",
        "        batch_size_power = self.study.best_params[\"batch_size_power\"]\n",
        "        batch_size = 2 ** batch_size_power\n",
        "\n",
        "        # load model\n",
        "        decoder = torch.load(\"data/final_lstm_decoder.pt\", map_location=torch.device('cpu')).to(computing_device)\n",
        "        \n",
        "        # define new embedding for the selected catchment\n",
        "        embedding = nn.Embedding(1, LATENT_dim).to(computing_device)\n",
        "        embedding_input = torch.zeros(size = (batch_size,), dtype=torch.long, device=computing_device)\n",
        "\n",
        "        # define model optimizer\n",
        "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
        "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
        "\n",
        "        # validation data\n",
        "        x_test, y_test = dtest.get_catchment_val_batch(self.selected_catchment)\n",
        "        x_test, y_test = x_test.to(computing_device).contiguous(), y_test.to(computing_device).contiguous()\n",
        "        \n",
        "        # start training\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        for update in range(updates):\n",
        "            \n",
        "            decoder.train()\n",
        "            embedding.train()\n",
        "            \n",
        "            decoder_optimizer.zero_grad()\n",
        "            embedding_optimizer.zero_grad()\n",
        "            \n",
        "            # put the models into training mode\n",
        "            decoder.train()\n",
        "            embedding.train()\n",
        "            \n",
        "            # get training batch and pass to device\n",
        "            (x_batch, y_batch, _) = dtrain_val.get_catchment_random_batch(\n",
        "                selected_catchment=self.selected_catchment, batch_size=batch_size\n",
        "            )\n",
        "            \n",
        "            x_batch, y_batch = (\n",
        "                x_batch.to(computing_device),\n",
        "                y_batch.to(computing_device),\n",
        "            )\n",
        "            \n",
        "            # slice batch for training\n",
        "            with torch.autocast(\n",
        "                device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
        "            ):\n",
        "                code = embedding(embedding_input)\n",
        "\n",
        "                # pass through decoder\n",
        "                out = decoder.decode(code, x_batch)\n",
        "\n",
        "                # compute loss\n",
        "                loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
        "                \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(embedding_optimizer)\n",
        "            scaler.step(decoder_optimizer)\n",
        "            scaler.update()\n",
        "        \n",
        "        decoder.eval()\n",
        "        embedding.eval()\n",
        "        \n",
        "        with torch.autocast(\n",
        "            device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
        "        ):\n",
        "            with torch.no_grad():                    \n",
        "                code = embedding(torch.zeros(size = (x_test.shape[0],), dtype=torch.long, device=computing_device))\n",
        "                pred = decoder.decode(code, x_test).view(-1).detach().cpu().numpy()\n",
        "                \n",
        "                ob = y_test.view(-1).detach().cpu().numpy()\n",
        "                \n",
        "                gof = self.eval_fun(simulated_array=pred, observed_array=ob)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        if return_model:\n",
        "            return gof, embedding, decoder\n",
        "        else:\n",
        "            return gof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-01-11 21:23:24,129]\u001b[0m A new study created in memory with name: fine_tune\u001b[0m\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:3118: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  pr = top_pr / (bot1_pr * bot2_pr)\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [1270 1271 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029\n",
            " 4030 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043\n",
            " 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057\n",
            " 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071\n",
            " 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085\n",
            " 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099\n",
            " 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113\n",
            " 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127\n",
            " 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141\n",
            " 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155\n",
            " 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169\n",
            " 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183\n",
            " 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197\n",
            " 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211\n",
            " 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225\n",
            " 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239\n",
            " 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253\n",
            " 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267\n",
            " 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281\n",
            " 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295\n",
            " 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309\n",
            " 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323\n",
            " 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337\n",
            " 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351\n",
            " 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365\n",
            " 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939\n",
            " 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953\n",
            " 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967\n",
            " 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981\n",
            " 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995\n",
            " 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009\n",
            " 4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023\n",
            " 4024 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037\n",
            " 4038 4039 4040 4041 4042 4043 4044 4045 4046 4047 4048 4049 4050 4051\n",
            " 4052 4053 4054 4055 4056 4057 4058 4059 4060 4061 4062 4063 4064 4065\n",
            " 4066 4067 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079\n",
            " 4080 4081 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093\n",
            " 4094 4095 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107\n",
            " 4108 4109 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121\n",
            " 4122 4123 4124 4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135\n",
            " 4136 4137 4138 4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149\n",
            " 4150 4151 4152 4153 4154 4155 4156 4157 4158 4159 4160 4161 4162 4163\n",
            " 4164 4165 4166 4167 4168 4169 4170 4171 4172 4173 4174 4175 4176 4177\n",
            " 4178 4179 4180 4181 4182 4183 4184 4185 4186 4187 4188 4189 4190 4191\n",
            " 4192 4193 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205\n",
            " 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4218 4219\n",
            " 4220 4221 4222 4223 4224 4225 4226 4227 4228 4229 4230 4231 4232 4233\n",
            " 4234 4235 4236 4237 4238 4239 4240 4241 4242 4243 4244 4245 4246 4247\n",
            " 4248 4249 4250 4251 4252 4253 4254 4255 4256 4257 4258 4259 4260 4261\n",
            " 4262 4263 4264 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275\n",
            " 4276 4277 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289\n",
            " 4290 4291 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303\n",
            " 4304 4305 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317\n",
            " 4318 4319 4320 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331\n",
            " 4332 4333 4334 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345\n",
            " 4346 4347 4348 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359\n",
            " 4360 4361 4362 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373\n",
            " 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3941\n",
            " 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955\n",
            " 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971\n",
            " 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985\n",
            " 3986 3987 3988 3989 3990 3991 3992 3993 3994 3996 3997 3998 3999 4000\n",
            " 4003 4004 4006 4007 4008 4009 4010 4011 4012 4013 4014 4015 4016 4017\n",
            " 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031\n",
            " 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
            " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
            " 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073\n",
            " 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087\n",
            " 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101\n",
            " 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115\n",
            " 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129\n",
            " 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143\n",
            " 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157\n",
            " 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171\n",
            " 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185\n",
            " 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
            " 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213\n",
            " 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227\n",
            " 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241\n",
            " 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255\n",
            " 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269\n",
            " 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283\n",
            " 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297\n",
            " 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311\n",
            " 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325\n",
            " 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339\n",
            " 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353\n",
            " 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367\n",
            " 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939\n",
            " 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953\n",
            " 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967\n",
            " 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981\n",
            " 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995\n",
            " 3996 3997 3998 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 4010\n",
            " 4011 4012 4013 4014 4015 4016 4018 4019 4020 4021 4022 4023 4024 4025\n",
            " 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 4039\n",
            " 4040 4041 4042 4043 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053\n",
            " 4054 4055 4056 4057 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067\n",
            " 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081\n",
            " 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095\n",
            " 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109\n",
            " 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123\n",
            " 4124 4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137\n",
            " 4138 4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151\n",
            " 4152 4153 4154 4155 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165\n",
            " 4166 4167 4168 4169 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179\n",
            " 4180 4181 4182 4183 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193\n",
            " 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207\n",
            " 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221\n",
            " 4222 4223 4224 4225 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235\n",
            " 4236 4237 4238 4239 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249\n",
            " 4250 4251 4252 4253 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263\n",
            " 4264 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277\n",
            " 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291\n",
            " 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305\n",
            " 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319\n",
            " 4320 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333\n",
            " 4334 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347\n",
            " 4348 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361\n",
            " 4362 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375\n",
            " 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n",
            "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\HydroErr\\HydroErr.py:6248: UserWarning: Row(s) [3199 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030\n",
            " 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044\n",
            " 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058\n",
            " 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072\n",
            " 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086\n",
            " 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100\n",
            " 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114\n",
            " 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128\n",
            " 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142\n",
            " 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156\n",
            " 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170\n",
            " 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184\n",
            " 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198\n",
            " 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212\n",
            " 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226\n",
            " 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240\n",
            " 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254\n",
            " 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268\n",
            " 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282\n",
            " 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296\n",
            " 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310\n",
            " 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324\n",
            " 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338\n",
            " 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352\n",
            " 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366\n",
            " 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379] contained NaN values and the row(s) have been removed (Rows are zero indexed).\n",
            "  warnings.warn(\"Row(s) {} contained NaN values and the row(s) have been \"\n"
          ]
        }
      ],
      "source": [
        "calibrated_KGES = np.ones(N_CATCHMENTS)\n",
        "\n",
        "for i in range(N_CATCHMENTS):\n",
        "\n",
        "    fine_tune = FINE_TUNE(i)\n",
        "    calibrated_KGES[i], embedding, decoder = fine_tune.test_final_model(n_trials=200, return_model=True)\n",
        "        \n",
        "    torch.save(embedding.cpu(), f\"data/fine_tune/embedding{i}.pt\")\n",
        "    torch.save(decoder.cpu(), f\"data/fine_tune/ecoder{i}.pt\")\n",
        "    \n",
        "    joblib.dump(fine_tune.study, f\"data/fine_tune/study{i}.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 9.54218269e-01,  8.11622569e-01,  8.38468636e-01,  8.98815595e-01,\n",
              "        8.74192556e-01,  7.88224383e-01,            -inf,  8.42629996e-01,\n",
              "        9.31827136e-01,  9.26296478e-01,  7.64779545e-01,  8.12631643e-01,\n",
              "        8.72957926e-01,  7.11989348e-01,  8.67060093e-01,  7.99651637e-01,\n",
              "        8.60920194e-01,  8.96567433e-01,  7.07621627e-01,  7.67831474e-01,\n",
              "        7.42835202e-01,  8.57891300e-01,  5.17352432e-01,  6.73915300e-01,\n",
              "        8.49930805e-01,  6.86605690e-01,  8.01798471e-01,  5.82533001e-01,\n",
              "        8.50537332e-01,  8.14062762e-01,  8.27264444e-01,  8.38849573e-01,\n",
              "        7.02512023e-01,            -inf,  8.53369415e-01,  8.91727744e-01,\n",
              "        8.24684196e-01,  7.59362095e-01,  7.93227559e-01,  8.16988366e-01,\n",
              "        8.04394901e-01,  5.12223471e-01,  8.21856233e-01,  7.49875860e-01,\n",
              "        7.55278985e-01,  8.30614259e-01,  8.82011354e-01,  8.93942826e-01,\n",
              "        6.28285376e-01,  3.44952268e-01,  6.97872846e-01,  7.43325343e-01,\n",
              "        6.92894095e-01,  7.96762870e-01,  8.55007272e-01,  8.61577726e-01,\n",
              "        7.12904393e-01,  7.69010907e-01,  8.63125740e-01,  7.58346999e-01,\n",
              "        7.37603191e-01,  6.24765088e-01,  7.12928094e-01,  7.37276942e-01,\n",
              "        8.57310477e-01,  2.61231077e-01,  6.98157452e-01,  6.30755172e-01,\n",
              "        6.29560361e-01,  2.47099378e-01,  6.71920690e-01,  5.76531929e-01,\n",
              "        6.87844612e-01,  7.36744722e-01,  7.54069331e-01,  6.79034763e-01,\n",
              "        6.33447816e-01,  4.05339690e-01,  5.30050586e-01,  8.10030684e-01,\n",
              "        7.44344141e-01,  7.30944022e-01,  7.17179171e-01,  5.33165109e-01,\n",
              "        8.01492865e-01,  7.41963761e-01,  6.18418258e-01,  4.67999360e-01,\n",
              "        5.80379252e-01,  3.35220582e-01,  6.50312517e-01, -1.18898133e-01,\n",
              "        7.18796828e-01,  7.78635208e-01,  4.40037410e-01,  5.38281563e-01,\n",
              "        6.95332382e-01,  6.16186193e-01,  6.24038266e-02,  5.88213181e-01,\n",
              "        6.25857583e-01,  7.41398924e-01,  6.10157527e-01,  6.92508289e-01,\n",
              "        8.25345202e-01,  6.53171283e-01,  5.67327787e-01,  6.07651500e-01,\n",
              "        8.53036124e-01,  7.85863564e-01,  5.16096790e-01,  7.52577877e-01,\n",
              "        4.92419626e-01,  6.63531985e-01,  6.78065660e-01,  6.99121985e-01,\n",
              "        7.40734645e-01,  5.07057132e-01,  7.39240878e-01,  7.00368913e-01,\n",
              "        7.51280563e-01,  5.47586557e-01,  7.17956084e-01,  8.26101380e-01,\n",
              "        3.17945952e-01,  4.11683635e-01, -1.52971688e-01,  3.83380707e-01,\n",
              "        5.42467738e-01,  3.79282329e-01,  7.99597453e-01,  6.76503671e-01,\n",
              "        6.10730297e-01,  3.45323808e-01,  3.62536047e-01,  6.90674566e-01,\n",
              "        6.55520412e-01,  5.92021747e-01,  6.41998878e-01,  3.98962830e-01,\n",
              "        6.90136117e-01,  2.21432163e-01,  7.18799149e-01,  4.94910810e-01,\n",
              "        7.97485199e-01,  8.49520360e-01,  4.60766344e-01,  7.40626010e-01,\n",
              "        5.62217873e-01,  8.37830965e-01,  7.97609820e-01,  6.60090608e-01,\n",
              "        6.94823646e-01,  8.21751213e-01,  8.57437570e-01,  5.09825032e-01,\n",
              "        7.39601781e-01,  5.04366436e-01,  8.41464368e-01,  7.47369788e-01,\n",
              "        7.46021898e-01,  8.83260771e-01,  8.99868247e-01,  8.71293644e-01,\n",
              "        8.23978844e-01,  8.43285052e-01,  8.86624387e-01,  3.42355520e-01,\n",
              "        8.84411680e-01,  7.03203286e-01,  6.36072157e-01,  7.40462501e-01,\n",
              "        7.87641083e-01,  2.37974365e-01,  9.11478460e-01,  9.35960067e-01,\n",
              "        8.38503723e-01,  5.85820036e-01,  8.24831554e-01,  4.56018049e-01,\n",
              "        9.07839735e-01,  6.78491067e-01,  8.19971025e-01,  8.30830248e-01,\n",
              "        6.95026997e-01,  8.78386327e-01,  7.37340081e-01,  8.44086404e-01,\n",
              "        8.27949923e-01,  7.27988306e-01,  8.66798713e-01,  4.38603123e-01,\n",
              "        8.25068822e-01,  7.74721982e-01,  7.48542786e-01,  7.85629613e-01,\n",
              "        8.29502495e-01,  7.39042114e-01,  8.49775988e-01,  8.66092633e-01,\n",
              "        8.43368155e-01,  8.56030247e-01,  7.14941987e-01,  8.48439599e-01,\n",
              "        6.51955522e-01,  6.24812535e-01,  8.55994224e-01,  8.90516114e-01,\n",
              "        7.08825986e-01,  7.61706916e-01,  8.74388158e-01,            -inf,\n",
              "        7.40202740e-01,  8.00794712e-01,  6.87510126e-01,  8.40279713e-01,\n",
              "                   nan,  6.13984780e-01,  8.77970715e-01,  5.75657324e-01,\n",
              "        8.55475961e-01,  4.41213126e-01,  7.12539935e-01,  8.92313723e-01,\n",
              "        6.73266840e-01,  8.07060506e-01,  8.25207259e-01,  8.07079914e-01,\n",
              "        7.82496231e-01,  6.44086646e-01,  6.45461732e-01,  6.79034688e-01,\n",
              "        7.65054161e-01,  7.32205690e-01,  6.72505864e-01,  8.70385585e-01,\n",
              "        7.62409804e-01,  8.97250715e-01,  7.18537579e-01,  5.12594670e-01,\n",
              "        7.92444911e-01,  8.62226923e-01,  5.76995765e-01,  7.97739363e-01,\n",
              "        8.17651880e-01,  5.06776747e-01,  7.65201497e-01,  8.33187870e-01,\n",
              "        5.99181511e-01,  7.97208087e-01,  8.86656190e-01,  7.29592794e-01,\n",
              "        4.62381611e-01,  2.64047782e-01,  1.36534055e-01,  7.39796435e-01,\n",
              "        8.46197215e-01,  4.18640079e-01,  7.48058681e-01,  7.76903395e-01,\n",
              "        5.79500160e-01,  7.09241407e-01,  4.38861966e-01,  6.62776065e-01,\n",
              "        6.02607274e-01,  6.77610699e-01,  7.41285976e-01,  8.54731493e-01,\n",
              "        7.49732490e-01,  5.16063336e-01,  7.46482035e-01,  7.92744682e-01,\n",
              "        7.46526011e-01,  7.05407897e-01,  7.28520020e-01,            -inf,\n",
              "        7.69755955e-01,  5.43012562e-01,  7.24089490e-01,  5.35147477e-01,\n",
              "        7.83350849e-01,  8.37534275e-01,  8.34095732e-01,  7.19591071e-01,\n",
              "        7.32279890e-01,  7.25060312e-01,  8.28001959e-01,  7.93996467e-01,\n",
              "        8.87716648e-01,  4.64960142e-01,  9.10212854e-01,  9.32825821e-01,\n",
              "        8.71929144e-01,  8.72355673e-01,  7.92329936e-01,  6.46167832e-01,\n",
              "        8.53890150e-01,  7.71376449e-01,  5.48771996e-01,  7.98387651e-01,\n",
              "        4.37338720e-01,  3.32071926e-01,  5.69526477e-01,  3.87524225e-01,\n",
              "       -4.33194723e+00,  3.54561430e-01,  2.72880219e-01, -4.65401228e+00,\n",
              "       -3.95034158e+00, -2.47815031e-01,  4.50910046e-01, -1.41686900e+00,\n",
              "        3.57270003e-01,  5.78102686e-01, -1.03585774e+01,  6.67263221e-01,\n",
              "       -9.38402354e+00, -9.32661958e-02,  6.13867883e-01,  3.90993695e-01,\n",
              "       -5.09803411e-02,  6.33033287e-01,  5.23824954e-01,  7.26643626e-01,\n",
              "        1.25531400e-01,  1.74601311e-01,  3.39103806e-01,  7.51190878e-01,\n",
              "        8.75437693e-01,  8.53413763e-01,  8.85885595e-01,  8.54347305e-02,\n",
              "        5.95469021e-01,  7.06693855e-01,  7.20334028e-01, -7.79132625e-01,\n",
              "       -9.77640675e-02,  9.21016416e-02,  4.91404981e-01,  4.11236254e-01,\n",
              "        6.49339845e-01,  5.79358008e-01,  3.82118096e-01,  5.67129590e-01,\n",
              "        6.91371787e-01,  4.64927488e-01,  3.35011429e-01,  8.70646408e-01,\n",
              "        8.05346331e-01,  4.99682664e-01,  7.16216845e-01,  8.56097847e-01,\n",
              "        6.82573573e-01,  8.46116926e-01,  7.45553858e-01,  9.07206444e-01,\n",
              "        8.31226857e-01,  6.82741273e-01,  7.85941672e-01,  6.07959057e-01,\n",
              "        5.37544198e-01,  9.46182316e-01, -6.57383131e-01,  5.82224036e-01,\n",
              "        7.36797962e-01,  5.87275987e-01,  7.86217886e-01,  5.30598367e-01,\n",
              "       -8.43209998e-02,  4.49650741e-01,  6.45027675e-01,  5.81777419e-01,\n",
              "        6.74470775e-01, -3.82898606e-02, -1.68707177e-02,  6.40095100e-01,\n",
              "        6.16529909e-01,  5.51505131e-01,  6.21912902e-01,  8.08813888e-01,\n",
              "        6.59696167e-01,  5.07717592e-01, -4.91638172e-02, -1.34166809e+00,\n",
              "        2.30589534e-01,  3.37403306e-01,            -inf,            -inf,\n",
              "        6.58658923e-01,            -inf,  7.69248293e-01,            -inf,\n",
              "        6.93917542e-01,  5.00720057e-01,  5.74295617e-01,  8.07206374e-01,\n",
              "        7.78067685e-01, -1.00864168e+00,  6.71992273e-01,  7.23251211e-01,\n",
              "        5.42206972e-01,  6.53429971e-01,  4.66368078e-01, -3.65472931e-01,\n",
              "        6.00843369e-01,  6.11539198e-01,  5.05630525e-01, -2.88432035e-01,\n",
              "        1.94391747e-01,  4.40713384e-01,  3.23317594e-01,  5.73656649e-01,\n",
              "                   nan, -8.15011867e-01, -3.98202997e-01,  5.66750234e-01,\n",
              "        2.50742664e-01,  7.24363452e-01, -3.00988323e-01,  3.43355106e-01,\n",
              "        2.66306120e-01,  8.20517074e-01,  7.61314602e-01, -1.49029932e+00,\n",
              "       -4.78324270e-01,  5.53706138e-01,  5.46682860e-01,  4.47667567e-01,\n",
              "        3.36270231e-01,  2.96905621e-01,  7.36117065e-01,  3.05649239e-01,\n",
              "        7.84847628e-01,  4.85840662e-01,  8.31654842e-01, -8.35878773e-01,\n",
              "        8.40229693e-01,  8.83527092e-01,  4.73455744e-01,  8.49410079e-01,\n",
              "        8.72361959e-01,  7.53105677e-01,  7.52656450e-01,  9.04059415e-01,\n",
              "        8.90441149e-01,  7.46279502e-01,  8.29366973e-01, -1.22423472e+01,\n",
              "        8.14002841e-01,  8.40261166e-01,  5.13378484e-01,  3.86320112e-01,\n",
              "        7.36334611e-01, -1.33837932e+00,  1.98089163e-01, -2.10485790e-01,\n",
              "       -9.36300909e-03, -1.96566832e-02,  7.53918026e-01,  5.07734879e-01,\n",
              "       -1.15792264e+00,  6.61441237e-01,  6.79163296e-01,  5.32443182e-01,\n",
              "        8.09651074e-01,  7.31782146e-01, -1.57482454e-01,  5.31094477e-01,\n",
              "        6.41718503e-01, -1.86468548e-02,  7.83823454e-01,  8.17449352e-01,\n",
              "        8.04218803e-01,  4.09019971e-03, -6.42669156e-01,  5.30376135e-01,\n",
              "        6.17777074e-01, -1.74644522e+01,  8.21004455e-01,  5.65386440e-01,\n",
              "        5.23150669e-01,  8.27424685e-01,  8.36926592e-01,  6.74201510e-01,\n",
              "        7.91828803e-01,  5.67636364e-01,  8.78249209e-01,            -inf,\n",
              "                  -inf,  7.66582892e-01,            -inf,            -inf,\n",
              "                  -inf,            -inf,  5.75189023e-01,  5.24158535e-01,\n",
              "        7.84958501e-01,  7.73834411e-01,  9.14323662e-01,            -inf,\n",
              "                  -inf,            -inf,            -inf,            -inf,\n",
              "                  -inf,  7.37499822e-01,  5.91653877e-01,            -inf,\n",
              "                  -inf,            -inf,            -inf,            -inf,\n",
              "                  -inf,            -inf,            -inf,  9.42286361e-01,\n",
              "        4.24853518e-01,  8.42066403e-01,  7.88383763e-01,  5.16367502e-01,\n",
              "        4.57159618e-01,  7.70054229e-01,  7.34120300e-01,  8.80039267e-01,\n",
              "        7.72723538e-01,            -inf,  8.68549629e-01,  7.36664298e-01,\n",
              "        8.19475047e-01,  9.13818324e-01,  9.55064359e-01,  6.36672323e-01,\n",
              "        8.58070705e-01,  8.02503560e-01,  8.47835358e-01,  8.17647942e-01,\n",
              "        8.82792843e-01,  8.37157134e-01,  8.43587777e-01,  9.02704479e-01,\n",
              "        8.61718701e-01,  8.60854370e-01,  7.78338824e-01,            -inf,\n",
              "                  -inf,            -inf,            -inf,            -inf,\n",
              "        9.51360255e-01,            -inf,            -inf,            -inf,\n",
              "                  -inf,            -inf,            -inf,            -inf,\n",
              "                  -inf,            -inf,            -inf])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calibrated_KGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savetxt(\"data/ft_KGEs.csv\", calibrated_KGES, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calibrated_KGES.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(calibrated_KGES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.hist(calibrated_KGES[calibrated_KGES>0], bins = 30)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch1.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a6512bb800c64caa3cb3d76ef3bcdbc9727db239056cfe174e1a886fceb8ed0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
