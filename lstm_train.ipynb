{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7c2bd-33c6-4a80-81ed-e08829fbfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: half percision training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a11959c-c2bb-4731-8d89-0cc6123d28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENT = 1758\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "TRAIN_YEAR = 19\n",
    "\n",
    "PATIENCE = 10\n",
    "\n",
    "dtypes = defaultdict(lambda: float)\n",
    "dtypes[\"catchment_id\"] = str\n",
    "\n",
    "use_amp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a865b605-c842-48d1-bce6-1d7c8c39884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/Caravan/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "    ):\n",
    "        data_raw = pd.read_csv(fpath, dtype=dtypes)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = data_raw.loc[:, \"P\":\"PET\"]\n",
    "\n",
    "        x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        x = x.view(-1, record_length, n_feature)\n",
    "        self.x = x.to(DEVICE)\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = data_raw[\"Q\"]\n",
    "\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        y = y.view(-1, record_length)\n",
    "        self.y = y.to(DEVICE)\n",
    "        \n",
    "        self.record_length = self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # SEQ_LENGTH, BASE_LENGTH, and DEVICE is from global\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - SEQ_LENGTH + 1,\n",
    "            size=(N_CATCHMENT,),\n",
    "            device = DEVICE\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of SEQ_LENGTH, adding 0 to SEQ_LENGTH to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, SEQ_LENGTH) + torch.arange(SEQ_LENGTH, device = DEVICE)\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, FORCING_DIM)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = self.x.gather(dim=1, index=index_x), self.y.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return x_batch, y_batch[:, BASE_LENGTH:]\n",
    "    \n",
    "    def get_val_batch(self):\n",
    "        n_years = math.ceil((self.record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "        \n",
    "        out_x = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH, FORCING_DIM], device = DEVICE)*torch.nan\n",
    "        out_y = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH], device = DEVICE)*torch.nan\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH*i\n",
    "                \n",
    "            if i==n_years-1:\n",
    "                end_record_ind = self.record_length\n",
    "                \n",
    "                out_x[i,:,0:(end_record_ind-start_record_ind),:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,0:(end_record_ind-start_record_ind)] = self.y[:,start_record_ind:end_record_ind]\n",
    "                \n",
    "            else:\n",
    "                end_record_ind = start_record_ind + SEQ_LENGTH\n",
    "\n",
    "                out_x[i,:,:,:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,:] = self.y[:,start_record_ind:end_record_ind]\n",
    "        \n",
    "        return out_x, out_y[:,:,BASE_LENGTH:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7666e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = Forcing_Data(\"data_train_w_missing.csv\", record_length=7304)\n",
    "dval = Forcing_Data(\"data_val_w_missing.csv\",  record_length=4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfe6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Reference: https://stackoverflow.com/a/73704579/3361298\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ffaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    # Adatpted from https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "\n",
    "    def __init__(self, module, batch_first=False, base_length=BASE_LENGTH):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "        self.base_length = base_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # subsetting x that the first base_length elements are not interested\n",
    "        x = x[:,self.base_length:,:]\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(\n",
    "            -1, x.size(-1)\n",
    "        )  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(\n",
    "                x.size(0), -1, y.size(-1)\n",
    "            )  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d55f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        feature_dim,\n",
    "        lstm_hidden_dim,\n",
    "        fc_hidden_dims,\n",
    "        num_lstm_layers=1,\n",
    "        output_dim=1,\n",
    "        p=0.2,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.feature_dim + self.latent_dim,\n",
    "            self.lstm_hidden_dim,\n",
    "            num_layers=self.num_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # LSTM to latent code\n",
    "        self.fc_hidden_dims = fc_hidden_dims\n",
    "        self.fc_layers = []\n",
    "        self.p = p\n",
    "        for i in range((len(self.fc_hidden_dims))):\n",
    "            in_dim = self.lstm_hidden_dim if i == 0 else self.fc_hidden_dims[i - 1]\n",
    "            out_dim = self.fc_hidden_dims[i]\n",
    "            \n",
    "            self.fc_layers += [nn.Linear(in_dim, out_dim)]\n",
    "            self.fc_layers += [nn.ReLU()]\n",
    "            self.fc_layers += [nn.Dropout(p=self.p)]\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.fc_layers += [nn.Linear(self.fc_hidden_dims[-1], self.output_dim)]\n",
    "\n",
    "        self.fc_layers = TimeDistributed(nn.Sequential(*self.fc_layers), batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.out, (_, _) = self.lstm(inputs)\n",
    "        self.out = self.fc_layers(self.out)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def decode(self, code, x):\n",
    "        code = code.expand(x.shape[1], -1, -1).transpose(0, 1)\n",
    "\n",
    "        x = torch.cat((code, x), 2)\n",
    "        out = self.forward(x).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1415dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_with_nans(input, target):\n",
    "    # Adapted from https://stackoverflow.com/a/59851632/3361298\n",
    "\n",
    "    # Missing data are nans\n",
    "    mask = torch.isnan(target)\n",
    "\n",
    "    out = (input[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa30061-5561-4b87-b361-ab56c2bd1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(embedding, decoder, data, val_metric=mse_loss_with_nans, return_summary=True):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = data.get_val_batch()\n",
    "    \n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size = y.shape).to(DEVICE)\n",
    "\n",
    "    selected_catchments = torch.arange(N_CATCHMENT).to(DEVICE)\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "        with torch.no_grad():\n",
    "            code = embedding(selected_catchments)\n",
    "            for i in range(x.shape[0]):\n",
    "                x_sub = x[i,:,:,:]\n",
    "                preds[i,:,:] = decoder.decode(code, x_sub)\n",
    "            \n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9755c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    lstm_hidden_dim = trial.suggest_int(\"lstm_hidden_dim\", 4, 256)\n",
    "    n_lstm_layers = trial.suggest_int(\"n_lstm_layers\", 1, 2)\n",
    "    n_fc_layers = trial.suggest_int(\"n_fc_layers\", 1, 3)\n",
    "    LATENT_DIM_power = trial.suggest_int(\"LATENT_DIM_power\", 1, 2)\n",
    "    LATENT_DIM = 2**LATENT_DIM_power\n",
    "\n",
    "    drop_out_flag = trial.suggest_categorical(\"drop_out_flag\", [True, False])\n",
    "\n",
    "    if drop_out_flag:\n",
    "        p = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    else:\n",
    "        p = 0\n",
    "\n",
    "    fc_hidden_dims = []\n",
    "    for i in range(n_fc_layers):\n",
    "        fc_dim = trial.suggest_int(f\"fc_dim{i}\", 4, 16)\n",
    "        fc_hidden_dims.append(fc_dim)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        latent_dim=LATENT_DIM,\n",
    "        feature_dim=FORCING_DIM,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        fc_hidden_dims=fc_hidden_dims,\n",
    "        num_lstm_layers=n_lstm_layers,\n",
    "        output_dim=1,\n",
    "        p=p,\n",
    "    )\n",
    "\n",
    "    embedding = nn.Embedding(N_CATCHMENT, LATENT_DIM)\n",
    "\n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a68aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    # prepare early stopper\n",
    "    early_stopper = EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "    \n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "        \n",
    "        for year in range(TRAIN_YEAR):\n",
    "\n",
    "            x_batch, y_batch = dtrain.get_random_batch()\n",
    "            catchment_index = torch.randperm(N_CATCHMENT).to(DEVICE)  # add randomness\n",
    "\n",
    "            # interate over catchments\n",
    "            for i in range(int(N_CATCHMENT / batch_size)):\n",
    "\n",
    "                # prepare data\n",
    "                ind_s = i * batch_size\n",
    "                ind_e = (i + 1) * batch_size\n",
    "\n",
    "                selected_catchments = catchment_index[ind_s:ind_e]\n",
    "\n",
    "                x_sub, y_sub = x_batch[ind_s:ind_e, :, :], y_batch[ind_s:ind_e, :]\n",
    "\n",
    "                # prepare training, put the models into training mode\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                    code = embedding(selected_catchments)\n",
    "                    out = decoder.decode(code, x_sub)\n",
    "\n",
    "                    # backprop\n",
    "                    loss = mse_loss_with_nans(out, y_sub)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "        # validate model after each epochs\n",
    "        decoder.eval()\n",
    "        embedding.eval()\n",
    "        \n",
    "        val_loss = val_model(embedding, decoder, dval).detach().cpu().numpy()\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Early stop using early_stopper, break for loop\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return early_stopper.min_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444ea97a-c3a7-4731-94ea-3e23ea9cc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-02 13:59:11,967]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "\u001b[32m[I 2022-12-02 14:06:52,437]\u001b[0m Trial 0 finished with value: 4.8680806159973145 and parameters: {'lstm_hidden_dim': 85, 'n_lstm_layers': 1, 'n_fc_layers': 2, 'LATENT_DIM_power': 1, 'drop_out_flag': True, 'dropout_rate': 0.2247425454744759, 'fc_dim0': 5, 'fc_dim1': 15, 'lr_embedding': 0.006484614769524122, 'lr_decoder': 0.009356491716128904, 'batch_size_power': 5}. Best is trial 0 with value: 4.8680806159973145.\u001b[0m\n",
      "\u001b[33m[W 2022-12-02 14:07:46,316]\u001b[0m Trial 1 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_17969/3289153609.py\", line 63, in objective\n",
      "    scaler.step(decoder_optimizer)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\", line 310, in step\n",
      "    return optimizer.step(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py\", line 141, in step\n",
      "    F.adam(params_with_grad,\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/optim/_functional.py\", line 109, in adam\n",
      "    step_size = lr / bias_correction1\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m      2\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mNopPruner()\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     61\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     62\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mstep(embedding_optimizer)\n\u001b[0;32m---> 63\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# validate model after each epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:310\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/_functional.py:109\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction1\u001b[49m\n\u001b[1;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner()\n",
    ")\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590e626-3ba7-43f8-9679-4af442102c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"lstm_base_study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
