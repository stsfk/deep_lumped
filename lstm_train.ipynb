{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7c2bd-33c6-4a80-81ed-e08829fbfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: half percision training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a11959c-c2bb-4731-8d89-0cc6123d28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENT = 1758\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "TRAIN_YEAR = 19\n",
    "\n",
    "PATIENCE = 10\n",
    "\n",
    "dtypes = defaultdict(lambda: float)\n",
    "dtypes[\"catchment_id\"] = str\n",
    "\n",
    "use_amp = False\n",
    "VAL_STEPS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a865b605-c842-48d1-bce6-1d7c8c39884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forcing_Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fpath=\"data/Caravan/data_train_w_missing.csv\",\n",
    "        record_length=7304,\n",
    "        n_feature=3,\n",
    "    ):\n",
    "        data_raw = pd.read_csv(fpath, dtype=dtypes)\n",
    "\n",
    "        # normalization and then reshape to catchment*record*feature\n",
    "        x = data_raw.loc[:, \"P\":\"PET\"]\n",
    "\n",
    "        x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        x = x.view(-1, record_length, n_feature)\n",
    "        self.x = x.to(DEVICE)\n",
    "\n",
    "        # normalization and then reshape to catchment*record\n",
    "        y = data_raw[\"Q\"]\n",
    "\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        y = y.view(-1, record_length)\n",
    "        self.y = y.to(DEVICE)\n",
    "        \n",
    "        self.record_length = self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        # This fuction return a input and output pair for each catchment\n",
    "        # SEQ_LENGTH, BASE_LENGTH, and DEVICE is from global\n",
    "        # reference: https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "\n",
    "        # randomly selects a starting time step for each catchment\n",
    "        index = torch.randint(\n",
    "            low=0,\n",
    "            high=self.record_length - SEQ_LENGTH + 1,\n",
    "            size=(N_CATCHMENT,)\n",
    "        )\n",
    "\n",
    "        # expand the index to have the length of SEQ_LENGTH, adding 0 to SEQ_LENGTH to get correct index\n",
    "        index_y = index.unsqueeze(-1).repeat(1, SEQ_LENGTH) + torch.arange(SEQ_LENGTH)\n",
    "        index_x = index_y.unsqueeze(-1).repeat(1, 1, FORCING_DIM)\n",
    "\n",
    "        # use gather function to output values\n",
    "        x_batch, y_batch = self.x.gather(dim=1, index=index_x), self.y.gather(\n",
    "            dim=1, index=index_y\n",
    "        )\n",
    "\n",
    "        return x_batch, y_batch[:, BASE_LENGTH:]\n",
    "    \n",
    "    def get_val_batch(self):\n",
    "        n_years = math.ceil((self.record_length - BASE_LENGTH) / TARGET_SEQ_LENGTH)\n",
    "        \n",
    "        out_x = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH, FORCING_DIM])*torch.nan\n",
    "        out_y = torch.ones([n_years, N_CATCHMENT, SEQ_LENGTH])*torch.nan\n",
    "\n",
    "        for i in range(n_years):\n",
    "            start_record_ind = BASE_LENGTH*i\n",
    "                \n",
    "            if i==n_years-1:\n",
    "                end_record_ind = self.record_length\n",
    "                \n",
    "                out_x[i,:,0:(end_record_ind-start_record_ind),:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,0:(end_record_ind-start_record_ind)] = self.y[:,start_record_ind:end_record_ind]\n",
    "                \n",
    "            else:\n",
    "                end_record_ind = start_record_ind + SEQ_LENGTH\n",
    "\n",
    "                out_x[i,:,:,:] = self.x[:,start_record_ind:end_record_ind,:]\n",
    "                out_y[i,:,:] = self.y[:,start_record_ind:end_record_ind]\n",
    "        \n",
    "        return out_x, out_y[:,:,BASE_LENGTH:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7666e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = Forcing_Data(\"data/data_train_w_missing.csv\", record_length=7304)\n",
    "dval = Forcing_Data(\"data/data_val_w_missing.csv\",  record_length=4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfe6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Reference: https://stackoverflow.com/a/73704579/3361298\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ffaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    # Adatpted from https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "\n",
    "    def __init__(self, module, batch_first=False, base_length=BASE_LENGTH):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "        self.base_length = base_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # subsetting x that the first base_length elements are not interested\n",
    "        x = x[:,self.base_length:,:]\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(\n",
    "            -1, x.size(-1)\n",
    "        )  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(\n",
    "                x.size(0), -1, y.size(-1)\n",
    "            )  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d55f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        feature_dim,\n",
    "        lstm_hidden_dim,\n",
    "        fc_hidden_dims,\n",
    "        num_lstm_layers=1,\n",
    "        output_dim=1,\n",
    "        p=0.2,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.feature_dim + self.latent_dim,\n",
    "            self.lstm_hidden_dim,\n",
    "            num_layers=self.num_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # LSTM to latent code\n",
    "        self.fc_hidden_dims = fc_hidden_dims\n",
    "        self.fc_layers = []\n",
    "        self.p = p\n",
    "        for i in range((len(self.fc_hidden_dims))):\n",
    "            in_dim = self.lstm_hidden_dim if i == 0 else self.fc_hidden_dims[i - 1]\n",
    "            out_dim = self.fc_hidden_dims[i]\n",
    "            \n",
    "            self.fc_layers += [nn.Linear(in_dim, out_dim)]\n",
    "            self.fc_layers += [nn.ReLU()]\n",
    "            self.fc_layers += [nn.Dropout(p=self.p)]\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.fc_layers += [nn.Linear(self.fc_hidden_dims[-1], self.output_dim)]\n",
    "\n",
    "        self.fc_layers = TimeDistributed(nn.Sequential(*self.fc_layers), batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.out, (_, _) = self.lstm(inputs)\n",
    "        self.out = self.fc_layers(self.out)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def decode(self, code, x):\n",
    "        code = code.expand(x.shape[1], -1, -1).transpose(0, 1)\n",
    "\n",
    "        x = torch.cat((code, x), 2)\n",
    "        out = self.forward(x).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1415dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_with_nans(input, target):\n",
    "    # Adapted from https://stackoverflow.com/a/59851632/3361298\n",
    "\n",
    "    # Missing data are nans\n",
    "    mask = torch.isnan(target)\n",
    "\n",
    "    out = (input[~mask] - target[~mask]) ** 2\n",
    "    loss = out.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4b266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(\n",
    "    embedding, decoder, dataset, val_metric=mse_loss_with_nans, return_summary=True, val_steps = VAL_STEPS\n",
    "):\n",
    "    \"\"\"Validate embedding and decoder using the validation batch from dataset and val_metric.\n",
    "\n",
    "    Args:\n",
    "        embedding (Embedding): model that map catchment_id (Tensor.int) to latent code [tensor].\n",
    "        decoder (Decoder): decorder model.\n",
    "        dataset (Forcing_Data): dataset to be used in validation.\n",
    "        val_metric (function, optional): compute gof metric. Defaults to mse_loss_with_nans.\n",
    "        return_summary (bool, optional): whether the gof metric or the raw prediciton should be returned. Defaults to True.\n",
    "        val_steps(int, optional): Number of catchments evaluated at each steps. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        tensor: gof metric or raw prediction.\n",
    "    \"\"\"\n",
    "    x, y = dataset.get_val_batch()\n",
    "    \n",
    "    embedding.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    preds = torch.ones(size = y.shape, device=DEVICE)*torch.nan\n",
    "\n",
    "    # iterate over years\n",
    "    for i in range(x.shape[0]):\n",
    "        # iterate over catchments\n",
    "        for j in range(math.ceil(N_CATCHMENT / val_steps)):\n",
    "            start_catchment_ind = j * val_steps\n",
    "            end_catchment_ind = min((j + 1) * val_steps, N_CATCHMENT)\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                with torch.no_grad():\n",
    "                    code = embedding(torch.arange(start=start_catchment_ind, end = end_catchment_ind).to(DEVICE))\n",
    "                    x_sub = x[i,start_catchment_ind:end_catchment_ind,:,:]\n",
    "                    preds[i,start_catchment_ind:end_catchment_ind,:] = decoder.decode(code, x_sub).cpu()\n",
    "                \n",
    "    if return_summary:\n",
    "        out = val_metric(preds, y)\n",
    "    else:\n",
    "        out = preds\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9755c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    lstm_hidden_dim = trial.suggest_int(\"lstm_hidden_dim\", 4, 128)\n",
    "    n_lstm_layers = trial.suggest_int(\"n_lstm_layers\", 1, 2)\n",
    "    n_fc_layers = trial.suggest_int(\"n_fc_layers\", 1, 3)\n",
    "    LATENT_DIM_power = trial.suggest_int(\"LATENT_DIM_power\", 1, 2)\n",
    "    LATENT_DIM = 2**LATENT_DIM_power\n",
    "\n",
    "    drop_out_flag = trial.suggest_categorical(\"drop_out_flag\", [True, False])\n",
    "\n",
    "    if drop_out_flag:\n",
    "        p = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    else:\n",
    "        p = 0\n",
    "\n",
    "    fc_hidden_dims = []\n",
    "    for i in range(n_fc_layers):\n",
    "        fc_dim = trial.suggest_int(f\"fc_dim{i}\", 4, 16)\n",
    "        fc_hidden_dims.append(fc_dim)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        latent_dim=LATENT_DIM,\n",
    "        feature_dim=FORCING_DIM,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        fc_hidden_dims=fc_hidden_dims,\n",
    "        num_lstm_layers=n_lstm_layers,\n",
    "        output_dim=1,\n",
    "        p=p,\n",
    "    )\n",
    "\n",
    "    embedding = nn.Embedding(N_CATCHMENT, LATENT_DIM)\n",
    "\n",
    "    return embedding, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a68aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    # prepare early stopper\n",
    "    early_stopper = EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "    # define model\n",
    "    embedding, decoder = define_model(trial)\n",
    "    embedding, decoder = embedding.to(DEVICE), decoder.to(DEVICE)\n",
    "\n",
    "    # define optimizers\n",
    "    lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "    embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "    \n",
    "    lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # define batch size\n",
    "    batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 7)\n",
    "    batch_size = 2**batch_size_power\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "        # repeat TRAIN_YEAR times to finish an epoch\n",
    "        decoder.train()\n",
    "        embedding.train()\n",
    "        \n",
    "        for year in range(TRAIN_YEAR):\n",
    "\n",
    "            x_batch, y_batch = dtrain.get_random_batch()\n",
    "            catchment_index = torch.randperm(N_CATCHMENT).to(DEVICE)  # add randomness\n",
    "\n",
    "            # interate over catchments\n",
    "            for i in range(int(N_CATCHMENT / batch_size)):\n",
    "\n",
    "                # prepare data\n",
    "                ind_s = i * batch_size\n",
    "                ind_e = (i + 1) * batch_size\n",
    "\n",
    "                selected_catchments = catchment_index[ind_s:ind_e]\n",
    "\n",
    "                x_sub, y_sub = x_batch[ind_s:ind_e, :, :], y_batch[ind_s:ind_e, :]\n",
    "\n",
    "                # prepare training, put the models into training mode\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                    code = embedding(selected_catchments)\n",
    "                    out = decoder.decode(code, x_sub)\n",
    "\n",
    "                    # backprop\n",
    "                    loss = mse_loss_with_nans(out, y_sub)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "        # validate model after each epochs\n",
    "        decoder.eval()\n",
    "        embedding.eval()\n",
    "        \n",
    "        val_loss = val_model(embedding, decoder, dval).detach().cpu().numpy()\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Early stop using early_stopper, break for loop\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return early_stopper.min_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444ea97a-c3a7-4731-94ea-3e23ea9cc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-02 12:15:19,834]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\u001b[33m[W 2022-12-02 12:16:51,377]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/0j/tmjcqbl14mz0t6hplcmbj7yh0000gn/T/ipykernel_3911/3289153609.py\", line 56, in objective\n",
      "    out = decoder.decode(code, x_sub)\n",
      "  File \"/var/folders/0j/tmjcqbl14mz0t6hplcmbj7yh0000gn/T/ipykernel_3911/410528352.py\", line 53, in decode\n",
      "    out = self.forward(x).squeeze()\n",
      "  File \"/var/folders/0j/tmjcqbl14mz0t6hplcmbj7yh0000gn/T/ipykernel_3911/410528352.py\", line 44, in forward\n",
      "    self.out, (_, _) = self.lstm(inputs)\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 769, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbase_model\u001b[39m\u001b[39m\"\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mNopPruner()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb Cell 12\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16, enabled\u001b[39m=\u001b[39muse_amp):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     code \u001b[39m=\u001b[39m embedding(selected_catchments)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     out \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(code, x_sub)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# backprop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss \u001b[39m=\u001b[39m mse_loss_with_nans(out, y_sub)\n",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb Cell 12\u001b[0m in \u001b[0;36mDecoder.decode\u001b[0;34m(self, code, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m code \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mexpand(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((code, x), \u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb Cell 12\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout, (_, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_layers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/lstm_memory_saving.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner()\n",
    ")\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590e626-3ba7-43f8-9679-4af442102c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"data/base__study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
