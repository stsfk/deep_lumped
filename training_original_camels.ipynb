{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 5\n",
    "\n",
    "N_CATCHMENTS = 671\n",
    "\n",
    "# training hyperparameters\n",
    "EPOCHS = 500\n",
    "TRAIN_YEAR = 10\n",
    "PATIENCE = 20\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_val_test_data(\n",
    "    forcing_dataset,\n",
    "    train_val_length=5478,\n",
    "    train_length=4017,\n",
    "    val_length=1826,\n",
    "    test_length=5844,\n",
    "):\n",
    "    dtrain_val = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train_val.csv\",\n",
    "        record_length=train_val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtrain = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_train.csv\",\n",
    "        record_length=train_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dval = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_val.csv\",\n",
    "        record_length=val_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "\n",
    "    dtest = dataloader.Forcing_Data(\n",
    "        f\"data/671_{forcing_dataset}_original_camels_test.csv\",\n",
    "        record_length=test_length,\n",
    "        n_feature=FORCING_DIM,\n",
    "        storge_device=storge_device,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        target_seq_length=TARGET_SEQ_LENGTH,\n",
    "        base_length=BASE_LENGTH,\n",
    "    )\n",
    "    \n",
    "    return dtrain_val, dtrain, dval, dtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, model_builder):\n",
    "        self.model_builder = model_builder\n",
    "\n",
    "    def objective(self, trial):\n",
    "\n",
    "        # prepare early stopper\n",
    "        early_stopper = training_fun.EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "        # define model\n",
    "        embedding, decoder = self.model_builder.define_model(trial)\n",
    "        embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "            computing_device\n",
    "        )\n",
    "\n",
    "        if compile_model:\n",
    "            # pytorch2.0 new feature, complile model for fast training\n",
    "            embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "        # define optimizers\n",
    "        lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "        lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "        # define batch size\n",
    "        batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "        batch_size = 2**batch_size_power\n",
    "\n",
    "        # steps per epoch\n",
    "        steps = round(N_CATCHMENTS * TRAIN_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "            # repeat TRAIN_YEAR times to finish an epoch\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            for step in range(steps):\n",
    "\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # put the models into training mode\n",
    "                decoder.train()\n",
    "                embedding.train()\n",
    "\n",
    "                # get training batch and pass to device\n",
    "                invalid_batch = True\n",
    "                while invalid_batch:\n",
    "                    (x_batch, y_batch, selected_catchments) = dtrain.get_random_batch(\n",
    "                        batch_size\n",
    "                    )\n",
    "                    \n",
    "                    if len(x_batch) > 0:\n",
    "                        invalid_batch = False     \n",
    "\n",
    "                x_batch, y_batch, selected_catchments = (\n",
    "                    x_batch.to(computing_device),\n",
    "                    y_batch.to(computing_device),\n",
    "                    selected_catchments.to(computing_device),\n",
    "                )\n",
    "\n",
    "                # slice batch for training\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "                ):\n",
    "                    code = embedding(selected_catchments)\n",
    "\n",
    "                    # pass through decoder\n",
    "                    out = decoder.decode(code, x_batch)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            # validate model after each epochs\n",
    "            decoder.eval()\n",
    "            embedding.eval()\n",
    "\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if memory_saving:\n",
    "                val_loss = training_fun.val_model_mem_saving(\n",
    "                    embedding=embedding,\n",
    "                    decoder=decoder,\n",
    "                    dataset=dval,\n",
    "                    storge_device=storge_device,\n",
    "                    computing_device=computing_device,\n",
    "                    use_amp=use_amp,\n",
    "                    val_metric=training_fun.mse_loss_with_nans,\n",
    "                    return_summary=True,\n",
    "                    val_steps=VAL_STEPS,\n",
    "                )\n",
    "            else:\n",
    "                val_loss = (\n",
    "                    training_fun.val_model(\n",
    "                        embedding=embedding,\n",
    "                        decoder=decoder,\n",
    "                        dataset=dval,\n",
    "                        storge_device=storge_device,\n",
    "                        computing_device=computing_device,\n",
    "                        use_amp=use_amp,\n",
    "                        val_metric=training_fun.mse_loss_with_nans,\n",
    "                        return_summary=True,\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                torch.cuda.empty_cache()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Early stop using early_stopper, break for loop\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return early_stopper.min_validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 12:55:35,374]\u001b[0m A new study created in memory with name: nldas_671_study\u001b[0m\n",
      "/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\u001b[33m[W 2023-04-23 12:56:50,519]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/0j/tmjcqbl14mz0t6hplcmbj7yh0000gn/T/ipykernel_25754/2216805167.py\", line 81, in objective\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/Users/yang/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m LSTM_objective \u001b[39m=\u001b[39m Objective(LSTM_model_builder)\u001b[39m.\u001b[39mobjective\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mforcing_dataset\u001b[39m}\u001b[39;00m\u001b[39m_671_study\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mNopPruner(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(LSTM_objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m joblib\u001b[39m.\u001b[39mdump(study, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mforcing_dataset\u001b[39m}\u001b[39;00m\u001b[39m_671_study.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb Cell 5\u001b[0m in \u001b[0;36mObjective.objective\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     loss \u001b[39m=\u001b[39m training_fun\u001b[39m.\u001b[39mmse_loss_with_nans(out, y_batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m scaler\u001b[39m.\u001b[39mstep(embedding_optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yang/Documents/projects/deep_lumped/training_original_camels.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m scaler\u001b[39m.\u001b[39mstep(decoder_optimizer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forcing_datasets = [\"nldas\", \"daymet\", \"maurer\"]\n",
    "\n",
    "for i in range(len(forcing_datasets)):\n",
    "    forcing_dataset = forcing_datasets[i]\n",
    "\n",
    "    dtrain_val, dtrain, dval, dtest = read_train_val_test_data(forcing_dataset)\n",
    "\n",
    "    LSTM_model_builder = training_fun.LSTM_model_builder(\n",
    "        n_catchments=N_CATCHMENTS, base_length=BASE_LENGTH, forcing_dim=FORCING_DIM\n",
    "    )\n",
    "\n",
    "    LSTM_objective = Objective(LSTM_model_builder).objective\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{forcing_dataset}_671_study\",\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.NopPruner(),\n",
    "    )\n",
    "    study.optimize(LSTM_objective, n_trials=200)\n",
    "\n",
    "    joblib.dump(study, f\"data/{forcing_dataset}_671_study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
