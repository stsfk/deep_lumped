{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import dataloader\n",
    "import models\n",
    "import training_fun\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEQ_LENGTH = 365 * 2\n",
    "TARGET_SEQ_LENGTH = 365\n",
    "BASE_LENGTH = SEQ_LENGTH - TARGET_SEQ_LENGTH\n",
    "\n",
    "FORCING_DIM = 3\n",
    "\n",
    "N_CATCHMENTS = 671\n",
    "\n",
    "# training hyperparameters\n",
    "EPOCHS = 500\n",
    "TRAIN_YEAR = 15\n",
    "PATIENCE = 20\n",
    "\n",
    "use_amp = True\n",
    "compile_model = False\n",
    "\n",
    "if compile_model:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "memory_saving = False\n",
    "if memory_saving:\n",
    "    storge_device = \"cpu\"\n",
    "    computing_device = DEVICE\n",
    "    VAL_STEPS = 500\n",
    "else:\n",
    "    storge_device = DEVICE\n",
    "    computing_device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_val = dataloader.Forcing_Data(\n",
    "    \"data/original_camels_train_val.csv\",\n",
    "    record_length=5478,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtrain = dataloader.Forcing_Data(\n",
    "    \"data/original_camels_train.csv\",\n",
    "    record_length=4017,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dval = dataloader.Forcing_Data(\n",
    "    \"data/original_camels_val.csv\",\n",
    "    record_length=1826,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")\n",
    "\n",
    "dtest = dataloader.Forcing_Data(\n",
    "    \"data/original_camels_test.csv\",\n",
    "    record_length=5844,\n",
    "    storge_device=storge_device,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    target_seq_length=TARGET_SEQ_LENGTH,\n",
    "    base_length=BASE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, model_builder):\n",
    "        self.model_builder = model_builder\n",
    "\n",
    "    def objective(self, trial):\n",
    "\n",
    "        # prepare early stopper\n",
    "        early_stopper = training_fun.EarlyStopper(patience=PATIENCE, min_delta=0)\n",
    "\n",
    "        # define model\n",
    "        embedding, decoder = self.model_builder.define_model(trial)\n",
    "        embedding, decoder = embedding.to(computing_device), decoder.to(\n",
    "            computing_device\n",
    "        )\n",
    "\n",
    "        if compile_model:\n",
    "            # pytorch2.0 new feature, complile model for fast training\n",
    "            embedding, decoder = torch.compile(embedding), torch.compile(decoder)\n",
    "\n",
    "        # define optimizers\n",
    "        lr_embedding = trial.suggest_float(\"lr_embedding\", 5e-5, 1e-2, log=True)\n",
    "        embedding_optimizer = optim.Adam(embedding.parameters(), lr=lr_embedding)\n",
    "\n",
    "        lr_decoder = trial.suggest_float(\"lr_decoder\", 5e-5, 1e-2, log=True)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "        # define batch size\n",
    "        batch_size_power = trial.suggest_int(\"batch_size_power\", 4, 8)\n",
    "        batch_size = 2**batch_size_power\n",
    "\n",
    "        # steps per epoch\n",
    "        steps = round(N_CATCHMENTS * TRAIN_YEAR / batch_size)\n",
    "\n",
    "        # train model\n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            # for each epoch get_random_batch method generates a batch that contains one year data for each catchment\n",
    "            # repeat TRAIN_YEAR times to finish an epoch\n",
    "            decoder.train()\n",
    "            embedding.train()\n",
    "\n",
    "            for step in range(steps):\n",
    "\n",
    "                decoder_optimizer.zero_grad()\n",
    "                embedding_optimizer.zero_grad()\n",
    "\n",
    "                # put the models into training mode\n",
    "                decoder.train()\n",
    "                embedding.train()\n",
    "\n",
    "                # get training batch and pass to device\n",
    "                (x_batch, y_batch, selected_catchments) = dtrain.get_random_batch(\n",
    "                    batch_size\n",
    "                )\n",
    "\n",
    "                x_batch, y_batch, selected_catchments = (\n",
    "                    x_batch.to(computing_device),\n",
    "                    y_batch.to(computing_device),\n",
    "                    selected_catchments.to(computing_device),\n",
    "                )\n",
    "\n",
    "                # slice batch for training\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=use_amp\n",
    "                ):\n",
    "                    code = embedding(selected_catchments)\n",
    "\n",
    "                    # pass through decoder\n",
    "                    out = decoder.decode(code, x_batch)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = training_fun.mse_loss_with_nans(out, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(embedding_optimizer)\n",
    "                scaler.step(decoder_optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            # validate model after each epochs\n",
    "            decoder.eval()\n",
    "            embedding.eval()\n",
    "\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if memory_saving:\n",
    "                val_loss = training_fun.val_model_mem_saving(\n",
    "                    embedding=embedding,\n",
    "                    decoder=decoder,\n",
    "                    dataset=dval,\n",
    "                    storge_device=storge_device,\n",
    "                    computing_device=computing_device,\n",
    "                    use_amp=use_amp,\n",
    "                    val_metric=training_fun.mse_loss_with_nans,\n",
    "                    return_summary=True,\n",
    "                    val_steps=VAL_STEPS,\n",
    "                )\n",
    "            else:\n",
    "                val_loss = (\n",
    "                    training_fun.val_model(\n",
    "                        embedding=embedding,\n",
    "                        decoder=decoder,\n",
    "                        dataset=dval,\n",
    "                        storge_device=storge_device,\n",
    "                        computing_device=computing_device,\n",
    "                        use_amp=use_amp,\n",
    "                        val_metric=training_fun.mse_loss_with_nans,\n",
    "                        return_summary=True,\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                torch.cuda.empty_cache()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Early stop using early_stopper, break for loop\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return early_stopper.min_validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_builder = training_fun.LSTM_model_builder(\n",
    "    n_catchments=N_CATCHMENTS, base_length=BASE_LENGTH, forcing_dim=FORCING_DIM\n",
    ")\n",
    "\n",
    "LSTM_objective = Objective(LSTM_model_builder).objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-03 16:43:14,595]\u001b[0m A new study created in memory with name: base_model\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 17:52:06,976]\u001b[0m Trial 0 finished with value: inf and parameters: {'lstm_hidden_dim': 237, 'n_lstm_layers': 1, 'n_fc_layers': 3, 'LATENT_DIM_power': 3, 'drop_out_flag': True, 'dropout_rate': 0.35744606818474844, 'fc_dim0': 15, 'fc_dim1': 12, 'fc_dim2': 3, 'lr_embedding': 0.0005570104621519474, 'lr_decoder': 0.00010802265737148678, 'batch_size_power': 4}. Best is trial 0 with value: inf.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 18:05:48,248]\u001b[0m Trial 1 finished with value: 14.110627174377441 and parameters: {'lstm_hidden_dim': 98, 'n_lstm_layers': 1, 'n_fc_layers': 2, 'LATENT_DIM_power': 3, 'drop_out_flag': True, 'dropout_rate': 0.4784528827753634, 'fc_dim0': 2, 'fc_dim1': 8, 'lr_embedding': 0.005105355295843709, 'lr_decoder': 0.0001866595258822522, 'batch_size_power': 6}. Best is trial 1 with value: 14.110627174377441.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 18:08:12,475]\u001b[0m Trial 2 finished with value: 14.137568473815918 and parameters: {'lstm_hidden_dim': 16, 'n_lstm_layers': 2, 'n_fc_layers': 2, 'LATENT_DIM_power': 3, 'drop_out_flag': True, 'dropout_rate': 0.2990280841088995, 'fc_dim0': 9, 'fc_dim1': 15, 'lr_embedding': 0.00048079316886656887, 'lr_decoder': 0.001061935143474042, 'batch_size_power': 8}. Best is trial 1 with value: 14.110627174377441.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 19:00:35,633]\u001b[0m Trial 3 finished with value: inf and parameters: {'lstm_hidden_dim': 100, 'n_lstm_layers': 2, 'n_fc_layers': 3, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 3, 'fc_dim1': 8, 'fc_dim2': 6, 'lr_embedding': 0.00010298963582717577, 'lr_decoder': 0.009827834678261602, 'batch_size_power': 4}. Best is trial 1 with value: 14.110627174377441.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 20:36:12,841]\u001b[0m Trial 4 finished with value: 12.86882209777832 and parameters: {'lstm_hidden_dim': 135, 'n_lstm_layers': 2, 'n_fc_layers': 3, 'LATENT_DIM_power': 2, 'drop_out_flag': True, 'dropout_rate': 0.3955770673707024, 'fc_dim0': 10, 'fc_dim1': 6, 'fc_dim2': 14, 'lr_embedding': 0.004050648391865389, 'lr_decoder': 0.0003946986633386159, 'batch_size_power': 5}. Best is trial 4 with value: 12.86882209777832.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 21:41:36,843]\u001b[0m Trial 5 finished with value: 12.039201736450195 and parameters: {'lstm_hidden_dim': 218, 'n_lstm_layers': 2, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': True, 'dropout_rate': 0.42006050650434845, 'fc_dim0': 11, 'lr_embedding': 0.0007423347617027886, 'lr_decoder': 0.0005090598400989293, 'batch_size_power': 5}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 21:57:50,879]\u001b[0m Trial 6 finished with value: 13.042505264282227 and parameters: {'lstm_hidden_dim': 120, 'n_lstm_layers': 2, 'n_fc_layers': 1, 'LATENT_DIM_power': 3, 'drop_out_flag': False, 'fc_dim0': 15, 'lr_embedding': 0.004846322149183772, 'lr_decoder': 0.0024307641707847862, 'batch_size_power': 7}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 22:04:11,104]\u001b[0m Trial 7 finished with value: 14.321733474731445 and parameters: {'lstm_hidden_dim': 55, 'n_lstm_layers': 2, 'n_fc_layers': 1, 'LATENT_DIM_power': 3, 'drop_out_flag': True, 'dropout_rate': 0.45174069181864734, 'fc_dim0': 9, 'lr_embedding': 0.0008943926393116382, 'lr_decoder': 0.00325199690191758, 'batch_size_power': 7}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-03 22:09:34,257]\u001b[0m Trial 8 finished with value: 13.577187538146973 and parameters: {'lstm_hidden_dim': 86, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 3, 'drop_out_flag': True, 'dropout_rate': 0.463872889727524, 'fc_dim0': 9, 'lr_embedding': 7.087754395999115e-05, 'lr_decoder': 6.798849974127504e-05, 'batch_size_power': 7}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 00:19:18,753]\u001b[0m Trial 9 finished with value: 13.020485877990723 and parameters: {'lstm_hidden_dim': 181, 'n_lstm_layers': 2, 'n_fc_layers': 3, 'LATENT_DIM_power': 1, 'drop_out_flag': True, 'dropout_rate': 0.254830223298998, 'fc_dim0': 13, 'fc_dim1': 6, 'fc_dim2': 12, 'lr_embedding': 0.0012957131556195949, 'lr_decoder': 0.00021745074218052323, 'batch_size_power': 8}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 00:58:38,754]\u001b[0m Trial 10 finished with value: 12.25361156463623 and parameters: {'lstm_hidden_dim': 254, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 5, 'lr_embedding': 0.00019824790501851968, 'lr_decoder': 0.0009336284988309245, 'batch_size_power': 5}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 01:33:53,387]\u001b[0m Trial 11 finished with value: 13.372075080871582 and parameters: {'lstm_hidden_dim': 256, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 5, 'lr_embedding': 0.00020428534682270837, 'lr_decoder': 0.0008602932498338193, 'batch_size_power': 5}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 02:15:27,789]\u001b[0m Trial 12 finished with value: 14.557113647460938 and parameters: {'lstm_hidden_dim': 201, 'n_lstm_layers': 1, 'n_fc_layers': 2, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 5, 'fc_dim1': 3, 'lr_embedding': 0.00034876576099380124, 'lr_decoder': 0.000468576325466759, 'batch_size_power': 5}. Best is trial 5 with value: 12.039201736450195.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 02:42:22,890]\u001b[0m Trial 13 finished with value: 11.895861625671387 and parameters: {'lstm_hidden_dim': 203, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 6, 'lr_embedding': 0.0018699326446079217, 'lr_decoder': 0.0022007586805941684, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 03:53:53,163]\u001b[0m Trial 14 finished with value: 12.72764778137207 and parameters: {'lstm_hidden_dim': 178, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 11, 'lr_embedding': 0.0018501599707650403, 'lr_decoder': 0.002712916912754051, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 04:45:19,248]\u001b[0m Trial 15 finished with value: 13.767295837402344 and parameters: {'lstm_hidden_dim': 203, 'n_lstm_layers': 2, 'n_fc_layers': 2, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 12, 'fc_dim1': 15, 'lr_embedding': 0.002337260019908979, 'lr_decoder': 0.006659790807127711, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 05:55:15,513]\u001b[0m Trial 16 finished with value: inf and parameters: {'lstm_hidden_dim': 154, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': True, 'dropout_rate': 0.20798481932277632, 'fc_dim0': 6, 'lr_embedding': 0.009894233326845282, 'lr_decoder': 0.0014686806902732804, 'batch_size_power': 4}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 07:05:07,138]\u001b[0m Trial 17 finished with value: 14.255025863647461 and parameters: {'lstm_hidden_dim': 226, 'n_lstm_layers': 2, 'n_fc_layers': 2, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 7, 'fc_dim1': 12, 'lr_embedding': 0.0009702071060627649, 'lr_decoder': 0.0003561863781009162, 'batch_size_power': 5}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 07:28:35,393]\u001b[0m Trial 18 finished with value: 12.959497451782227 and parameters: {'lstm_hidden_dim': 214, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': True, 'dropout_rate': 0.40620644482348484, 'fc_dim0': 7, 'lr_embedding': 0.002410415340212908, 'lr_decoder': 0.0017409597769320947, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 09:37:10,872]\u001b[0m Trial 19 finished with value: 13.315119743347168 and parameters: {'lstm_hidden_dim': 166, 'n_lstm_layers': 2, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 14, 'lr_embedding': 0.0002731401262126499, 'lr_decoder': 0.00483880007050462, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 09:53:06,273]\u001b[0m Trial 20 finished with value: 15.192280769348145 and parameters: {'lstm_hidden_dim': 194, 'n_lstm_layers': 1, 'n_fc_layers': 2, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 7, 'fc_dim1': 2, 'lr_embedding': 0.0014899812959237554, 'lr_decoder': 0.0006250519760602378, 'batch_size_power': 7}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 10:28:19,313]\u001b[0m Trial 21 finished with value: 12.037874221801758 and parameters: {'lstm_hidden_dim': 256, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 4, 'lr_embedding': 0.00010718887053838476, 'lr_decoder': 0.0010541400406668803, 'batch_size_power': 5}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 11:02:13,252]\u001b[0m Trial 22 finished with value: 15.091703414916992 and parameters: {'lstm_hidden_dim': 224, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 3, 'lr_embedding': 0.00014561131213433273, 'lr_decoder': 0.001535113478226513, 'batch_size_power': 5}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 12:11:19,221]\u001b[0m Trial 23 finished with value: inf and parameters: {'lstm_hidden_dim': 244, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 3, 'lr_embedding': 0.0006800733252053223, 'lr_decoder': 0.00023215627858224383, 'batch_size_power': 4}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 12:52:47,694]\u001b[0m Trial 24 finished with value: 12.08531379699707 and parameters: {'lstm_hidden_dim': 225, 'n_lstm_layers': 1, 'n_fc_layers': 1, 'LATENT_DIM_power': 1, 'drop_out_flag': False, 'fc_dim0': 11, 'lr_embedding': 5.64468981732789e-05, 'lr_decoder': 0.0011958740152065454, 'batch_size_power': 5}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[32m[I 2023-04-04 13:39:58,577]\u001b[0m Trial 25 finished with value: 12.342576026916504 and parameters: {'lstm_hidden_dim': 155, 'n_lstm_layers': 1, 'n_fc_layers': 2, 'LATENT_DIM_power': 2, 'drop_out_flag': False, 'fc_dim0': 4, 'fc_dim1': 12, 'lr_embedding': 0.00040538028326234365, 'lr_decoder': 0.004316832182213472, 'batch_size_power': 6}. Best is trial 13 with value: 11.895861625671387.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 13:48:45,584]\u001b[0m Trial 26 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_1324\\2228413111.py\", line 100, in objective\n",
      "    training_fun.val_model(\n",
      "  File \"c:\\Users\\User\\Documents\\deep_lumped\\training_fun.py\", line 84, in val_model\n",
      "    out = val_metric(preds, y)\n",
      "  File \"c:\\Users\\User\\Documents\\deep_lumped\\training_fun.py\", line 37, in mse_loss_with_nans\n",
      "    out = (input[~mask] - target[~mask]) ** 2\n",
      "  File \"c:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\torch\\_tensor.py\", line 37, in wrapped\n",
      "    if has_torch_function(args):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbase_model\u001b[39m\u001b[39m\"\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mNopPruner()\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m study\u001b[39m.\u001b[39;49moptimize(LSTM_objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     _optimize(\n\u001b[0;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m, in \u001b[0;36mObjective.objective\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     87\u001b[0m     val_loss \u001b[39m=\u001b[39m training_fun\u001b[39m.\u001b[39mval_model_mem_saving(\n\u001b[0;32m     88\u001b[0m         embedding\u001b[39m=\u001b[39membedding,\n\u001b[0;32m     89\u001b[0m         decoder\u001b[39m=\u001b[39mdecoder,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m         val_steps\u001b[39m=\u001b[39mVAL_STEPS,\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     val_loss \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 100\u001b[0m         training_fun\u001b[39m.\u001b[39;49mval_model(\n\u001b[0;32m    101\u001b[0m             embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[0;32m    102\u001b[0m             decoder\u001b[39m=\u001b[39;49mdecoder,\n\u001b[0;32m    103\u001b[0m             dataset\u001b[39m=\u001b[39;49mdval,\n\u001b[0;32m    104\u001b[0m             storge_device\u001b[39m=\u001b[39;49mstorge_device,\n\u001b[0;32m    105\u001b[0m             computing_device\u001b[39m=\u001b[39;49mcomputing_device,\n\u001b[0;32m    106\u001b[0m             use_amp\u001b[39m=\u001b[39;49muse_amp,\n\u001b[0;32m    107\u001b[0m             val_metric\u001b[39m=\u001b[39;49mtraining_fun\u001b[39m.\u001b[39;49mmse_loss_with_nans,\n\u001b[0;32m    108\u001b[0m             return_summary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    110\u001b[0m         \u001b[39m.\u001b[39mdetach()\n\u001b[0;32m    111\u001b[0m         \u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    112\u001b[0m         \u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    115\u001b[0m trial\u001b[39m.\u001b[39mreport(val_loss, epoch)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m trial\u001b[39m.\u001b[39mshould_prune():\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\deep_lumped\\training_fun.py:84\u001b[0m, in \u001b[0;36mval_model\u001b[1;34m(embedding, decoder, dataset, storge_device, computing_device, use_amp, val_metric, return_summary)\u001b[0m\n\u001b[0;32m     81\u001b[0m             preds[i, :, :] \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mdecode(code, x_sub)\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m return_summary:\n\u001b[1;32m---> 84\u001b[0m     out \u001b[39m=\u001b[39m val_metric(preds, y)\n\u001b[0;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     out \u001b[39m=\u001b[39m preds\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\deep_lumped\\training_fun.py:37\u001b[0m, in \u001b[0;36mmse_loss_with_nans\u001b[1;34m(input, target)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmse_loss_with_nans\u001b[39m(\u001b[39minput\u001b[39m, target):\n\u001b[0;32m     32\u001b[0m     \u001b[39m# Adapted from https://stackoverflow.com/a/59851632/3361298\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m     \u001b[39m# Missing data are nans\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39misnan(target)\n\u001b[1;32m---> 37\u001b[0m     out \u001b[39m=\u001b[39m (\u001b[39minput\u001b[39;49m[\u001b[39m~\u001b[39;49mmask] \u001b[39m-\u001b[39;49m target[\u001b[39m~\u001b[39;49mmask]) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[0;32m     38\u001b[0m     loss \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\torch\\_tensor.py:37\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f, assigned\u001b[39m=\u001b[39massigned)\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     35\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[39m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     38\u001b[0m             \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"base_model\", direction=\"minimize\", pruner=optuna.pruners.NopPruner()\n",
    ")\n",
    "study.optimize(LSTM_objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/camels_lstm_study.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(study, \"data/camels_lstm_study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ddfd6d42648f68c476c776315986cac60a18b45e56ba9b8a233e8441d39da2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
